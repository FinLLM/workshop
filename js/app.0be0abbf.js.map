{"version":3,"file":"js/app.0be0abbf.js","mappings":"mEAAIA,EAAS,WAAa,IAAIC,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,MAAM,CAAC,GAAK,QAAQ,CAACF,EAAG,gBAAgB,EAAE,EAChJG,EAAkB,GCMtB,GACA,OACA,QACA,EACA,YACA,YCZwgB,I,UCQpgBC,GAAY,OACd,EACAT,EACAQ,GACA,EACA,KACA,KACA,MAIF,EAAeC,EAAiB,Q,UCjBhCC,EAAAA,EAAIC,IAAIC,EAAAA,IACR,MAAMC,EAAQ,IAAID,EAAAA,GAAAA,MAAW,CAC3BE,MAAO,CAAC,EACRC,UAAW,CAAC,EACZC,QAAS,CAAC,EACVC,QAAS,CAAC,EACVC,QAAS,CAAC,IAEZ,Q,UCVI,EAAS,WAAa,IAAIjB,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACc,YAAY,QAAQ,CAAClB,EAAImB,GAAG,GAAGnB,EAAImB,GAAG,GAAGf,EAAG,MAAM,CAACc,YAAY,sBAAsB,CAAClB,EAAImB,GAAG,GAAGf,EAAG,SAAS,CAACc,YAAY,eAAeE,GAAG,CAAC,MAAQ,SAASC,GAAQ,OAAOrB,EAAIsB,QAAQC,KAAK,OAAO,IAAI,CAACvB,EAAIwB,GAAG,wBAAwBxB,EAAImB,GAAG,GAAGnB,EAAImB,GAAG,GAAGnB,EAAImB,GAAG,GAAGnB,EAAImB,GAAG,IAAI,EACvX,EAAkB,CAAC,WAAa,IAAInB,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACqB,YAAY,CAAC,QAAU,SAAS,CAACrB,EAAG,MAAM,CAACE,MAAM,CAAC,IAAM,EAAQ,MAAyB,IAAM,OAAO,EAAE,WAAa,IAAIN,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACc,YAAY,UAAU,CAACd,EAAG,MAAM,CAACc,YAAY,kBAAkB,CAACd,EAAG,OAAO,CAACJ,EAAIwB,GAAG,iBAAiBpB,EAAG,MAAM,CAACc,YAAY,SAAS,CAACd,EAAG,MAAM,CAACE,MAAM,CAAC,IAAM,EAAQ,MAA0B,IAAM,WAAW,EAAE,WAAa,IAAIN,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACc,YAAY,YAAY,CAAClB,EAAIwB,GAAG,gCAAgCpB,EAAG,MAAMJ,EAAIwB,GAAG,gDAAgDpB,EAAG,MAAMJ,EAAIwB,GAAG,+BAA+B,EAAE,WAAa,IAAIxB,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACc,YAAY,wBAAwB,CAACd,EAAG,MAAM,CAACc,YAAY,kBAAkB,CAACd,EAAG,MAAM,CAACc,YAAY,mBAAmB,CAACd,EAAG,MAAM,CAACc,YAAY,uBAAuB,CAACd,EAAG,MAAM,CAACc,YAAY,mBAAmB,CAACd,EAAG,OAAO,CAACc,YAAY,qBAAqB,CAAClB,EAAIwB,GAAG,oBAAoBpB,EAAG,QAAQA,EAAG,OAAO,CAACc,YAAY,oBAAoB,CAAClB,EAAIwB,GAAG,oBAAoBpB,EAAG,UAAUA,EAAG,OAAO,CAACc,YAAY,qBAAqB,CAAClB,EAAIwB,GAAG,WAAWpB,EAAG,QAAQA,EAAG,OAAO,CAACc,YAAY,oBAAoB,CAAClB,EAAIwB,GAAG,0CAA0CpB,EAAG,MAAM,CAACc,YAAY,aAAaZ,MAAM,CAAC,IAAM,EAAQ,WAA+BF,EAAG,KAAK,CAACJ,EAAIwB,GAAG,yBAAyBpB,EAAG,MAAM,CAACc,YAAY,wBAAwB,CAAClB,EAAIwB,GAAG,whBAAwhBpB,EAAG,MAAMA,EAAG,MAAMJ,EAAIwB,GAAG,qeAAqepB,EAAG,MAAMA,EAAG,MAAMJ,EAAIwB,GAAG,qUAAqUpB,EAAG,KAAK,CAACJ,EAAIwB,GAAG,qBAAqBpB,EAAG,MAAM,CAACc,YAAY,wBAAwB,CAACd,EAAG,MAAM,CAACc,YAAY,oBAAoB,CAAClB,EAAIwB,GAAG,mNAAmNpB,EAAG,MAAM,CAACc,YAAY,yBAAyB,CAACd,EAAG,MAAM,CAACc,YAAY,QAAQ,CAACd,EAAG,OAAO,CAACJ,EAAIwB,GAAG,kBAAkBpB,EAAG,MAAMJ,EAAIwB,GAAG,sDAAsDpB,EAAG,MAAMJ,EAAIwB,GAAG,oEAAoEpB,EAAG,MAAMJ,EAAIwB,GAAG,0EAA0EpB,EAAG,MAAMJ,EAAIwB,GAAG,iFAAiFpB,EAAG,MAAMJ,EAAIwB,GAAG,8DAA8DpB,EAAG,MAAMJ,EAAIwB,GAAG,+DAA+DpB,EAAG,MAAMJ,EAAIwB,GAAG,iEAAiEpB,EAAG,MAAMA,EAAG,MAAMA,EAAG,OAAO,CAACJ,EAAIwB,GAAG,mBAAmBpB,EAAG,MAAMJ,EAAIwB,GAAG,uCAAuCpB,EAAG,MAAMJ,EAAIwB,GAAG,8DAA8DpB,EAAG,MAAMJ,EAAIwB,GAAG,8DAA8DpB,EAAG,MAAMJ,EAAIwB,GAAG,4DAA4DpB,EAAG,MAAMJ,EAAIwB,GAAG,wDAAwDpB,EAAG,MAAMJ,EAAIwB,GAAG,qDAAqDpB,EAAG,MAAMJ,EAAIwB,GAAG,4EAA4EpB,EAAG,MAAMJ,EAAIwB,GAAG,oEAAoEpB,EAAG,MAAMA,EAAG,MAAMA,EAAG,OAAO,CAACJ,EAAIwB,GAAG,iBAAiBpB,EAAG,MAAMJ,EAAIwB,GAAG,gDAAgDpB,EAAG,MAAMJ,EAAIwB,GAAG,yEAAyEpB,EAAG,MAAMJ,EAAIwB,GAAG,+EAA+EpB,EAAG,MAAMJ,EAAIwB,GAAG,yEAAyEpB,EAAG,MAAMJ,EAAIwB,GAAG,kFAAkFpB,EAAG,MAAM,CAACc,YAAY,WAAW,CAACd,EAAG,MAAM,CAACE,MAAM,CAAC,IAAM,EAAQ,MAAwB,IAAM,YAAYF,EAAG,KAAK,CAACJ,EAAIwB,GAAG,qBAAqBpB,EAAG,MAAM,CAACc,YAAY,sBAAsB,CAACd,EAAG,MAAM,CAACc,YAAY,QAAQ,CAACd,EAAG,IAAI,CAACc,YAAY,SAAS,CAAClB,EAAIwB,GAAG,qBAAqBpB,EAAG,IAAI,CAACc,YAAY,SAAS,CAACd,EAAG,OAAO,CAACc,YAAY,SAAS,CAAClB,EAAIwB,GAAG,mBAAmBpB,EAAG,OAAO,CAACc,YAAY,QAAQ,CAAClB,EAAIwB,GAAG,wBAAwBpB,EAAG,MAAM,CAACc,YAAY,QAAQ,CAACd,EAAG,IAAI,CAACc,YAAY,SAAS,CAAClB,EAAIwB,GAAG,uBAAuBpB,EAAG,IAAI,CAACc,YAAY,SAAS,CAACd,EAAG,OAAO,CAACc,YAAY,SAAS,CAAClB,EAAIwB,GAAG,kBAAkBpB,EAAG,OAAO,CAACc,YAAY,QAAQ,CAAClB,EAAIwB,GAAG,wBAAwBpB,EAAG,MAAM,CAACc,YAAY,QAAQ,CAACd,EAAG,IAAI,CAACc,YAAY,SAAS,CAAClB,EAAIwB,GAAG,wBAAwBpB,EAAG,IAAI,CAACc,YAAY,SAAS,CAACd,EAAG,OAAO,CAACc,YAAY,SAAS,CAAClB,EAAIwB,GAAG,oBAAoBpB,EAAG,OAAO,CAACc,YAAY,QAAQ,CAAClB,EAAIwB,GAAG,wBAAwBpB,EAAG,MAAM,CAACc,YAAY,QAAQ,CAACd,EAAG,IAAI,CAACc,YAAY,SAAS,CAAClB,EAAIwB,GAAG,qBAAqBpB,EAAG,IAAI,CAACc,YAAY,SAAS,CAACd,EAAG,OAAO,CAACc,YAAY,SAAS,CAAClB,EAAIwB,GAAG,2BAA2BpB,EAAG,KAAK,CAACJ,EAAIwB,GAAG,6BAA6BpB,EAAG,MAAM,CAACc,YAAY,wBAAwB,CAACd,EAAG,MAAM,CAACc,YAAY,4BAA4B,CAAClB,EAAIwB,GAAG,6hBAA6hBpB,EAAG,MAAMA,EAAG,MAAMJ,EAAIwB,GAAG,mGAAmGpB,EAAG,MAAMA,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,uDAAuD,CAACN,EAAIwB,GAAG,8DAA8D,EAAE,WAAa,IAAIxB,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACc,YAAY,UAAU,CAACd,EAAG,MAAM,CAACc,YAAY,aAAa,CAACd,EAAG,KAAK,CAACJ,EAAIwB,GAAG,wBAAwBpB,EAAG,MAAM,CAACA,EAAG,MAAM,CAACc,YAAY,aAAa,CAACd,EAAG,OAAO,CAACJ,EAAIwB,GAAG,cAAcpB,EAAG,MAAMJ,EAAIwB,GAAG,iCAAiCpB,EAAG,MAAMJ,EAAIwB,GAAG,kCAAkCpB,EAAG,MAAMJ,EAAIwB,GAAG,uCAAuCpB,EAAG,MAAMJ,EAAIwB,GAAG,0BAA0BpB,EAAG,MAAMJ,EAAIwB,GAAG,yCAAyCpB,EAAG,MAAM,CAACA,EAAG,OAAO,CAACJ,EAAIwB,GAAG,gBAAgBpB,EAAG,MAAMJ,EAAIwB,GAAG,kCAAkCpB,EAAG,MAAMJ,EAAIwB,GAAG,uCAAuCpB,EAAG,MAAMJ,EAAIwB,GAAG,0BAA0BpB,EAAG,MAAMJ,EAAIwB,GAAG,qCAAqCpB,EAAG,MAAMJ,EAAIwB,GAAG,0CAA0C,EAAE,WAAa,IAAIxB,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACc,YAAY,wBAAwB,CAACd,EAAG,KAAK,CAACJ,EAAIwB,GAAG,kBAAkBpB,EAAG,MAAM,CAACc,YAAY,YAAY,CAACd,EAAG,MAAM,CAACc,YAAY,kBAAkB,CAACd,EAAG,IAAI,CAACJ,EAAIwB,GAAG,sBAAsBpB,EAAG,MAAM,CAACA,EAAG,MAAM,CAACc,YAAY,YAAY,CAACd,EAAG,MAAM,CAACc,YAAY,OAAO,CAACd,EAAG,MAAM,CAACE,MAAM,CAAC,IAAM,EAAQ,MAAgC,IAAM,QAAQF,EAAG,MAAM,CAACc,YAAY,QAAQ,CAACd,EAAG,IAAI,CAACc,YAAY,QAAQ,CAAClB,EAAIwB,GAAG,yBAAyBpB,EAAG,IAAI,CAACc,YAAY,QAAQ,CAAClB,EAAIwB,GAAG,mBAAmBpB,EAAG,MAAM,CAACc,YAAY,YAAY,CAACd,EAAG,MAAM,CAACc,YAAY,OAAO,CAACd,EAAG,MAAM,CAACE,MAAM,CAAC,IAAM,EAAQ,MAAuB,IAAM,QAAQF,EAAG,MAAM,CAACc,YAAY,QAAQ,CAACd,EAAG,IAAI,CAACc,YAAY,QAAQ,CAAClB,EAAIwB,GAAG,cAAcpB,EAAG,IAAI,CAACc,YAAY,QAAQ,CAAClB,EAAIwB,GAAG,8BAA8BpB,EAAG,MAAM,CAACc,YAAY,YAAY,CAACd,EAAG,MAAM,CAACc,YAAY,OAAO,CAACd,EAAG,MAAM,CAACE,MAAM,CAAC,IAAM,EAAQ,MAAwB,IAAM,QAAQF,EAAG,MAAM,CAACc,YAAY,QAAQ,CAACd,EAAG,IAAI,CAACc,YAAY,QAAQ,CAAClB,EAAIwB,GAAG,iBAAiBpB,EAAG,IAAI,CAACc,YAAY,QAAQ,CAAClB,EAAIwB,GAAG,8BAA8BpB,EAAG,MAAM,CAACc,YAAY,YAAY,CAACd,EAAG,MAAM,CAACc,YAAY,OAAO,CAACd,EAAG,MAAM,CAACE,MAAM,CAAC,IAAM,EAAQ,MAAuB,IAAM,QAAQF,EAAG,MAAM,CAACc,YAAY,QAAQ,CAACd,EAAG,IAAI,CAACc,YAAY,QAAQ,CAAClB,EAAIwB,GAAG,eAAepB,EAAG,IAAI,CAACc,YAAY,QAAQ,CAAClB,EAAIwB,GAAG,oCAAoCpB,EAAG,IAAI,CAACA,EAAG,QAAQA,EAAG,KAAK,CAACJ,EAAIwB,GAAG,wCAAwCpB,EAAG,MAAM,CAACc,YAAY,wBAAwB,CAACd,EAAG,MAAM,CAACc,YAAY,iBAAiB,CAACd,EAAG,MAAM,CAACc,YAAY,cAAc,CAAClB,EAAIwB,GAAG,mBAAmBpB,EAAG,MAAM,CAACc,YAAY,cAAc,CAACd,EAAG,MAAM,CAACc,YAAY,YAAY,CAACd,EAAG,MAAM,CAACc,YAAY,OAAO,CAACd,EAAG,MAAM,CAACE,MAAM,CAAC,IAAM,EAAQ,MAAsB,IAAM,QAAQF,EAAG,MAAM,CAACc,YAAY,QAAQ,CAACd,EAAG,IAAI,CAACc,YAAY,QAAQ,CAAClB,EAAIwB,GAAG,gBAAgBpB,EAAG,IAAI,CAACc,YAAY,QAAQ,CAAClB,EAAIwB,GAAG,sBAAsBpB,EAAG,MAAM,CAACc,YAAY,YAAY,CAACd,EAAG,MAAM,CAACc,YAAY,OAAO,CAACd,EAAG,MAAM,CAACE,MAAM,CAAC,IAAM,EAAQ,KAAsB,IAAM,QAAQF,EAAG,MAAM,CAACc,YAAY,QAAQ,CAACd,EAAG,IAAI,CAACc,YAAY,QAAQ,CAAClB,EAAIwB,GAAG,iBAAiBpB,EAAG,IAAI,CAACc,YAAY,QAAQ,CAAClB,EAAIwB,GAAG,oBAAoBpB,EAAG,MAAM,CAACc,YAAY,iBAAiB,CAACd,EAAG,MAAM,CAACc,YAAY,aAAaO,YAAY,CAAC,aAAa,SAAS,CAACzB,EAAIwB,GAAG,yCAAyCpB,EAAG,MAAM,CAACc,YAAY,cAAc,CAACd,EAAG,MAAM,CAACc,YAAY,YAAY,CAACd,EAAG,MAAM,CAACc,YAAY,OAAO,CAACd,EAAG,MAAM,CAACE,MAAM,CAAC,IAAM,EAAQ,KAAsB,IAAM,QAAQF,EAAG,MAAM,CAACc,YAAY,QAAQ,CAACd,EAAG,IAAI,CAACc,YAAY,QAAQ,CAAClB,EAAIwB,GAAG,kBAAkBpB,EAAG,IAAI,CAACc,YAAY,QAAQ,CAAClB,EAAIwB,GAAG,oBAAoBpB,EAAG,MAAM,CAACc,YAAY,iBAAiB,CAACd,EAAG,MAAM,CAACc,YAAY,cAAc,CAAClB,EAAIwB,GAAG,mBAAmBpB,EAAG,MAAM,CAACc,YAAY,cAAc,CAACd,EAAG,MAAM,CAACc,YAAY,YAAY,CAACd,EAAG,MAAM,CAACc,YAAY,OAAO,CAACd,EAAG,MAAM,CAACE,MAAM,CAAC,IAAM,EAAQ,MAA8B,IAAM,QAAQF,EAAG,MAAM,CAACc,YAAY,QAAQ,CAACd,EAAG,IAAI,CAACc,YAAY,QAAQ,CAAClB,EAAIwB,GAAG,gBAAgBpB,EAAG,IAAI,CAACc,YAAY,QAAQ,CAAClB,EAAIwB,GAAG,mBAAmBpB,EAAG,MAAM,CAACc,YAAY,YAAY,CAACd,EAAG,MAAM,CAACc,YAAY,OAAO,CAACd,EAAG,MAAM,CAACE,MAAM,CAAC,IAAM,EAAQ,MAAkC,IAAM,QAAQF,EAAG,MAAM,CAACc,YAAY,QAAQ,CAACd,EAAG,IAAI,CAACc,YAAY,QAAQ,CAAClB,EAAIwB,GAAG,mBAAmBpB,EAAG,IAAI,CAACc,YAAY,QAAQ,CAAClB,EAAIwB,GAAG,gBAAgBpB,EAAG,MAAM,CAACc,YAAY,YAAY,CAACd,EAAG,MAAM,CAACc,YAAY,OAAO,CAACd,EAAG,MAAM,CAACE,MAAM,CAAC,IAAM,EAAQ,MAA2B,IAAM,QAAQF,EAAG,MAAM,CAACc,YAAY,QAAQ,CAACd,EAAG,IAAI,CAACc,YAAY,QAAQ,CAAClB,EAAIwB,GAAG,aAAapB,EAAG,IAAI,CAACc,YAAY,QAAQ,CAAClB,EAAIwB,GAAG,qBAAqBpB,EAAG,MAAM,CAACc,YAAY,wBAAwB,CAACd,EAAG,MAAM,CAACc,YAAY,4BAA4B,CAACd,EAAG,OAAO,CAACJ,EAAIwB,GAAG,kBAAkBxB,EAAIwB,GAAG,uYAAuYpB,EAAG,MAAMJ,EAAIwB,GAAG,qBAAqBpB,EAAG,MAAMA,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,2DAA2D,CAACN,EAAIwB,GAAG,4DAA4DpB,EAAG,MAAMA,EAAG,MAAMA,EAAG,OAAO,CAACJ,EAAIwB,GAAG,mBAAmBxB,EAAIwB,GAAG,sGAAsGpB,EAAG,MAAMA,EAAG,MAAMA,EAAG,OAAO,CAACJ,EAAIwB,GAAG,oBAAoBxB,EAAIwB,GAAG,qKAAqKpB,EAAG,MAAMA,EAAG,MAAMA,EAAG,OAAO,CAACJ,EAAIwB,GAAG,kBAAkBxB,EAAIwB,GAAG,+NAA+NpB,EAAG,MAAMA,EAAG,MAAMA,EAAG,OAAO,CAACJ,EAAIwB,GAAG,qBAAqBxB,EAAIwB,GAAG,oIAAoIpB,EAAG,MAAMA,EAAG,MAAMA,EAAG,OAAO,CAACJ,EAAIwB,GAAG,eAAexB,EAAIwB,GAAG,wQAAwQpB,EAAG,KAAK,CAACJ,EAAIwB,GAAG,qCAAqCpB,EAAG,MAAM,CAACc,YAAY,wBAAwB,CAACd,EAAG,MAAM,CAACc,YAAY,oBAAoB,CAAClB,EAAIwB,GAAG,iBAAiBpB,EAAG,MAAMJ,EAAIwB,GAAG,wEAAwEpB,EAAG,MAAMJ,EAAIwB,GAAG,0BAA0BpB,EAAG,MAAMJ,EAAIwB,GAAG,iBAAiBpB,EAAG,MAAMA,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,0BAA0B,CAACN,EAAIwB,GAAG,gCAAgC,EAAE,WAAa,IAAIxB,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACc,YAAY,kBAAkB,CAACd,EAAG,MAAM,CAACA,EAAG,MAAM,CAACc,YAAY,eAAed,EAAG,MAAM,CAACc,YAAY,cAAc,CAACd,EAAG,IAAI,CAACJ,EAAIwB,GAAG,wDAAwDpB,EAAG,IAAI,CAACqB,YAAY,CAAC,MAAQ,YAAY,CAACzB,EAAIwB,GAAG,mCAAmC,GCyblpc,KC1b4hB,ICQxhB,GAAY,OACd,EACA,EACA,GACA,EACA,KACA,WACA,MAIF,EAAe,EAAiB,QCnB5B,EAAS,WAAa,IAAIxB,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACJ,EAAImB,GAAG,GAAGnB,EAAImB,GAAG,GAAGf,EAAG,MAAM,CAACc,YAAY,sBAAsB,CAACd,EAAG,KAAK,CAACJ,EAAIwB,GAAG,cAAcpB,EAAG,MAAM,CAACc,YAAY,iBAAiBlB,EAAI0B,GAAI1B,EAAY,UAAE,SAAS2B,EAAKC,GAAO,OAAOxB,EAAG,MAAM,CAACyB,IAAID,EAAMV,YAAY,YAAY,CAACd,EAAG,MAAM,CAACc,YAAY,QAAQ,CAACd,EAAG,OAAO,CAACJ,EAAIwB,GAAGxB,EAAI8B,GAAGH,EAAKI,SAAS/B,EAAImB,GAAG,GAAE,KAAQf,EAAG,MAAM,CAACc,YAAY,eAAe,CAACd,EAAG,IAAI,CAACc,YAAY,SAAS,CAAClB,EAAIwB,GAAGxB,EAAI8B,GAAGH,EAAKK,UAAU5B,EAAG,MAAM,CAACc,YAAY,aAAa,CAACd,EAAG,MAAM,CAACc,YAAY,QAAQ,CAAClB,EAAIwB,GAAGxB,EAAI8B,GAAGH,EAAKM,OAAO7B,EAAG,MAAMJ,EAAIwB,GAAGxB,EAAI8B,GAAGH,EAAKO,cAAc,IAAG,GAAG9B,EAAG,KAAK,CAACc,YAAY,UAAU,CAAClB,EAAIwB,GAAG,yBAAyBxB,EAAImB,GAAG,GAAGf,EAAG,KAAK,CAACJ,EAAIwB,GAAG,YAAYxB,EAAImB,GAAG,KAAKnB,EAAImB,GAAG,IAAI,EAC7wB,EAAkB,CAAC,WAAa,IAAInB,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACc,YAAY,UAAU,CAACd,EAAG,MAAM,CAACc,YAAY,kBAAkB,CAACd,EAAG,OAAO,CAACJ,EAAIwB,GAAG,iBAAiBpB,EAAG,MAAM,CAACc,YAAY,SAAS,CAACd,EAAG,MAAM,CAACE,MAAM,CAAC,IAAM,EAAQ,MAA0B,IAAM,WAAW,EAAE,WAAa,IAAIN,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACc,YAAY,sBAAsB,CAACd,EAAG,MAAM,CAACc,YAAY,YAAY,CAAClB,EAAIwB,GAAG,iCAAiC,EAAE,WAAa,IAAIxB,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACc,YAAY,YAAY,CAACd,EAAG,MAAM,CAACc,YAAY,gBAAgB,EAAE,WAAa,IAAIlB,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACc,YAAY,wBAAwB,CAAClB,EAAIwB,GAAG,whBAAwhBpB,EAAG,MAAMA,EAAG,MAAMJ,EAAIwB,GAAG,meAAmepB,EAAG,MAAMA,EAAG,MAAMJ,EAAIwB,GAAG,oUAAoU,EAAE,WAAa,IAAIxB,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACc,YAAY,wBAAwB,CAACd,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,6EAA6ExB,EAAIwB,GAAG,orCAAorCpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,6DAA6DxB,EAAIwB,GAAG,8YAA8YpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,uFAAuFxB,EAAIwB,GAAG,2qCAA2qCpB,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,8DAA8D,CAACN,EAAIwB,GAAG,+DAA+DxB,EAAIwB,GAAG,MAAMpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,sDAAsDxB,EAAIwB,GAAG,0/BAA0/BpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,sFAAsFxB,EAAIwB,GAAG,0hCAA0hCpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,kHAAkHxB,EAAIwB,GAAG,k+CAAk+CpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,mDAAmDxB,EAAIwB,GAAG,qrCAAqrCpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,4DAA4DxB,EAAIwB,GAAG,yrBAAyrBpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,4HAA4HxB,EAAIwB,GAAG,87BAA87BpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,yHAAyHxB,EAAIwB,GAAG,igCAAigCpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,6DAA6DxB,EAAIwB,GAAG,wwCAAwwCpB,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,oDAAoD,CAACN,EAAIwB,GAAG,qDAAqDxB,EAAIwB,GAAG,SAASpB,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,mDAAmD,CAACN,EAAIwB,GAAG,oDAAoDpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,uDAAuDxB,EAAIwB,GAAG,w4BAAw4BpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,mFAAmFxB,EAAIwB,GAAG,46BAA46BpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,6HAA6HxB,EAAIwB,GAAG,qrCAAqrCpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,mHAAmHxB,EAAIwB,GAAG,yzDAAyzDpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,wJAAwJxB,EAAIwB,GAAG,ipCAAipCpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,qFAAqFxB,EAAIwB,GAAG,8jCAA8jCpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,gGAAgGxB,EAAIwB,GAAG,0oCAA0oCpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,mGAAmGxB,EAAIwB,GAAG,glCAAglC,EAAE,WAAa,IAAIxB,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACc,YAAY,kBAAkB,CAACd,EAAG,MAAM,CAACA,EAAG,MAAM,CAACc,YAAY,eAAed,EAAG,MAAM,CAACc,YAAY,cAAc,CAACd,EAAG,IAAI,CAACJ,EAAIwB,GAAG,wDAAwDpB,EAAG,IAAI,CAACqB,YAAY,CAAC,MAAQ,YAAY,CAACzB,EAAIwB,GAAG,mCAAmC,GCkVts0B,GACA,OACA,OACA,UAQA,CACA,mBACA,uBACA,WACA,oBACA,qCAEA,CACA,mBACA,uBACA,WACA,kBACA,uCACA,qCAEA,CACA,mBACA,uBACA,YACA,eACA,8BACA,qCAEA,CACA,mBACA,uBACA,WACA,gBACA,WACA,qCAEA,CACA,mBACA,cAEA,CACA,mBACA,uBACA,YACA,wBACA,wDACA,qCAEA,CACA,mBACA,4BACA,gBACA,qGACA,kCAEA,CACA,mBACA,4BACA,qBACA,2DACA,kCAEA,CACA,mBACA,4BACA,qBACA,oHACA,kCAEA,CACA,mBACA,uBACA,YACA,gBACA,gDACA,8BAEA,CACA,mBACA,uBACA,YACA,mBACA,+DACA,8BAEA,CACA,mBACA,uBACA,WACA,iBACA,uJACA,8BAEA,CACA,mBACA,uBACA,WACA,mBACA,+FACA,8BAEA,CACA,mBACA,uBACA,WACA,kBACA,qFACA,8BAEA,CACA,mBACA,cAEA,CACA,mBACA,wBACA,kBACA,qDACA,mCAEA,CACA,mBACA,wBACA,mBACA,uGACA,mCAEA,CACA,mBACA,6BACA,sBACA,iGACA,kCAEA,CACA,mBACA,6BACA,qBACA,oFACA,0BAEA,CACA,mBACA,6BACA,uBACA,0EACA,0BAEA,CACA,mBACA,6BACA,kBACA,iDACA,0BAEA,CACA,mBACA,6BACA,+BACA,mJACA,0BAEA,CACA,mBACA,6BACA,iBACA,wHACA,0BAEA,CACA,mBACA,wBAIA,GCxgB4hB,ICQxhB,GAAY,OACd,EACA,EACA,GACA,EACA,KACA,WACA,MAIF,EAAe,EAAiB,QCnB5B,EAAS,WAAa,IAAIxB,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACc,YAAY,uBAAuB,CAAClB,EAAImB,GAAG,GAAGf,EAAG,KAAK,CAACc,YAAY,mBAAmB,CAAClB,EAAIwB,GAAG,gCAAgCxB,EAAImB,GAAG,GAAGf,EAAG,MAAM,CAACc,YAAY,oBAAoBlB,EAAI0B,GAAG,GAAI,SAASC,GAAM,OAAOvB,EAAG,MAAM,CAACyB,IAAIF,EAAKrB,MAAM,CAAC,IAAM,QAAS,kBAAmCqB,EAAO,SAAU,IAAM,KAAK,IAAG,GAAG3B,EAAImB,GAAG,IAAI,EAClb,EAAkB,CAAC,WAAa,IAAInB,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACc,YAAY,UAAU,CAACd,EAAG,MAAM,CAACc,YAAY,kBAAkB,CAACd,EAAG,OAAO,CAACJ,EAAIwB,GAAG,iBAAiBpB,EAAG,MAAM,CAACc,YAAY,SAAS,CAACd,EAAG,MAAM,CAACE,MAAM,CAAC,IAAM,EAAQ,MAA0B,IAAM,WAAW,EAAE,WAAa,IAAIN,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACc,YAAY,wBAAwB,CAACd,EAAG,KAAK,CAACc,YAAY,aAAa,CAAClB,EAAIwB,GAAG,yBAAyBpB,EAAG,MAAM,CAACc,YAAY,QAAQ,CAAClB,EAAIwB,GAAG,yzCAAyzCpB,EAAG,KAAK,CAACc,YAAY,aAAa,CAAClB,EAAIwB,GAAG,qBAAqBpB,EAAG,MAAM,CAACc,YAAY,wBAAwB,CAACd,EAAG,MAAM,CAACc,YAAY,kBAAkB,CAAClB,EAAIwB,GAAG,oEAAoExB,EAAIwB,GAAG,qrCAAqrCpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,8EAA8ExB,EAAIwB,GAAG,62CAA62CpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,yIAAyIxB,EAAIwB,GAAG,igCAAigCpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,6IAA6IxB,EAAIwB,GAAG,+7BAA+7BpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,6IAA6IxB,EAAIwB,GAAG,qrCAAqrCpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,yGAAyGxB,EAAIwB,GAAG,0hCAA0hCpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,gGAAgGxB,EAAIwB,GAAG,qrCAAqrCpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,mJAAmJxB,EAAIwB,GAAG,ipCAAipCpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,gHAAgHxB,EAAIwB,GAAG,0oCAA0oCpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,mFAAmFxB,EAAIwB,GAAG,quCAAquCpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,8GAA8GxB,EAAIwB,GAAG,k+CAAk+CpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,kDAAkDxB,EAAIwB,GAAG,w4BAAw4BpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,8GAA8GxB,EAAIwB,GAAG,yzDAAyzDpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,gFAAgFxB,EAAIwB,GAAG,8jCAA8jCpB,EAAG,MAAM,CAACc,YAAY,eAAe,CAAClB,EAAIwB,GAAG,8FAA8FxB,EAAIwB,GAAG,klCAAklCpB,EAAG,KAAK,CAACc,YAAY,aAAa,CAAClB,EAAIwB,GAAG,YAAY,EAAE,WAAa,IAAIxB,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACc,YAAY,kBAAkB,CAACd,EAAG,MAAM,CAACA,EAAG,MAAM,CAACc,YAAY,eAAed,EAAG,MAAM,CAACc,YAAY,cAAc,CAACd,EAAG,IAAI,CAACJ,EAAIwB,GAAG,wDAAwDpB,EAAG,IAAI,CAACqB,YAAY,CAAC,MAAQ,YAAY,CAACzB,EAAIwB,GAAG,mCAAmC,GCuRhzsB,KCxR4hB,ICQxhB,GAAY,OACd,EACA,EACA,GACA,EACA,KACA,WACA,MAIF,EAAe,EAAiB,QCjBhCf,EAAAA,EAAIC,IAAIyB,EAAAA,IAIR,MAAMC,EAAS,CACb,CACEC,KAAM,IACNJ,KAAM,OACNzB,UAAW8B,GAEb,CACED,KAAM,YACNJ,KAAM,WACNzB,UAAW+B,GAEb,CACEF,KAAM,OACNJ,KAAM,qBACNzB,UAAWgC,IAITC,EAAS,IAAIN,EAAAA,GAAU,CAAEO,KAAM,OAAQN,WAE7C,Q,4DCxBA,MAAMO,EAAa,CAAAC,EAAAA,EAAAC,EAAAA,GAEbC,EAAeA,KACnBrC,EAAAA,EAAIsC,UAAUC,cAAaC,EAAAA,EAC3BN,EAAWO,SAAQ1C,IACjBC,EAAAA,EAAID,UAAUA,EAAUyB,KAAMzB,EAAU,GACxC,EAGJ,QCXA,MAAM2C,EAASA,KACb,MAAMC,EAAcC,OAAOC,YAAcC,SAASC,gBAAgBC,YAClE,IAAIC,EAAQ,EACZ,MAAMC,EAAW,IAGfD,EADEN,GAAeO,EACTP,EAAc,KACbA,GAAe,IAChBA,EAAc,KAEdA,EAAc,KAGxB,MAAMQ,EAAM,GAAKF,EACjBH,SAASC,gBAAgBK,MAAMC,SAAY,GAAEF,KAAO,ECRtDd,IACArC,EAAAA,EAAIsD,OAAOC,eAAgB,EAC3Bb,IACAE,OAAOY,SAAW,KAChBd,GAAQ,EAEV,IAAI1C,EAAAA,EAAI,CACNG,MAAK,EACL6B,OAAM,EACN1C,OAAQmE,GAAKA,EAAEC,KACdC,OAAO,O,uBChBV,IAAIC,EAAM,CACT,wBAAyB,KACzB,wBAAyB,KACzB,wBAAyB,KACzB,wBAAyB,KACzB,wBAAyB,KACzB,wBAAyB,MAI1B,SAASC,EAAeC,GACvB,IAAIC,EAAKC,EAAsBF,GAC/B,OAAOG,EAAoBF,EAC5B,CACA,SAASC,EAAsBF,GAC9B,IAAIG,EAAoBC,EAAEN,EAAKE,GAAM,CACpC,IAAIK,EAAI,IAAIC,MAAM,uBAAyBN,EAAM,KAEjD,MADAK,EAAEE,KAAO,mBACHF,CACP,CACA,OAAOP,EAAIE,EACZ,CACAD,EAAeS,KAAO,WACrB,OAAOC,OAAOD,KAAKV,EACpB,EACAC,EAAeW,QAAUR,EACzBS,EAAOC,QAAUb,EACjBA,EAAeE,GAAK,I,igDC1BhBY,EAA2B,CAAC,EAGhC,SAASV,EAAoBW,GAE5B,IAAIC,EAAeF,EAAyBC,GAC5C,QAAqBE,IAAjBD,EACH,OAAOA,EAAaH,QAGrB,IAAID,EAASE,EAAyBC,GAAY,CAGjDF,QAAS,CAAC,GAOX,OAHAK,EAAoBH,GAAUI,KAAKP,EAAOC,QAASD,EAAQA,EAAOC,QAAST,GAGpEQ,EAAOC,OACf,CAGAT,EAAoBgB,EAAIF,E,WCzBxB,IAAIG,EAAW,GACfjB,EAAoBkB,EAAI,SAASC,EAAQC,EAAUC,EAAIC,GACtD,IAAGF,EAAH,CAMA,IAAIG,EAAeC,IACnB,IAASC,EAAI,EAAGA,EAAIR,EAASS,OAAQD,IAAK,CACrCL,EAAWH,EAASQ,GAAG,GACvBJ,EAAKJ,EAASQ,GAAG,GACjBH,EAAWL,EAASQ,GAAG,GAE3B,IAJA,IAGIE,GAAY,EACPC,EAAI,EAAGA,EAAIR,EAASM,OAAQE,MACpB,EAAXN,GAAsBC,GAAgBD,IAAahB,OAAOD,KAAKL,EAAoBkB,GAAGW,OAAM,SAAS1E,GAAO,OAAO6C,EAAoBkB,EAAE/D,GAAKiE,EAASQ,GAAK,IAChKR,EAASU,OAAOF,IAAK,IAErBD,GAAY,EACTL,EAAWC,IAAcA,EAAeD,IAG7C,GAAGK,EAAW,CACbV,EAASa,OAAOL,IAAK,GACrB,IAAIM,EAAIV,SACER,IAANkB,IAAiBZ,EAASY,EAC/B,CACD,CACA,OAAOZ,CArBP,CAJCG,EAAWA,GAAY,EACvB,IAAI,IAAIG,EAAIR,EAASS,OAAQD,EAAI,GAAKR,EAASQ,EAAI,GAAG,GAAKH,EAAUG,IAAKR,EAASQ,GAAKR,EAASQ,EAAI,GACrGR,EAASQ,GAAK,CAACL,EAAUC,EAAIC,EAwB/B,C,eC5BAtB,EAAoBgC,EAAI,SAASxB,GAChC,IAAIyB,EAASzB,GAAUA,EAAO0B,WAC7B,WAAa,OAAO1B,EAAO,UAAY,EACvC,WAAa,OAAOA,CAAQ,EAE7B,OADAR,EAAoBmC,EAAEF,EAAQ,CAAEG,EAAGH,IAC5BA,CACR,C,eCNAjC,EAAoBmC,EAAI,SAAS1B,EAAS4B,GACzC,IAAI,IAAIlF,KAAOkF,EACXrC,EAAoBC,EAAEoC,EAAYlF,KAAS6C,EAAoBC,EAAEQ,EAAStD,IAC5EmD,OAAOgC,eAAe7B,EAAStD,EAAK,CAAEoF,YAAY,EAAMC,IAAKH,EAAWlF,IAG3E,C,eCPA6C,EAAoByC,EAAI,WACvB,GAA0B,kBAAfC,WAAyB,OAAOA,WAC3C,IACC,OAAOnH,MAAQ,IAAIoH,SAAS,cAAb,EAChB,CAAE,MAAOzC,GACR,GAAsB,kBAAXvB,OAAqB,OAAOA,MACxC,CACA,CAPuB,E,eCAxBqB,EAAoBC,EAAI,SAAS2C,EAAKC,GAAQ,OAAOvC,OAAOjC,UAAUyE,eAAe/B,KAAK6B,EAAKC,EAAO,C,eCAtG7C,EAAoB+C,EAAI,Y,eCKxB,IAAIC,EAAkB,CACrB,IAAK,GAaNhD,EAAoBkB,EAAEU,EAAI,SAASqB,GAAW,OAAoC,IAA7BD,EAAgBC,EAAgB,EAGrF,IAAIC,EAAuB,SAASC,EAA4BC,GAC/D,IAKIzC,EAAUsC,EALV7B,EAAWgC,EAAK,GAChBC,EAAcD,EAAK,GACnBE,EAAUF,EAAK,GAGI3B,EAAI,EAC3B,GAAGL,EAASmC,MAAK,SAASzD,GAAM,OAA+B,IAAxBkD,EAAgBlD,EAAW,IAAI,CACrE,IAAIa,KAAY0C,EACZrD,EAAoBC,EAAEoD,EAAa1C,KACrCX,EAAoBgB,EAAEL,GAAY0C,EAAY1C,IAGhD,GAAG2C,EAAS,IAAInC,EAASmC,EAAQtD,EAClC,CAEA,IADGmD,GAA4BA,EAA2BC,GACrD3B,EAAIL,EAASM,OAAQD,IACzBwB,EAAU7B,EAASK,GAChBzB,EAAoBC,EAAE+C,EAAiBC,IAAYD,EAAgBC,IACrED,EAAgBC,GAAS,KAE1BD,EAAgBC,GAAW,EAE5B,OAAOjD,EAAoBkB,EAAEC,EAC9B,EAEIqC,EAAqBC,KAAK,wBAA0BA,KAAK,yBAA2B,GACxFD,EAAmBhF,QAAQ0E,EAAqBQ,KAAK,KAAM,IAC3DF,EAAmB3G,KAAOqG,EAAqBQ,KAAK,KAAMF,EAAmB3G,KAAK6G,KAAKF,G,IC/CvF,IAAIG,EAAsB3D,EAAoBkB,OAAEL,EAAW,CAAC,MAAM,WAAa,OAAOb,EAAoB,KAAO,IACjH2D,EAAsB3D,EAAoBkB,EAAEyC,E","sources":["webpack://workshop/./src/App.vue?afce","webpack://workshop/src/App.vue","webpack://workshop/./src/App.vue?5fff","webpack://workshop/./src/App.vue","webpack://workshop/./src/store/index.js","webpack://workshop/./src/views/Home/index.vue?cc18","webpack://workshop/src/views/Home/index.vue","webpack://workshop/./src/views/Home/index.vue?3c31","webpack://workshop/./src/views/Home/index.vue","webpack://workshop/./src/views/Brochure/index.vue?1616","webpack://workshop/src/views/Brochure/index.vue","webpack://workshop/./src/views/Brochure/index.vue?a7ae","webpack://workshop/./src/views/Brochure/index.vue","webpack://workshop/./src/views/ConferenceBrochure/index.vue?c10f","webpack://workshop/src/views/ConferenceBrochure/index.vue","webpack://workshop/./src/views/ConferenceBrochure/index.vue?80ca","webpack://workshop/./src/views/ConferenceBrochure/index.vue","webpack://workshop/./src/routes/index.js","webpack://workshop/./src/plugins/ant.js","webpack://workshop/./src/utils/rem.js","webpack://workshop/./src/main.js","webpack://workshop/./src/assets/img/ sync ^\\.\\/jiangzhuang \\(.*\\)\\.jpg$","webpack://workshop/webpack/bootstrap","webpack://workshop/webpack/runtime/chunk loaded","webpack://workshop/webpack/runtime/compat get default export","webpack://workshop/webpack/runtime/define property getters","webpack://workshop/webpack/runtime/global","webpack://workshop/webpack/runtime/hasOwnProperty shorthand","webpack://workshop/webpack/runtime/publicPath","webpack://workshop/webpack/runtime/jsonp chunk loading","webpack://workshop/webpack/startup"],"sourcesContent":["var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{attrs:{\"id\":\"app\"}},[_c('router-view')],1)}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","<template>\r\n  <div id=\"app\">\r\n    <router-view />\r\n  </div>\r\n</template>\r\n\r\n<script>\r\nexport default {\r\n  data() {\r\n    return {}\r\n  },\r\n  mounted() {},\r\n  methods: {},\r\n}\r\n</script>\r\n\r\n<style>\r\nhtml {\r\n  -webkit-text-size-adjust: 100%; /* WebKit */\r\n  -ms-text-size-adjust: 100%; /* IE */\r\n  -moz-text-size-adjust: 100%; /* Firefox */\r\n}\r\n\r\nbody {\r\n  margin: 0;\r\n}\r\n</style>\r\n","import mod from \"-!../node_modules/.pnpm/registry.npmmirror.com+thread-loader@3.0.4_webpack@5.88.2/node_modules/thread-loader/dist/cjs.js!../node_modules/.pnpm/registry.npmmirror.com+babel-loader@8.3.0_@babel+core@7.12.16_webpack@5.88.2/node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../node_modules/.pnpm/registry.npmmirror.com+vue-loader@15.10.1_css-loader@6.8.1_vue-template-compiler@2.6.14_webpack@5.88.2/node_modules/vue-loader/lib/index.js??vue-loader-options!./App.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../node_modules/.pnpm/registry.npmmirror.com+thread-loader@3.0.4_webpack@5.88.2/node_modules/thread-loader/dist/cjs.js!../node_modules/.pnpm/registry.npmmirror.com+babel-loader@8.3.0_@babel+core@7.12.16_webpack@5.88.2/node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../node_modules/.pnpm/registry.npmmirror.com+vue-loader@15.10.1_css-loader@6.8.1_vue-template-compiler@2.6.14_webpack@5.88.2/node_modules/vue-loader/lib/index.js??vue-loader-options!./App.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./App.vue?vue&type=template&id=70eba6d6&\"\nimport script from \"./App.vue?vue&type=script&lang=js&\"\nexport * from \"./App.vue?vue&type=script&lang=js&\"\nimport style0 from \"./App.vue?vue&type=style&index=0&id=70eba6d6&prod&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../node_modules/.pnpm/registry.npmmirror.com+vue-loader@15.10.1_css-loader@6.8.1_vue-template-compiler@2.6.14_webpack@5.88.2/node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  null,\n  null\n  \n)\n\nexport default component.exports","import Vue from 'vue'\r\nimport Vuex from 'vuex'\r\nVue.use(Vuex)\r\nconst store = new Vuex.Store({\r\n  state: {},\r\n  mutations: {},\r\n  actions: {},\r\n  getters: {},\r\n  modules: {},\r\n})\r\nexport default store\r\n","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"home\"},[_vm._m(0),_vm._m(1),_c('div',{staticClass:\"subtitle padding w\"},[_vm._m(2),_c('button',{staticClass:\"registration\",on:{\"click\":function($event){return _vm.$router.push('/fcb')}}},[_vm._v(\" List of Awards \")])]),_vm._m(3),_vm._m(4),_vm._m(5),_vm._m(6)])}\nvar staticRenderFns = [function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticStyle:{\"display\":\"none\"}},[_c('img',{attrs:{\"src\":require(\"@/assets/img/logo.jpg\"),\"alt\":\"\"}})])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"header\"},[_c('div',{staticClass:\"w padding flex\"},[_c('span',[_vm._v(\"FinLLM 2023\")]),_c('div',{staticClass:\"right\"},[_c('img',{attrs:{\"src\":require(\"@/assets/img/ijcai.png\"),\"alt\":\"\"}})])])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"sub-text\"},[_vm._v(\" International Symposium on \"),_c('br'),_vm._v(\"Large Language Models for Financial Services\"),_c('br'),_vm._v(\" (FinLLM 2023)@IJCAI 2023 \")])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"center-box w padding\"},[_c('div',{staticClass:\"center-img-box\"},[_c('div',{staticClass:\"center-text-box\"},[_c('div',{staticClass:\"center-text-content\"},[_c('div',{staticClass:\"center-text-top\"},[_c('span',{staticClass:\"center-text-title\"},[_vm._v(\"Symposium Date: \"),_c('br')]),_c('span',{staticClass:\"center-text-body\"},[_vm._v(\"August 20, 2023 \"),_c('br')])]),_c('span',{staticClass:\"center-text-title\"},[_vm._v(\"Venue: \"),_c('br')]),_c('span',{staticClass:\"center-text-body\"},[_vm._v(\"Sheraton Grand Macao Hotel,Macau\")])])]),_c('img',{staticClass:\"center-img\",attrs:{\"src\":require(\"@/assets/img/hotel.jpg\")}})]),_c('h1',[_vm._v(\"Scope and Objective\")]),_c('div',{staticClass:\"cente-text mg-bottom\"},[_vm._v(\" Pretrained large language models (LLMs) have demonstrated tremendous potential in various natural language processing tasks, including language generation, machine translation, and question answering. In the financial services industry, pretrained LLMs have the potential to significantly impact tasks such as financial forecasting, risk management, and sentiment analysis. Moreover, LLMs can help automate the process of analyzing financial reports and extracting key insights that can aid businesses in making informed decisions.\"),_c('br'),_c('br'),_vm._v(\" This symposium provides a platform for researchers, practitioners, and industry experts from around the world to share new ideas, exchange research findings, and discuss the challenges and opportunities in the field of pretrained LLMs for financial services. The symposium will cover two themes: 1) potential applications and best practices of pretrained LLMs for financial services; and 2) challenges that need to be addressed to make them efficient, effective, and trustworthy. \"),_c('br'),_c('br'),_vm._v(\" The symposium will also feature invited talks by leading researchers and industry experts, as well as panel discussions on the latest trends and challenges in the field. We welcome researchers, practitioners, and industry experts from academia and industry to submit their work and participate in this exciting event. \")]),_c('h1',[_vm._v(\"Call for Papers\")]),_c('div',{staticClass:\"cente-text mg-bottom\"},[_c('div',{staticClass:\"cente-text-title\"},[_vm._v(\" We welcome submissions on recent advances and applications of large language models (LLMs) in financial services. We encourage submissions that cover a wide range of topics, including but not limited to: \")]),_c('div',{staticClass:\"cente-text-cente flex\"},[_c('div',{staticClass:\"text\"},[_c('span',[_vm._v(\"Techniques: \")]),_c('br'),_vm._v(\"-Multimodal modeling of financial data using LLMs \"),_c('br'),_vm._v(\"-Preprocessing and cleaning of financial data for use with LLMs \"),_c('br'),_vm._v(\"-Integration of LLMs with other AI technologies in financial services \"),_c('br'),_vm._v(\"-Novel architectures and training techniques for LLMs in financial services. \"),_c('br'),_vm._v(\"-Scalability and efficiency of LLMs in financial services \"),_c('br'),_vm._v(\"-Cross-lingual and multilingual LLMs in financial services \"),_c('br'),_vm._v(\"-Human-in-the-loop approaches for LLMs in financial services \"),_c('br'),_c('br'),_c('span',[_vm._v(\"Applications:\")]),_c('br'),_vm._v(\" -Financial forecasting using LLMs \"),_c('br'),_vm._v(\"-Sentiment analysis and opinion mining for financial data \"),_c('br'),_vm._v(\"-LLM-based trading algorithms and decision-making systems \"),_c('br'),_vm._v(\"-Analysis of financial news and social media using LLMs \"),_c('br'),_vm._v(\"-Semantic analysis of financial reports and filings \"),_c('br'),_vm._v(\"-Explainable AI in financial services using LLMs \"),_c('br'),_vm._v(\"-Transfer learning and domain adaptation for LLMs in financial services \"),_c('br'),_vm._v(\"-Case studies and success stories of LLMs in financial services \"),_c('br'),_c('br'),_c('span',[_vm._v(\"Challenges:\")]),_c('br'),_vm._v(\" -Evaluation of LLMs for financial services \"),_c('br'),_vm._v(\"-Social economics and trustworthiness for LLMs in financial services \"),_c('br'),_vm._v(\"-Ethical and legal considerations in the use of LLMs in financial services \"),_c('br'),_vm._v(\"-Privacy and security concerns in the use of LLMs for financial data \"),_c('br'),_vm._v(\"-Bias and fairness considerations in the use of LLMs for financial services \")]),_c('div',{staticClass:\"img-box\"},[_c('img',{attrs:{\"src\":require(\"@/assets/img/IMG.png\"),\"alt\":\"\"}})])])]),_c('h1',[_vm._v(\"Important Dates\")]),_c('div',{staticClass:\"card-box mg-bottom\"},[_c('div',{staticClass:\"card\"},[_c('p',{staticClass:\"title\"},[_vm._v(\"Submission Due:\")]),_c('p',{staticClass:\"timer\"},[_c('span',{staticClass:\"month\"},[_vm._v(\"June 20, 2023\")]),_c('span',{staticClass:\"date\"},[_vm._v(\"(23:59:59 AoE)\")])])]),_c('div',{staticClass:\"card\"},[_c('p',{staticClass:\"title\"},[_vm._v(\"Notification Due:\")]),_c('p',{staticClass:\"timer\"},[_c('span',{staticClass:\"month\"},[_vm._v(\"July 4, 2023\")]),_c('span',{staticClass:\"date\"},[_vm._v(\"(23:59:59 AoE)\")])])]),_c('div',{staticClass:\"card\"},[_c('p',{staticClass:\"title\"},[_vm._v(\"Final Version Due:\")]),_c('p',{staticClass:\"timer\"},[_c('span',{staticClass:\"month\"},[_vm._v(\"August 3, 2023\")]),_c('span',{staticClass:\"date\"},[_vm._v(\"(23:59:59 AoE)\")])])]),_c('div',{staticClass:\"card\"},[_c('p',{staticClass:\"title\"},[_vm._v(\"Symposium Date:\")]),_c('p',{staticClass:\"timer\"},[_c('span',{staticClass:\"month\"},[_vm._v(\"August 20, 2023\")])])])]),_c('h1',[_vm._v(\"Submission Instructions\")]),_c('div',{staticClass:\"cente-text mg-bottom\"},[_c('div',{staticClass:\"cente-text-cente justify\"},[_vm._v(\" Submissions should be a maximum of 7 and a minimum of 4 pages including figures and tables in IJCAI'23 format. Additional pages containing only cited references are allowed. We do accept submissions of work recently published or currently under review. The submissions can contain author details. The Symposium will not have formal proceedings, but authors of accepted papers can choose to have their work published on the Symposium webpage. Top selected papers from the Symposium will be invited to publish in a journal special issue. \"),_c('br'),_c('br'),_vm._v(\" We recommend submitting your paper through EasyChair. The paper submission link is as follows:\"),_c('br'),_c('a',{attrs:{\"href\":\"https://easychair.org/conferences/?conf=finllm2023\"}},[_vm._v(\" https://easychair.org/conferences/?conf=finllm2023\")])])])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"middle\"},[_c('div',{staticClass:\"w padding\"},[_c('h1',[_vm._v(\"Tentative Schedule\")]),_c('div',[_c('div',{staticClass:\"middle-mg\"},[_c('span',[_vm._v(\"Morning:\")]),_c('br'),_vm._v(\"• 08:30 - 09:00 Registeration\"),_c('br'),_vm._v(\"• 09:00 - 10:00 Invited talks \"),_c('br'),_vm._v(\"• 10:00 - 10:45 Oral presentations \"),_c('br'),_vm._v(\"• 10:45 - 11:00 Break \"),_c('br'),_vm._v(\"• 11:00 - 11:45 Oral presentations \")]),_c('div',[_c('span',[_vm._v(\"Afternoon:\")]),_c('br'),_vm._v(\"• 13:00 - 14:30 Invited talks \"),_c('br'),_vm._v(\"• 14:30 - 15:15 Oral presentations \"),_c('br'),_vm._v(\"• 15:15 - 15:30 Break \"),_c('br'),_vm._v(\"• 15:30 - 16:30 Panel Discussion \"),_c('br'),_vm._v(\"• 16:30 - 17:00 Award Ceremony \")])])])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"center-box w padding\"},[_c('h1',[_vm._v(\"Organization\")]),_c('div',{staticClass:\"item-box\"},[_c('div',{staticClass:\"item-box-right\"},[_c('p',[_vm._v(\"Invited Speakers\")]),_c('div',[_c('div',{staticClass:\"info-box\"},[_c('div',{staticClass:\"avt\"},[_c('img',{attrs:{\"src\":require(\"@/assets/img/Mark Dredze.png\"),\"alt\":\"\"}})]),_c('div',{staticClass:\"info\"},[_c('p',{staticClass:\"name\"},[_vm._v(\"Mark Dredze(online)\")]),_c('p',{staticClass:\"note\"},[_vm._v(\"Bloomberg\")])])]),_c('div',{staticClass:\"info-box\"},[_c('div',{staticClass:\"avt\"},[_c('img',{attrs:{\"src\":require(\"@/assets/img/白硕.jpg\"),\"alt\":\"\"}})]),_c('div',{staticClass:\"info\"},[_c('p',{staticClass:\"name\"},[_vm._v(\"Shuo Bai\")]),_c('p',{staticClass:\"note\"},[_vm._v(\"Hundsun Technologies\")])])]),_c('div',{staticClass:\"info-box\"},[_c('div',{staticClass:\"avt\"},[_c('img',{attrs:{\"src\":require(\"@/assets/img/王巍巍.jpg\"),\"alt\":\"\"}})]),_c('div',{staticClass:\"info\"},[_c('p',{staticClass:\"name\"},[_vm._v(\"Weiwei Wang\")]),_c('p',{staticClass:\"note\"},[_vm._v(\"Alibaba DAMO Academy\")])])]),_c('div',{staticClass:\"info-box\"},[_c('div',{staticClass:\"avt\"},[_c('img',{attrs:{\"src\":require(\"@/assets/img/皱胜.jpg\"),\"alt\":\"\"}})]),_c('div',{staticClass:\"info\"},[_c('p',{staticClass:\"name\"},[_vm._v(\"Sheng Zou\")]),_c('p',{staticClass:\"note\"},[_vm._v(\"Archforce Technology\")])])])])])]),_c('p',[_c('br')]),_c('h1',[_vm._v(\"Organizers of Organizing Committee\")]),_c('div',{staticClass:\"cente-text mg-bottom\"},[_c('div',{staticClass:\"centet-people\"},[_c('div',{staticClass:\"left-title\"},[_vm._v(\"General Chair\")]),_c('div',{staticClass:\"people-box\"},[_c('div',{staticClass:\"info-box\"},[_c('div',{staticClass:\"avt\"},[_c('img',{attrs:{\"src\":require(\"@/assets/img/7.png\"),\"alt\":\"\"}})]),_c('div',{staticClass:\"info\"},[_c('p',{staticClass:\"name\"},[_vm._v(\"Qiang Yang\")]),_c('p',{staticClass:\"note\"},[_vm._v(\"WeBank/HKUST\")])])]),_c('div',{staticClass:\"info-box\"},[_c('div',{staticClass:\"avt\"},[_c('img',{attrs:{\"src\":require(\"@/assets/img/9.png\"),\"alt\":\"\"}})]),_c('div',{staticClass:\"info\"},[_c('p',{staticClass:\"name\"},[_vm._v(\"Liyuan Chen\")]),_c('p',{staticClass:\"note\"},[_vm._v(\"E Fund\")])])])])]),_c('div',{staticClass:\"centet-people\"},[_c('div',{staticClass:\"left-title\",staticStyle:{\"text-align\":\"left\"}},[_vm._v(\" Chairman of the Program Committee \")]),_c('div',{staticClass:\"people-box\"},[_c('div',{staticClass:\"info-box\"},[_c('div',{staticClass:\"avt\"},[_c('img',{attrs:{\"src\":require(\"@/assets/img/6.png\"),\"alt\":\"\"}})]),_c('div',{staticClass:\"info\"},[_c('p',{staticClass:\"name\"},[_vm._v(\"Shuoling Liu\")]),_c('p',{staticClass:\"note\"},[_vm._v(\"E Fund\")])])])])]),_c('div',{staticClass:\"centet-people\"},[_c('div',{staticClass:\"left-title\"},[_vm._v(\"Program Chair\")]),_c('div',{staticClass:\"people-box\"},[_c('div',{staticClass:\"info-box\"},[_c('div',{staticClass:\"avt\"},[_c('img',{attrs:{\"src\":require(\"@/assets/img/xueyangWu.png\"),\"alt\":\"\"}})]),_c('div',{staticClass:\"info\"},[_c('p',{staticClass:\"name\"},[_vm._v(\"Xueyang Wu\")]),_c('p',{staticClass:\"note\"},[_vm._v(\"Flaiverse\")])])]),_c('div',{staticClass:\"info-box\"},[_c('div',{staticClass:\"avt\"},[_c('img',{attrs:{\"src\":require(\"@/assets/img/yongpeng tang.png\"),\"alt\":\"\"}})]),_c('div',{staticClass:\"info\"},[_c('p',{staticClass:\"name\"},[_vm._v(\"Yongpeng Tang\")]),_c('p',{staticClass:\"note\"},[_vm._v(\"E Fund\")])])]),_c('div',{staticClass:\"info-box\"},[_c('div',{staticClass:\"avt\"},[_c('img',{attrs:{\"src\":require(\"@/assets/img/qianxu.png\"),\"alt\":\"\"}})]),_c('div',{staticClass:\"info\"},[_c('p',{staticClass:\"name\"},[_vm._v(\"Qian Xu\")]),_c('p',{staticClass:\"note\"},[_vm._v(\"HKUST\")])])])])])]),_c('div',{staticClass:\"cente-text mg-bottom\"},[_c('div',{staticClass:\"cente-text-cente justify\"},[_c('span',[_vm._v(\"Qiang Yang: \")]),_vm._v(\" Chief AI Officer of WeBank, Department Head of Computer Science and Engineering at the Hong Kong University of Science and Technology, New Bright Professorship in Engineering. AAAI Fellow, IEEE Fellow, AAAS Fellow, IAPR Fellow and ACM Distinguished Members. President of IJCAI (2017-2019). Vice Chair of ACM SIGART (SIGAI) (2009-2012), PC Chair and Co-Chair: IJCAI(2015), ACM KDD(2010).\"),_c('br'),_vm._v(\" Google Scholar: \"),_c('br'),_c('a',{attrs:{\"href\":\"https://scholar.google.com/citations?user=1LxWZLQAAAAJ\"}},[_vm._v(\"https://scholar.google.com/citations?user=1LxWZLQAAAAJ\")]),_c('br'),_c('br'),_c('span',[_vm._v(\"Liyuan Chen: \")]),_vm._v(\" Chief Operating Officer of E Fund Management, Chairman of the E Fund FinTech Executive Committee.\"),_c('br'),_c('br'),_c('span',[_vm._v(\"Shuoling Liu: \")]),_vm._v(\" Head of Institute of Innovation at E Fund Management, Secretary-General of E Fund FinTech Executive Committee, Member of the FinTech Special Committee of AMAC. \"),_c('br'),_c('br'),_c('span',[_vm._v(\"Xueyang Wu: \")]),_vm._v(\" PhD, HKUST (2023), Co-founder & Principal Scientist at Flaiverse. Research interests: transfer learning, federated learning, NLP, LLMs. Publications: research papers & patents. Reviewer: NeurIPS, KDD, ICML, ICLR, etc. \"),_c('br'),_c('br'),_c('span',[_vm._v(\"Yongpeng Tang: \")]),_vm._v(\" MD, Head of Application Development at E Fund Management, Member of E Fund Fintech Executive Committee, Senior FinTech expert. \"),_c('br'),_c('br'),_c('span',[_vm._v(\"Qian Xu: \")]),_vm._v(\" PhD, HKUST (2011), Adjunct Associate Professor at HKUST(GZ). Research interests: AI, transfer learning, federated learning, NLP, LLMs. Experience at Baidu Inc. & Webank Inc. Executive Director of GSAIR & Founding Member of GBA Society of AI and Robotics. \")])]),_c('h1',[_vm._v(\"The Main Conference Information\")]),_c('div',{staticClass:\"cente-text mg-bottom\"},[_c('div',{staticClass:\"cente-text-cente\"},[_vm._v(\" IJCAI 2023, \"),_c('br'),_vm._v(\"THE 32nd INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE. \"),_c('br'),_vm._v(\"19th-25th August 2023 \"),_c('br'),_vm._v(\"Macao, S.A.R \"),_c('br'),_c('a',{attrs:{\"href\":\"https://ijcai-23.org/\"}},[_vm._v(\"https://ijcai-23.org/\")])])])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"footer padding\"},[_c('div',[_c('div',{staticClass:\"email-icon\"}),_c('div',{staticClass:\"email-info\"},[_c('p',[_vm._v(\"If you have any question, feel free to contact us:\")]),_c('p',{staticStyle:{\"color\":\"#b3ebff\"}},[_vm._v(\"finllm2023@easychair.org\")])])])])}]\n\nexport { render, staticRenderFns }","<template>\r\n  <div class=\"home\">\r\n    <div style=\"display: none\">\r\n      <img\r\n        src=\"@/assets/img/logo.jpg\"\r\n        alt=\"\"\r\n      />\r\n    </div>\r\n    <div class=\"header\">\r\n      <div class=\"w padding flex\">\r\n        <span>FinLLM 2023</span>\r\n        <div class=\"right\">\r\n          <img\r\n            src=\"@/assets/img/ijcai.png\"\r\n            alt=\"\"\r\n          />\r\n        </div>\r\n      </div>\r\n    </div>\r\n    <div class=\"subtitle padding w\">\r\n      <div class=\"sub-text\">\r\n        International Symposium on <br />Large Language Models for Financial Services<br />\r\n        (FinLLM 2023)@IJCAI 2023\r\n      </div>\r\n      <button\r\n        class=\"registration\"\r\n        @click=\"$router.push('/fcb')\"\r\n      >\r\n        List of Awards\r\n      </button>\r\n      <!-- <div class=\"sub-img\">\r\n        <img\r\n          src=\"@/assets/img/subImg.png\"\r\n          alt=\"\"\r\n        />\r\n      </div> -->\r\n    </div>\r\n    <div class=\"center-box w padding\">\r\n      <div class=\"center-img-box\">\r\n        <div class=\"center-text-box\">\r\n          <div class=\"center-text-content\">\r\n            <div class=\"center-text-top\">\r\n              <span class=\"center-text-title\">Symposium Date: <br /></span>\r\n              <span class=\"center-text-body\">August 20, 2023 <br /></span>\r\n            </div>\r\n            <span class=\"center-text-title\">Venue: <br /></span>\r\n            <span class=\"center-text-body\">Sheraton Grand Macao Hotel,Macau</span>\r\n          </div>\r\n        </div>\r\n        <img\r\n          class=\"center-img\"\r\n          src=\"@/assets/img/hotel.jpg\"\r\n        />\r\n      </div>\r\n      <!-- <h1>Committee</h1> -->\r\n      <h1>Scope and Objective</h1>\r\n      <div class=\"cente-text mg-bottom\">\r\n        Pretrained large language models (LLMs) have demonstrated tremendous potential in various natural language\r\n        processing tasks, including language generation, machine translation, and question answering. In the financial\r\n        services industry, pretrained LLMs have the potential to significantly impact tasks such as financial\r\n        forecasting, risk management, and sentiment analysis. Moreover, LLMs can help automate the process of analyzing\r\n        financial reports and extracting key insights that can aid businesses in making informed decisions.<br /><br />\r\n        This symposium provides a platform for researchers, practitioners, and industry experts from around the world to\r\n        share new ideas, exchange research findings, and discuss the challenges and opportunities in the field of\r\n        pretrained LLMs for financial services. The symposium will cover two themes: 1) potential applications and best\r\n        practices of pretrained LLMs for financial services; and 2) challenges that need to be addressed to make them\r\n        efficient, effective, and trustworthy. <br /><br />\r\n        The symposium will also feature invited talks by leading researchers and industry experts, as well as panel\r\n        discussions on the latest trends and challenges in the field. We welcome researchers, practitioners, and\r\n        industry experts from academia and industry to submit their work and participate in this exciting event.\r\n      </div>\r\n      <h1>Call for Papers</h1>\r\n      <div class=\"cente-text mg-bottom\">\r\n        <div class=\"cente-text-title\">\r\n          We welcome submissions on recent advances and applications of large language models (LLMs) in financial\r\n          services. We encourage submissions that cover a wide range of topics, including but not limited to:\r\n        </div>\r\n        <div class=\"cente-text-cente flex\">\r\n          <div class=\"text\">\r\n            <span>Techniques: </span>\r\n            <br />-Multimodal modeling of financial data using LLMs <br />-Preprocessing and cleaning of financial data\r\n            for use with LLMs <br />-Integration of LLMs with other AI technologies in financial services <br />-Novel\r\n            architectures and training techniques for LLMs in financial services. <br />-Scalability and efficiency of\r\n            LLMs in financial services <br />-Cross-lingual and multilingual LLMs in financial services\r\n            <br />-Human-in-the-loop approaches for LLMs in financial services <br /><br /><span>Applications:</span\r\n            ><br />\r\n            -Financial forecasting using LLMs <br />-Sentiment analysis and opinion mining for financial data\r\n            <br />-LLM-based trading algorithms and decision-making systems <br />-Analysis of financial news and social\r\n            media using LLMs <br />-Semantic analysis of financial reports and filings <br />-Explainable AI in\r\n            financial services using LLMs <br />-Transfer learning and domain adaptation for LLMs in financial services\r\n            <br />-Case studies and success stories of LLMs in financial services <br /><br /><span>Challenges:</span\r\n            ><br />\r\n            -Evaluation of LLMs for financial services <br />-Social economics and trustworthiness for LLMs in financial\r\n            services <br />-Ethical and legal considerations in the use of LLMs in financial services <br />-Privacy and\r\n            security concerns in the use of LLMs for financial data <br />-Bias and fairness considerations in the use\r\n            of LLMs for financial services\r\n          </div>\r\n          <div class=\"img-box\">\r\n            <img\r\n              src=\"@/assets/img/IMG.png\"\r\n              alt=\"\"\r\n            />\r\n          </div>\r\n        </div>\r\n      </div>\r\n      <h1>Important Dates</h1>\r\n      <div class=\"card-box mg-bottom\">\r\n        <div class=\"card\">\r\n          <p class=\"title\">Submission Due:</p>\r\n          <p class=\"timer\">\r\n            <span class=\"month\">June 20, 2023</span>\r\n            <span class=\"date\">(23:59:59 AoE)</span>\r\n          </p>\r\n        </div>\r\n        <div class=\"card\">\r\n          <p class=\"title\">Notification Due:</p>\r\n          <p class=\"timer\">\r\n            <span class=\"month\">July 4, 2023</span>\r\n            <span class=\"date\">(23:59:59 AoE)</span>\r\n          </p>\r\n        </div>\r\n        <div class=\"card\">\r\n          <p class=\"title\">Final Version Due:</p>\r\n          <p class=\"timer\">\r\n            <span class=\"month\">August 3, 2023</span>\r\n            <span class=\"date\">(23:59:59 AoE)</span>\r\n          </p>\r\n        </div>\r\n        <div class=\"card\">\r\n          <p class=\"title\">Symposium Date:</p>\r\n          <p class=\"timer\">\r\n            <span class=\"month\">August 20, 2023</span>\r\n          </p>\r\n        </div>\r\n      </div>\r\n      <h1>Submission Instructions</h1>\r\n      <div class=\"cente-text mg-bottom\">\r\n        <div class=\"cente-text-cente justify\">\r\n          Submissions should be a maximum of 7 and a minimum of 4 pages including figures and tables in IJCAI'23 format.\r\n          Additional pages containing only cited references are allowed. We do accept submissions of work recently\r\n          published or currently under review. The submissions can contain author details. The Symposium will not have\r\n          formal proceedings, but authors of accepted papers can choose to have their work published on the Symposium\r\n          webpage. Top selected papers from the Symposium will be invited to publish in a journal special issue.\r\n          <br /><br />\r\n\r\n          We recommend submitting your paper through EasyChair. The paper submission link is as follows:<br />\r\n          <a href=\"https://easychair.org/conferences/?conf=finllm2023\">\r\n            https://easychair.org/conferences/?conf=finllm2023</a\r\n          >\r\n        </div>\r\n      </div>\r\n    </div>\r\n    <div class=\"middle\">\r\n      <div class=\"w padding\">\r\n        <h1>Tentative Schedule</h1>\r\n        <div>\r\n          <div class=\"middle-mg\">\r\n            <span>Morning:</span>\r\n            <br />• 08:30 - 09:00 Registeration<br />• 09:00 - 10:00 Invited talks <br />• 10:00 - 10:45 Oral\r\n            presentations <br />• 10:45 - 11:00 Break <br />• 11:00 - 11:45 Oral presentations\r\n          </div>\r\n          <div>\r\n            <span>Afternoon:</span>\r\n            <br />• 13:00 - 14:30 Invited talks <br />• 14:30 - 15:15 Oral presentations <br />• 15:15 - 15:30 Break\r\n            <br />• 15:30 - 16:30 Panel Discussion <br />• 16:30 - 17:00 Award Ceremony\r\n          </div>\r\n        </div>\r\n      </div>\r\n    </div>\r\n    <div class=\"center-box w padding\">\r\n      <h1>Organization</h1>\r\n      <div class=\"item-box\">\r\n        <!-- <div class=\"item-box-left\">\r\n          <p>Symposium Organizers:</p>\r\n          <div>\r\n            -Shuoling Liu (E Fund) <br />-Xueyang Wu (Flaiverse) <br />-Yongpeng Tang (E Fund) <br />-Qian Xu (HKUST)\r\n            <br />-Liyuan Chen (E Fund) <br />-Qiang Yang (WeBank/HKUST, Hong Kong)\r\n          </div>\r\n        </div> -->\r\n        <div class=\"item-box-right\">\r\n          <p>Invited Speakers</p>\r\n          <div>\r\n            <div class=\"info-box\">\r\n              <div class=\"avt\">\r\n                <img\r\n                  src=\"@/assets/img/Mark Dredze.png\"\r\n                  alt=\"\"\r\n                />\r\n              </div>\r\n              <div class=\"info\">\r\n                <p class=\"name\">Mark Dredze(online)</p>\r\n                <p class=\"note\">Bloomberg</p>\r\n              </div>\r\n            </div>\r\n            <div class=\"info-box\">\r\n              <div class=\"avt\">\r\n                <img\r\n                  src=\"@/assets/img/白硕.jpg\"\r\n                  alt=\"\"\r\n                />\r\n              </div>\r\n              <div class=\"info\">\r\n                <p class=\"name\">Shuo Bai</p>\r\n                <p class=\"note\">Hundsun Technologies</p>\r\n              </div>\r\n            </div>\r\n            <div class=\"info-box\">\r\n              <div class=\"avt\">\r\n                <img\r\n                  src=\"@/assets/img/王巍巍.jpg\"\r\n                  alt=\"\"\r\n                />\r\n              </div>\r\n              <div class=\"info\">\r\n                <p class=\"name\">Weiwei Wang</p>\r\n                <p class=\"note\">Alibaba DAMO Academy</p>\r\n              </div>\r\n            </div>\r\n            <div class=\"info-box\">\r\n              <div class=\"avt\">\r\n                <img\r\n                  src=\"@/assets/img/皱胜.jpg\"\r\n                  alt=\"\"\r\n                />\r\n              </div>\r\n              <div class=\"info\">\r\n                <p class=\"name\">Sheng Zou</p>\r\n                <p class=\"note\">Archforce Technology</p>\r\n              </div>\r\n            </div>\r\n            <!-- 1.Shijie Wu (Bloomberg) <br />2.Bai Shuo (Chief Senior Technical Advisor of the Intelligent Investment\r\n            Research Technology) <br />3.Zhong Yuyang (Chief Information Officer of China Southern Asset Management)\r\n            <br />4.Sun Jianing (General Manager of both the Quantitative Investment Department and Equity Research\r\n            Department at Guotai Junan Securities Asset Management) <br />5.Jinlong Li (Director of the AI Laboratory of\r\n            China Merchants Bank) <br />6.Jie Tang (Tsinghua University) <br />7.Xiaoyan Liu (President of E Fund)\r\n            <br />8.Kar Yan Tam (Dean of HKUST Business School) <br />9.Sau San Chan (Chairman of the Monetary Authority\r\n            of Macao) <br />10.Eddie Yue (Chief Executive of the Hong Kong Monetary Authority) <br />11.Jie He (Head of\r\n            the Shenzhen Local Financial Supervision and Administration Bureau) <br />12.Ravi Menon (Managing Director\r\n            of the Monetary Authority of Singapore) -->\r\n          </div>\r\n        </div>\r\n      </div>\r\n      <!-- <p class=\"h1-title\">Tentative Program Committee:</p>\r\n      <div class=\"item-box mg-bottom\">\r\n        <div class=\"item-box-left bg-none media-bottom\">\r\n          <div>\r\n            1.Adria Gascon (The Alan Turing Institute / University of Warwick) <br />2.Anis Elgabli (University of\r\n            Oulu)<br />\r\n            3.Aurélien Bellet (Inria) <br />4.Ayfer Ozgur (Stanford University) <br />5.Bai Shuo (Hengsheng Electronics\r\n            Co Ltd.)<br />\r\n            6.Bingsheng He (National University of Singapore) <br />7.Boi Faltings (Ecole Polytechnique Fédérale de\r\n            Lausanne)<br />\r\n            8.Chaoping Xing (Nanyang Technological University) <br />9.Chaoyang He (University of Southern\r\n            California)<br />\r\n            10.Dimitrios Papadopoulos (Hong Kong University of Science and Technology) <br />11.Fabio Casati (University\r\n            of Trento) <br />12.Farinaz Koushanfar (University of California San Diego) <br />13.Fangkai Tang (E Fund)\r\n            <br />14.Gauri Joshi (Carnegie Mellon University) <br />15.Graham Cormode (University of Warwick)\r\n            <br />16.Helen (Hai) Li (Duke University)<br />\r\n            17.Jalaj Upadhyay (Apple) <br />18.Ji Feng (Sinnovation Ventures AI Institute) <br />19.Jian Liu (Ping An\r\n            Asset Management) <br />20.Jianshu Weng (Swiss Re) <br />21.Jihong Park (University of Oulu) <br />22.Joshua\r\n            Gardner (University of Michigan) <br />23.Jun Zhao (Nanyang Technological University) <br />24.Junyang Li (E\r\n            Fund)\r\n          </div>\r\n        </div>\r\n        <div class=\"item-box-right\">\r\n          <div>\r\n            25.Lalitha Sankar (Arizona State University) <br />26.Leye Wang (Peking University) <br />27.Marco Gruteser\r\n            (Google)<br />\r\n            28.Martin Jaggi (Ecole Polytechnique Fédérale de Lausanne) <br />29.Mehdi Bennis (University of Oulu)\r\n            <br />30.Mingshu Cong (The University of Hong Kong) <br />31.Nguyen Tran (The University of Sydney)\r\n            <br />32.Ning Cheng (E Fund)<br />\r\n            33.Pingzhong Tang (Tsinghua University) <br />34.Praneeth Vepakomma (MIT) <br />35.Prateek Mittal (Princeton\r\n            University)<br />\r\n            36.Rui Lin (Chalmers University of Technology) <br />37.Sewoong Oh (University of Illinois at\r\n            Urbana-Champaign)<br />\r\n            38.Shiqiang Wang (IBM) <br />39.Siwei Feng (Nanyang Technological University) <br />40.Tara Javidi\r\n            (University of California San Diego) <br />41.Xiaoyu Wang (E Fund) <br />42.Yihan Jiang (University of\r\n            Washington) <br />43.Yongxin Tong (Beihang University) <br />44.Tongzhe Zhang (E Fund) <br />45.Yuxiang Wang\r\n            (E Fund) <br />46.Zelei Liu (Nanyang Technological University) <br />47.Zichen Chen (Nanyang Technological\r\n            University) <br />48.Yuanyuan Chen (Nanyang Technological University) <br />49.Zheng Xu (University of\r\n            Science and Technology of China) <br />50.Zhengfei Li (E Fund)\r\n          </div>\r\n        </div>\r\n      </div> -->\r\n      <p><br /></p>\r\n      <h1>Organizers of Organizing Committee</h1>\r\n      <div class=\"cente-text mg-bottom\">\r\n        <div class=\"centet-people\">\r\n          <div class=\"left-title\">General Chair</div>\r\n          <div class=\"people-box\">\r\n            <div class=\"info-box\">\r\n              <div class=\"avt\">\r\n                <img\r\n                  src=\"@/assets/img/7.png\"\r\n                  alt=\"\"\r\n                />\r\n              </div>\r\n              <div class=\"info\">\r\n                <p class=\"name\">Qiang Yang</p>\r\n                <p class=\"note\">WeBank/HKUST</p>\r\n              </div>\r\n            </div>\r\n            <div class=\"info-box\">\r\n              <div class=\"avt\">\r\n                <img\r\n                  src=\"@/assets/img/9.png\"\r\n                  alt=\"\"\r\n                />\r\n              </div>\r\n              <div class=\"info\">\r\n                <p class=\"name\">Liyuan Chen</p>\r\n                <p class=\"note\">E Fund</p>\r\n              </div>\r\n            </div>\r\n          </div>\r\n        </div>\r\n        <div class=\"centet-people\">\r\n          <div\r\n            class=\"left-title\"\r\n            style=\"text-align: left\"\r\n          >\r\n            Chairman of the Program Committee\r\n          </div>\r\n          <div class=\"people-box\">\r\n            <div class=\"info-box\">\r\n              <div class=\"avt\">\r\n                <img\r\n                  src=\"@/assets/img/6.png\"\r\n                  alt=\"\"\r\n                />\r\n              </div>\r\n              <div class=\"info\">\r\n                <p class=\"name\">Shuoling Liu</p>\r\n                <p class=\"note\">E Fund</p>\r\n              </div>\r\n            </div>\r\n          </div>\r\n        </div>\r\n        <div class=\"centet-people\">\r\n          <div class=\"left-title\">Program Chair</div>\r\n          <div class=\"people-box\">\r\n            <!-- <div class=\"info-box\">\r\n              <div class=\"avt\">\r\n                <img\r\n                  src=\"@/assets/img/6.png\"\r\n                  alt=\"\"\r\n                />\r\n              </div>\r\n              <div class=\"info\">\r\n                <p class=\"name\">Shuoling Liu</p>\r\n                <p class=\"note\">E Fund</p>\r\n              </div>\r\n            </div> -->\r\n            <div class=\"info-box\">\r\n              <div class=\"avt\">\r\n                <img\r\n                  src=\"@/assets/img/xueyangWu.png\"\r\n                  alt=\"\"\r\n                />\r\n              </div>\r\n              <div class=\"info\">\r\n                <p class=\"name\">Xueyang Wu</p>\r\n                <p class=\"note\">Flaiverse</p>\r\n              </div>\r\n            </div>\r\n            <div class=\"info-box\">\r\n              <div class=\"avt\">\r\n                <img\r\n                  src=\"@/assets/img/yongpeng tang.png\"\r\n                  alt=\"\"\r\n                />\r\n              </div>\r\n              <div class=\"info\">\r\n                <p class=\"name\">Yongpeng Tang</p>\r\n                <p class=\"note\">E Fund</p>\r\n              </div>\r\n            </div>\r\n            <div class=\"info-box\">\r\n              <div class=\"avt\">\r\n                <img\r\n                  src=\"@/assets/img/qianxu.png\"\r\n                  alt=\"\"\r\n                />\r\n              </div>\r\n              <div class=\"info\">\r\n                <p class=\"name\">Qian Xu</p>\r\n                <p class=\"note\">HKUST</p>\r\n              </div>\r\n            </div>\r\n          </div>\r\n        </div>\r\n      </div>\r\n      <div class=\"cente-text mg-bottom\">\r\n        <div class=\"cente-text-cente justify\">\r\n          <span>Qiang Yang: </span>\r\n          Chief AI Officer of WeBank, Department Head of Computer Science and Engineering at the Hong Kong University of\r\n          Science and Technology, New Bright Professorship in Engineering. AAAI Fellow, IEEE Fellow, AAAS Fellow, IAPR\r\n          Fellow and ACM Distinguished Members. President of IJCAI (2017-2019). Vice Chair of ACM SIGART (SIGAI)\r\n          (2009-2012), PC Chair and Co-Chair: IJCAI(2015), ACM KDD(2010).<br />\r\n          Google Scholar: <br /><a href=\"https://scholar.google.com/citations?user=1LxWZLQAAAAJ\"\r\n            >https://scholar.google.com/citations?user=1LxWZLQAAAAJ</a\r\n          ><br /><br />\r\n          <span>Liyuan Chen: </span>\r\n          Chief Operating Officer of E Fund Management, Chairman of the E Fund FinTech Executive Committee.<br /><br />\r\n          <span>Shuoling Liu: </span>\r\n          Head of Institute of Innovation at E Fund Management, Secretary-General of E Fund FinTech Executive Committee,\r\n          Member of the FinTech Special Committee of AMAC. <br /><br />\r\n          <span>Xueyang Wu: </span>\r\n          PhD, HKUST (2023), Co-founder & Principal Scientist at Flaiverse. Research interests: transfer learning,\r\n          federated learning, NLP, LLMs. Publications: research papers & patents. Reviewer: NeurIPS, KDD, ICML, ICLR,\r\n          etc. <br /><br />\r\n          <span>Yongpeng Tang: </span>\r\n          MD, Head of Application Development at E Fund Management, Member of E Fund Fintech Executive Committee, Senior\r\n          FinTech expert. <br /><br />\r\n          <span>Qian Xu: </span>\r\n          PhD, HKUST (2011), Adjunct Associate Professor at HKUST(GZ). Research interests: AI, transfer learning,\r\n          federated learning, NLP, LLMs. Experience at Baidu Inc. & Webank Inc. Executive Director of GSAIR & Founding\r\n          Member of GBA Society of AI and Robotics.\r\n        </div>\r\n      </div>\r\n      <h1>The Main Conference Information</h1>\r\n      <div class=\"cente-text mg-bottom\">\r\n        <div class=\"cente-text-cente\">\r\n          IJCAI 2023, <br />THE 32nd INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE. <br />19th-25th August\r\n          2023 <br />Macao, S.A.R <br />\r\n          <a href=\"https://ijcai-23.org/\">https://ijcai-23.org/</a>\r\n        </div>\r\n      </div>\r\n    </div>\r\n    <div class=\"footer padding\">\r\n      <div>\r\n        <div class=\"email-icon\"></div>\r\n        <div class=\"email-info\">\r\n          <p>If you have any question, feel free to contact us:</p>\r\n          <p style=\"color: #b3ebff\">finllm2023@easychair.org</p>\r\n        </div>\r\n      </div>\r\n    </div>\r\n  </div>\r\n</template>\r\n\r\n<script>\r\nexport default {}\r\n</script>\r\n\r\n<style lang=\"less\" scoped>\r\n@import '../../style/public.less';\r\n.padding {\r\n  padding: 0 130px;\r\n}\r\n.w {\r\n  min-width: 1715px;\r\n  max-width: 1920px;\r\n  margin: 0 auto;\r\n}\r\n.mg-bottom {\r\n  margin-bottom: 50px;\r\n}\r\n.flex {\r\n  display: flex;\r\n  align-items: center;\r\n  justify-content: space-between;\r\n}\r\n.header,\r\n.footer {\r\n  min-width: 1715px;\r\n  height: 103px;\r\n  line-height: 103px;\r\n  font-size: 28px;\r\n  color: #ffffff;\r\n  letter-spacing: 0;\r\n  font-weight: 700;\r\n  background: url('@/assets/img/CTA.png') no-repeat;\r\n  background-size: cover;\r\n  .right {\r\n    width: 172px;\r\n    height: 64px;\r\n    @media (max-width: 750px) {\r\n      width: 300px;\r\n      height: 120px;\r\n    }\r\n    img {\r\n      width: 100%;\r\n      height: 100%;\r\n      vertical-align: top;\r\n    }\r\n  }\r\n}\r\n.footer {\r\n  height: 151px;\r\n  margin-top: 100px;\r\n  font-size: 20px;\r\n  color: #ffffff;\r\n  font-weight: 500;\r\n  display: flex;\r\n  align-items: center;\r\n  & > div {\r\n    display: flex;\r\n    .email-icon {\r\n      width: 30px;\r\n      height: 30px;\r\n      margin-right: 9px;\r\n      background: url('@/assets/img/email.png') no-repeat;\r\n      background-size: cover;\r\n    }\r\n    p {\r\n      margin: 0;\r\n      line-height: 1.5;\r\n    }\r\n  }\r\n}\r\n.subtitle {\r\n  background: url('@/assets/img/header-bg.jpg') no-repeat;\r\n  height: 614px;\r\n  box-sizing: border-box;\r\n  background-size: 100% 100%;\r\n  // display: flex;\r\n  // justify-content: space-between;\r\n  padding-top: 144px;\r\n  @media (max-width: 750px) {\r\n    height: 1200px;\r\n  }\r\n  .sub-text {\r\n    font-size: 43px;\r\n    color: #ffffff;\r\n    line-height: 60px;\r\n    font-weight: 700;\r\n    margin-top: 50px;\r\n    @media (max-width: 750px) {\r\n      font-size: 80px;\r\n      line-height: 1.5;\r\n    }\r\n  }\r\n  .sub-img {\r\n    width: 300px;\r\n    height: 434px;\r\n    flex-shrink: 0;\r\n    @media (max-width: 750px) {\r\n      width: 330px;\r\n    }\r\n    img {\r\n      width: 100%;\r\n      height: 100%;\r\n    }\r\n  }\r\n  .registration {\r\n    width: 150px;\r\n    height: 50px;\r\n    background-color: #4270f1;\r\n    border: none;\r\n    color: #ffffff;\r\n    margin-top: 20px;\r\n    font-size: 16px;\r\n    cursor: pointer;\r\n    @media (max-width: 750px) {\r\n      width: 300px;\r\n      height: 100px;\r\n      font-size: 28px;\r\n    }\r\n  }\r\n}\r\n.center-box {\r\n  h1 {\r\n    font-size: 48px;\r\n    color: #0d152e;\r\n    line-height: 60px;\r\n    font-weight: 700;\r\n    &::before {\r\n      content: '';\r\n      display: inline-block;\r\n      width: 10px;\r\n      height: 40px;\r\n      background-color: #4774f4;\r\n      margin-right: 10px;\r\n      vertical-align: baseline;\r\n    }\r\n  }\r\n\r\n  .center-img-box {\r\n    display: flex;\r\n    margin-top: 72px;\r\n    margin-bottom: 32px;\r\n    width: 100%;\r\n    background-color: #299bff;\r\n    @media (max-width: 750px) {\r\n      flex-direction: column;\r\n      border-radius: 20px;\r\n      padding: 80px 0;\r\n    }\r\n\r\n    .center-text-box {\r\n      border-color: transparent;\r\n      color: #ffffff;\r\n      border-radius: 15px 0 0 15px;\r\n      width: 40%;\r\n      display: flex;\r\n      justify-content: center;\r\n      align-items: center;\r\n      @media (max-width: 750px) {\r\n        width: 100%;\r\n        justify-content: start;\r\n        padding-left: 30px;\r\n        font-size: 42px;\r\n      }\r\n\r\n      .center-text-top {\r\n        margin-bottom: 12px;\r\n        @media (max-width: 750px) {\r\n          margin-bottom: 20px;\r\n        }\r\n      }\r\n\r\n      .center-text-title {\r\n        font-weight: 600;\r\n        font-size: 32px;\r\n      }\r\n\r\n      .center-text-body {\r\n        font-weight: 400;\r\n        font-size: 24px;\r\n      }\r\n    }\r\n\r\n    @media (max-width: 750px) {\r\n      margin-top: -250px;\r\n      .center-text-content {\r\n        margin-left: 24px;\r\n        @media (max-width: 750px) {\r\n          span {\r\n            font-size: 50px !important;\r\n          }\r\n        }\r\n      }\r\n    }\r\n\r\n    .center-img {\r\n      flex-grow: 1;\r\n      border-radius: 0 15px 15px 0;\r\n      border-left: 1px solid #20b9e1;\r\n      height: 320px;\r\n      @media (max-width: 750px) {\r\n        width: 100%;\r\n        height: auto;\r\n        margin-top: 30px;\r\n      }\r\n    }\r\n  }\r\n  .cente-text {\r\n    font-size: 20px;\r\n    color: #81838c;\r\n    text-align: justify;\r\n    line-height: 34px;\r\n    font-weight: 500;\r\n    .centet-people {\r\n      display: flex;\r\n      align-items: center;\r\n      margin-top: 40px;\r\n      @media (max-width: 750px) {\r\n        display: block;\r\n      }\r\n      .left-title {\r\n        font-size: 24px;\r\n        color: #0078ce;\r\n        font-weight: 700;\r\n        width: 280px;\r\n        @media (max-width: 750px) {\r\n          width: auto;\r\n          font-size: 42px;\r\n          margin-bottom: 20px;\r\n        }\r\n      }\r\n      .people-box {\r\n        flex: 1;\r\n        display: flex;\r\n        align-items: center;\r\n        flex-wrap: wrap;\r\n        .info-box {\r\n          display: flex;\r\n          align-items: center;\r\n          width: 25%;\r\n          @media (max-width: 750px) {\r\n            width: 50%;\r\n            margin-bottom: 40px;\r\n          }\r\n          .avt {\r\n            width: 90px;\r\n            height: 90px;\r\n            margin-right: 20px;\r\n            background: rgba(95, 95, 95, 0.16);\r\n            border-radius: 50%;\r\n            // border: 1px solid #e2e2e2;\r\n            overflow: hidden;\r\n            @media (max-width: 750px) {\r\n              width: 180px;\r\n              height: 180px;\r\n            }\r\n            img {\r\n              width: 100%;\r\n              height: 100%;\r\n            }\r\n          }\r\n          .xuqian {\r\n            width: 100px;\r\n            height: 90px;\r\n          }\r\n          @media (max-width: 750px) {\r\n            .xuqian {\r\n              width: 200px;\r\n              height: 180px;\r\n            }\r\n          }\r\n          .info {\r\n            p {\r\n              margin: 0;\r\n            }\r\n            .name {\r\n              font-size: 19.8px;\r\n              color: #292f36;\r\n              font-weight: 700;\r\n              @media (max-width: 750px) {\r\n                font-size: 42px;\r\n              }\r\n            }\r\n            .note {\r\n              font-size: 16.2px;\r\n              color: #292f36;\r\n              font-weight: 500;\r\n              text-align: left;\r\n              @media (max-width: 750px) {\r\n                font-size: 32px;\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n    .cente-text-title {\r\n      font-size: 26px;\r\n      font-weight: 700;\r\n      margin-bottom: 34px;\r\n    }\r\n    .cente-text-cente {\r\n      text-align: left;\r\n      span {\r\n        font-size: 26px;\r\n        font-weight: 700;\r\n      }\r\n      .img-box {\r\n        width: 620px;\r\n        @media (max-width: 750px) {\r\n          display: none;\r\n        }\r\n        img {\r\n          width: 100%;\r\n          height: 100%;\r\n        }\r\n      }\r\n    }\r\n    .justify {\r\n      text-align: justify;\r\n    }\r\n    .cente-footer {\r\n      margin-top: 34px;\r\n    }\r\n    .p-text {\r\n      font-size: 26px;\r\n      font-weight: 700;\r\n    }\r\n  }\r\n  .card-box {\r\n    display: flex;\r\n    justify-content: space-between;\r\n    .card {\r\n      width: 285px;\r\n      height: 156px;\r\n      background: #f7f9fb;\r\n      border-radius: 23px;\r\n      display: flex;\r\n      flex-direction: column;\r\n      align-items: center;\r\n      justify-content: center;\r\n      p {\r\n        margin: 0;\r\n      }\r\n      .title {\r\n        font-size: 20px;\r\n        color: #81838c;\r\n        line-height: 33px;\r\n        font-weight: 700;\r\n      }\r\n      .timer {\r\n        color: #81838c;\r\n        .month {\r\n          font-size: 20px;\r\n          font-weight: 500;\r\n        }\r\n        .date {\r\n          font-size: 16px;\r\n          font-weight: 500;\r\n        }\r\n      }\r\n    }\r\n  }\r\n  .item-box {\r\n    display: flex;\r\n    justify-content: space-between;\r\n    font-size: 20px;\r\n    color: #81838c;\r\n    text-align: justify;\r\n    line-height: 34px;\r\n    font-weight: 500;\r\n    .item-box-left {\r\n      width: 50%;\r\n      background: url('@/assets/img/IMG.png') no-repeat;\r\n      background-size: contain;\r\n      text-align: left;\r\n      p {\r\n        font-size: 26px;\r\n        font-weight: 700;\r\n      }\r\n    }\r\n    .item-box-right {\r\n      width: 100%;\r\n      display: flex;\r\n      align-items: center;\r\n      justify-content: space-between;\r\n      & > p {\r\n        width: 280px;\r\n        @media (max-width: 750px) {\r\n          font-size: 42px !important;\r\n          width: auto;\r\n        }\r\n      }\r\n      @media (max-width: 750px) {\r\n        display: block;\r\n      }\r\n      & > div {\r\n        display: flex;\r\n        flex: 1;\r\n        justify-content: space-between;\r\n        // margin-left: 120px;\r\n        @media (max-width: 750px) {\r\n          // margin-left: 40px;\r\n          flex-wrap: wrap;\r\n          margin-top: 40px;\r\n        }\r\n        .info-box {\r\n          display: flex;\r\n          align-items: center;\r\n          width: 25%;\r\n          @media (max-width: 750px) {\r\n            width: 50%;\r\n            margin-bottom: 40px;\r\n          }\r\n          .avt {\r\n            width: 90px;\r\n            height: 90px;\r\n            margin-right: 20px;\r\n            background: rgba(95, 95, 95, 0.16);\r\n            border-radius: 50%;\r\n            // border: 1px solid #e2e2e2;\r\n            overflow: hidden;\r\n            @media (max-width: 750px) {\r\n              width: 180px;\r\n              height: 180px;\r\n            }\r\n            img {\r\n              width: 100%;\r\n              height: 100%;\r\n            }\r\n          }\r\n          .xuqian {\r\n            width: 100px;\r\n            height: 90px;\r\n          }\r\n          @media (max-width: 750px) {\r\n            .xuqian {\r\n              width: 200px;\r\n              height: 180px;\r\n            }\r\n          }\r\n          .info {\r\n            flex: 1;\r\n            p {\r\n              margin: 0;\r\n            }\r\n            .name {\r\n              font-size: 19.8px;\r\n              color: #292f36;\r\n              font-weight: 700;\r\n              @media (max-width: 750px) {\r\n                font-size: 42px;\r\n              }\r\n            }\r\n            .note {\r\n              font-size: 16.2px;\r\n              color: #292f36;\r\n              font-weight: 500;\r\n              text-align: left;\r\n              @media (max-width: 750px) {\r\n                font-size: 32px;\r\n              }\r\n            }\r\n          }\r\n        }\r\n        // & > div {\r\n        //   width: 90px;\r\n        //   // background: url('@/assets/img/rect.png') no-repeat;\r\n        //   // background-size: cover;\r\n        //   // display: flex;\r\n        //   // align-items: center;\r\n        //   display: flex;\r\n        //   align-items: center;\r\n        //   color: #ffffff;\r\n        //   // overflow: hidden;\r\n        //   @media (max-width: 750px) {\r\n        //     width: 180px;\r\n        //   }\r\n        //   & > div {\r\n        //     font-size: 26px;\r\n        //     color: #292f36;\r\n        //     font-weight: 600;\r\n        //     text-align: center;\r\n        //     &.img-box {\r\n        //       // height: 90px;\r\n        //       border-radius: 50%;\r\n        //       overflow: hidden;\r\n        //       margin-bottom: 10px;\r\n        //       // @media (max-width: 750px) {\r\n        //       //   height: 180px;\r\n        //       // }\r\n        //     }\r\n        //     @media (max-width: 750px) {\r\n        //       font-size: 42px;\r\n        //     }\r\n        //   }\r\n        //   img {\r\n        //     width: 100%;\r\n        //     height: 100%;\r\n        //   }\r\n        //   @media (max-width: 750px) {\r\n        //     width: 180px;\r\n        //     height: 180px;\r\n        //     margin-top: 40px;\r\n        //   }\r\n        //   @media (max-width: 375px) {\r\n        //     width: 200px;\r\n        //     height: 200px;\r\n        //     margin-top: 40px;\r\n        //   }\r\n        // }\r\n        // @media (max-width: 750px) {\r\n        //   .isH5 {\r\n        //     display: none;\r\n        //   }\r\n        // }\r\n      }\r\n      p {\r\n        font-size: 26px;\r\n        font-weight: 700;\r\n        margin: 0;\r\n        color: #0078ce;\r\n      }\r\n    }\r\n    .bg-none {\r\n      background: none;\r\n    }\r\n  }\r\n  .h1-title {\r\n    margin-top: 50px;\r\n    font-size: 26px;\r\n    color: #81838c;\r\n    line-height: 34px;\r\n    font-weight: 700;\r\n  }\r\n}\r\n.middle {\r\n  min-width: 1715px;\r\n  height: 544px;\r\n  background: url('@/assets/img/CTA.png') no-repeat;\r\n  background-size: cover;\r\n  padding-top: 72px;\r\n  margin-bottom: 50px;\r\n  & > div {\r\n    h1 {\r\n      font-size: 48px;\r\n      color: #ffffff;\r\n      line-height: 60px;\r\n      font-weight: 700;\r\n      margin-bottom: 50px;\r\n      &::before {\r\n        content: '';\r\n        display: inline-block;\r\n        width: 10px;\r\n        height: 40px;\r\n        background-color: #ffffff;\r\n        margin-right: 10px;\r\n        vertical-align: baseline;\r\n      }\r\n    }\r\n    & > div {\r\n      display: flex;\r\n      & > div {\r\n        font-size: 27px;\r\n        color: #ffffff;\r\n        line-height: 45.9px;\r\n        font-weight: 500;\r\n        span {\r\n          font-size: 35px;\r\n          font-weight: 700;\r\n        }\r\n      }\r\n    }\r\n    .middle-mg {\r\n      margin-right: 309px;\r\n    }\r\n  }\r\n}\r\n@media (max-width: 750px) {\r\n  .header,\r\n  .footer,\r\n  .middle {\r\n    min-width: 375px;\r\n  }\r\n  .header {\r\n    height: 200px;\r\n    line-height: 200px;\r\n    font-size: 80px;\r\n  }\r\n  .w {\r\n    min-width: 100vw;\r\n    max-width: 100vw;\r\n  }\r\n  .center-box {\r\n    .cente-text {\r\n      line-height: 50px;\r\n    }\r\n    .item-box {\r\n      line-height: 60px;\r\n      display: block;\r\n      .item-box-left,\r\n      .item-box-right {\r\n        width: 100%;\r\n        text-align: left;\r\n      }\r\n      .item-box-left {\r\n        margin-bottom: 30px;\r\n        background-position-x: 700px;\r\n      }\r\n      .media-bottom {\r\n        margin-bottom: 0;\r\n      }\r\n    }\r\n    .card-box {\r\n      flex-wrap: wrap;\r\n      .card {\r\n        width: 49%;\r\n        margin-bottom: 30px;\r\n        .title {\r\n          margin-bottom: 20px;\r\n        }\r\n      }\r\n    }\r\n  }\r\n  .middle {\r\n    height: auto;\r\n    padding: 72px 0;\r\n    & > div {\r\n      .middle-mg {\r\n        margin: 0;\r\n      }\r\n      & > div {\r\n        display: unset;\r\n        & > div {\r\n          line-height: 60px;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n/* 针对 Firefox 浏览器 */\r\n@supports (-moz-appearance: none) {\r\n  @media (max-width: 750px) {\r\n    .public-mixin();\r\n  }\r\n}\r\n/* 针对 iOS 设备，Retina 屏幕 */\r\n@media screen and (-webkit-min-device-pixel-ratio: 2) and (max-width: 750px) {\r\n  .public-mixin();\r\n}\r\n\r\n/* 针对 iOS 设备，非 Retina 屏幕 */\r\n@media screen and (-webkit-max-device-pixel-ratio: 1) and (max-width: 750px) {\r\n  .public-mixin();\r\n}\r\n\r\n/* 针对特定的 iOS 设备（例如：iPhone 6/7/8 Plus） */\r\n@media screen and (device-aspect-ratio: 16/9) and (-webkit-min-device-pixel-ratio: 3) and (max-width: 750px) {\r\n  .public-mixin();\r\n}\r\n</style>\r\n","import mod from \"-!../../../node_modules/.pnpm/registry.npmmirror.com+thread-loader@3.0.4_webpack@5.88.2/node_modules/thread-loader/dist/cjs.js!../../../node_modules/.pnpm/registry.npmmirror.com+babel-loader@8.3.0_@babel+core@7.12.16_webpack@5.88.2/node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../../node_modules/.pnpm/registry.npmmirror.com+vue-loader@15.10.1_css-loader@6.8.1_vue-template-compiler@2.6.14_webpack@5.88.2/node_modules/vue-loader/lib/index.js??vue-loader-options!./index.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../../node_modules/.pnpm/registry.npmmirror.com+thread-loader@3.0.4_webpack@5.88.2/node_modules/thread-loader/dist/cjs.js!../../../node_modules/.pnpm/registry.npmmirror.com+babel-loader@8.3.0_@babel+core@7.12.16_webpack@5.88.2/node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../../node_modules/.pnpm/registry.npmmirror.com+vue-loader@15.10.1_css-loader@6.8.1_vue-template-compiler@2.6.14_webpack@5.88.2/node_modules/vue-loader/lib/index.js??vue-loader-options!./index.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./index.vue?vue&type=template&id=c4d41f88&scoped=true&\"\nimport script from \"./index.vue?vue&type=script&lang=js&\"\nexport * from \"./index.vue?vue&type=script&lang=js&\"\nimport style0 from \"./index.vue?vue&type=style&index=0&id=c4d41f88&prod&lang=less&scoped=true&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/.pnpm/registry.npmmirror.com+vue-loader@15.10.1_css-loader@6.8.1_vue-template-compiler@2.6.14_webpack@5.88.2/node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"c4d41f88\",\n  null\n  \n)\n\nexport default component.exports","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',[_vm._m(0),_vm._m(1),_c('div',{staticClass:\"brochure padding w\"},[_c('h1',[_vm._v(\"Schedule\")]),_c('div',{staticClass:\"time-list-box\"},_vm._l((_vm.timeList),function(item,index){return _c('div',{key:index,staticClass:\"time-box\"},[_c('div',{staticClass:\"time\"},[_c('span',[_vm._v(_vm._s(item.time))]),_vm._m(2,true)]),_c('div',{staticClass:\"time-centen\"},[_c('p',{staticClass:\"title\"},[_vm._v(_vm._s(item.title))]),_c('div',{staticClass:\"avtar-box\"},[_c('div',{staticClass:\"name\"},[_vm._v(_vm._s(item.name)),_c('br'),_vm._v(_vm._s(item.text))])])])])}),0),_c('h1',{staticClass:\"mg-top\"},[_vm._v(\"Scope and Objective\")]),_vm._m(3),_c('h1',[_vm._v(\"Papers\")]),_vm._m(4)]),_vm._m(5)])}\nvar staticRenderFns = [function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"header\"},[_c('div',{staticClass:\"w padding flex\"},[_c('span',[_vm._v(\"FinLLM 2023\")]),_c('div',{staticClass:\"right\"},[_c('img',{attrs:{\"src\":require(\"@/assets/img/ijcai.png\"),\"alt\":\"\"}})])])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"subtitle padding w\"},[_c('div',{staticClass:\"sub-text\"},[_vm._v(\"FinLLM Conference Brochure\")])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"big-rect\"},[_c('div',{staticClass:\"small-rect\"})])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"cente-text mg-bottom\"},[_vm._v(\" Pretrained large language models (LLMs) have demonstrated tremendous potential in various natural language processing tasks, including language generation, machine translation, and question answering. In the financial services industry, pretrained LLMs have the potential to significantly impact tasks such as financial forecasting, risk management, and sentiment analysis. Moreover, LLMs can help automate the process of analyzing financial reports and extracting key insights that can aid businesses in making informed decisions.\"),_c('br'),_c('br'),_vm._v(\"This symposium provides a platform for researchers, practitioners, and industry experts from around the world to share new ideas, exchange research findings, and discuss the challenges and opportunities in the field of pretrained LLMs for financial services. The symposium will cover two themes: 1) potential applications and best practices of pretrained LLMs for financial services; and 2) challenges that need to be addressed to make them efficient, effective, and trustworthy.\"),_c('br'),_c('br'),_vm._v(\" The symposium will also feature invited talks by leading researchers and industry experts, as well as panel discussions on the latest trends and challenges in the field. We welcome researchers, practitioners, and industry experts from academia and industry to submit their work and participate in this exciting event. \")])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"cente-text mg-bottom\"},[_c('div',{staticClass:\"cente-title\"},[_vm._v(\"#1. NumGPT: Improving Numeracy Ability of Generative Pre-trained Models\")]),_vm._v(\" Abstract: Existing generative pre-trained language models (e.g., GPT) focus on modeling the language structure and semantics of general texts. However, those models do not consider the numerical properties of numbers and cannot perform robustly on numerical reasoning tasks (e.g., math word problems and measurement estimation). In this paper, we propose NumGPT, a generative pre-trained model that explicitly models the numerical properties of numbers in texts. Specifically, it leverages a prototype-based numeral embedding to encode the mantissa of the number and an individual embedding to encode the exponent of the number. A numeralaware loss function is designed to integrate numerals into the pre-training objective of NumGPT. We conduct extensive experiments on four different datasets to evaluate the numeracy ability of NumGPT. The experiment results show that NumGPT outperforms baseline models (e.g., GPT and GPT with DICE) on a range of numerical reasoning tasks such as measurement estimation, number comparison, math word problems, and magnitude classification. Ablation studies are also conducted to evaluate the impact of pre-training and model hyperparameters on the performance. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\"#2. Symmetry, Efficient Markets and Monetary Neutrality\")]),_vm._v(\" Abstract: We study the relationship between symmetry, efficient markets and monetary neutrality. We find that information symmetry can lead markets to reach efficient outcomes and will produce the prices which fluctuate randomly. However, information symmetry is almost impossible to achieve without considering the time factor! In addition, efficient markets can lead to monetary neutrality. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\"#3. SPAM-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection\")]),_vm._v(\" Abstract: This paper investigates the effectiveness of large language models (LLMs) in email spam detection by comparing prominent models from three distinct families: BERT-like, Sentence Transformers, and Seq2Seq. Additionally, we examine well-established machine learning techniques for spam detection, such as Na¨ıve Bayes and LightGBM, as baseline methods. We assess the performance of these models across four public datasets, utilizing different numbers of training samples (full training set and few-shot settings). Our findings reveal that, in the majority of cases, LLMs surpass the performance of the popular baseline techniques, particularly in few-shot scenarios. This adaptability renders LLMs uniquely suited to spam detection tasks, where labeled samples are limited in number and models require frequent updates. Additionally, we introduce SPAM-T5, a Flan-T5 model that has been specifically adapted and fine-tuned for the purpose of detecting email spam. Our results demonstrate that SPAM-T5 surpasses baseline models and other LLMs in the majority of scenarios, particularly when there are a limited number of training samples available. Our code is publicly available at \"),_c('a',{attrs:{\"href\":\"https://github.com/jpmorganchase/llm-email-spam-detection\"}},[_vm._v(\"https://github.com/jpmorganchase/llm-email-spam-detection\")]),_vm._v(\". \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\"#4. Efficient Fine-tuning on LLaMA-series Models\")]),_vm._v(\" Abstract: This paper presents a comprehensive analysis of the exceptional performance exhibited by LLaMA large-scale language models, along with the introduction of PEFT (Parameter-Efficient Fine-Tuning) methods to enhance model performance. Firstly, the paper showcases the remarkable advancements in LLaMA pre-training models, which are trained on vast text data and display remarkable general capabilities and excellent overall performance in language representation and semantic understanding. However, relying solely on pre-trained LLaMA may lead to limitations when addressing specific text processing tasks. To address these challenges, researchers have proposed PEFT methods, which enable the selection of optimal fine-tuning techniques based on task requirements and resource constraints, thus maximizing the potential of LLaMA. By providing a detailed examination of LLaMA and PEFT methods, this paper aims to offer users a clearer understanding of how to effectively apply large-scale language models. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\"#5. Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models\")]),_vm._v(\" Abstract: Entity-level fine-grained sentiment analysis in the financial domain is a crucial subtask of sentiment analysis and currently faces numerous challenges. The primary challenge stems from the lack of high-quality and large-scale annotated corpora specifically designed for financial text sentiment analysis, which in turn limits the availability of data necessary for developing effective text processing techniques. Recent advancements in large language models (LLMs) have yielded remarkable performance in natural language processing tasks, primarily centered around language pattern matching. In this paper, we propose a novel and extensive Chinese fine-grained financial sentiment analysis dataset, FinChina SA, for enterprise early warning. We thoroughly evaluate and experiment with well-known existing open-source LLMs using our dataset. We firmly believe that our dataset will serve as a valuable resource to advance the exploration of real-world financial sentiment analysis tasks, which should be the focus of future research. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\" #6. Exploring In-Context Learning for Overnight Stock Price Movement Prediction with Large Language Models \")]),_vm._v(\" Abstract: Stock price movement prediction is an important task in the financial domain since accurate price prediction for the next trading day brings great benefits for making decisions on stock transactions. Except for some common indicators like 20-days moving average, time-series regression models including linear regression, RNN-based models, and Transformer achieve satisfying performance in this task and are widely applied. However, all methods mentioned above only exploit historical data to predict stock price movement in the next trading day. Apart from stock historical data, data in text modality like finance-related news or social platform contents can also reflect the moving trend of stocks. Instead of training a new model to learn the representation of text data, we use large language models (LLMs) to directly predict the price movement of stocks. Considering the possible lack of domain data in training corpora of LLMs, we introduce in-context learning to explore if LLMs can learn necessary knowledge from given prediction examples consisting of both historical data and text data. In this paper, we focus on overnight stock price movement prediction. Our experiment results show that LLMs can boost performance in this task compared to competitive baselines and in-context learning can be beneficial for prediction in some cases. Our further analyses explore the influence of data modalities, sample selection strategies, and the number of examples on models’ performance. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\"#7. Financial Large Language Models: A Survey\")]),_vm._v(\" Abstract: In recent years, the emergence of a series of large language models, such as ChatGPT, and their powerful performance have garnered significant attention from both academia and industry. Expanding the application of these models and technologies to specialized fields, specifically the financial industry, has become an intriguing research topic, given its potential for immense value and impact. However, it is apparent that the financial industry presents a unique set of challenges for general large language models, due to its inherently complex and dynamic nature. In this survey, we will review the existing academic work on large language models in the financial domain, and summarize the unique and specific features of the financial field compared to general domains. In addition, we will examine several key peculiarities and discuss relevant research works in the field of large language models. Ultimately, the findings of this paper provide insights into the challenges and opportunities of deploying large language models in the financial field, and contribute to a better understanding of the possibilities for optimizing their performance in complex and specialized domains. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\"#8. Could LLM Replace Traditional AI in Loan Business?\")]),_vm._v(\" Abstract: This paper explores several typical applications in the real-world fintech industry and evaluates the possibility of replacing the AI models with the recently booming Large Language Model (LLM). The aim of using LLM is to improve the user experience of our financial services, while also considering the cost and benefits from a business point of view. To accomplish this, we have designed a series of feasible solutions that cover different types of business requirements. Experiments have been carried out in one of our most active scenes. Additionally, we discuss how LLM can be properly used to mitigate risks of misuse and other possible hazardous side-effects to the business \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\" #9. An Effective Data Creation Pipeline to Generate High-quality Financial Instruction Data for Large Language Model \")]),_vm._v(\" Abstract: At the beginning era of large language model, it is quite critical to generate a highquality financial dataset to fine-tune a large language model for financial related tasks. Thus, this paper presents a carefully designed data creation pipeline for this purpose. Particularly, we initiate a dialogue between an AI investor and financial expert using ChatGPT and incorporate the feedback of human financial experts, leading to the refinement of the dataset. This pipeline yielded a robust instruction tuning dataset comprised of 103k multi-turn chats. Extensive experiments have been conducted on this dataset to evaluate the model’s performance by adopting an external GPT-4 as the judge. The promising experimental results verify that our approach led to significant advancements in generating accurate, relevant, and financial-style responses from AI models, and thus providing a powerful tool for applications within the financial sector. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\" #10. Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models \")]),_vm._v(\" Abstract: Sentiment analysis is a vital tool for uncovering insights from financial articles, news, and social media, shaping our understanding of market movements. Despite the impressive capabilities of large language models (LLMs) in financial natural language processing (NLP), they still struggle with accurately interpreting numerical values and grasping financial context, limiting their effectiveness in predicting financial sentiment. In this paper, we introduce a simple yet effective instruction tuning approach to address these issues. By transforming a small portion of supervised financial sentiment analysis data into instruction data and finetuning a general-purpose LLM with this method, we achieve remarkable advancements in financial sentiment analysis. In the experiment, our approach outperforms state-of-the-art supervised sentiment analysis models, as well as widely used LLMs like ChatGPT and LLaMAs, particularly in scenarios where numerical understanding and contextual comprehension are vital. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\"#11. FinGPT: Open-Source Financial Large Language Model\")]),_vm._v(\" Abstract: Large language models (LLMs) have shown the potential of revolutionizing natural language processing in diverse domains, sparking great interest in finance. However, the finance domain presents unique challenges, including high temporal sensitivity, constant dynamism, and a low signal-to-noise ratio (SNR). While proprietary models like BloombergGPT have taken advantage of their unique data accumulation, such privileged access calls for an open-source alternative to democratize internet-scale financial data. In this paper, we present an opensource large language model, FinGPT, for the finance sector. Unlike proprietary models, FinGPT takes a data-centric approach, providing researchers and practitioners with accessible and transparent resources to customize their financial LLMs (FinLLMs). We highlight the importance of an automatic data curation pipeline and the lightweight low-rank adaptation technique in building FinGPT. Furthermore, we will showcase potential applications as stepping stones for users, such as robo-advising and sentiment analysis. Through collaborative efforts within the opensource AI4Finance community, FinGPT aims to stimulate innovation, democratize FinLLMs, and unlock new opportunities in open finance. Two associated code repos are \"),_c('a',{attrs:{\"href\":\"https://github.com/AI4Finance-Foundation/FinGPT\"}},[_vm._v(\"https://github.com/AI4Finance-Foundation/FinGPT\")]),_vm._v(\" and \"),_c('a',{attrs:{\"href\":\"https://github.com/AI4FinanceFoundation/FinNLP\"}},[_vm._v(\"https://github.com/AI4FinanceFoundation/FinNLP\")]),_c('div',{staticClass:\"cente-title\"},[_vm._v(\"#12. The Challenges of LLMs in Financial Services\")]),_vm._v(\" Abstract: With the rapid development of Large Language Models (LLMs), the financial services sector has shown outstanding advantages and potential. At the same time, risks and challenges for financial services are brought into traditional businesses, such as data privacy, ethics, misinformation, disinformation, and social injustices. In light of those challenges, this paper reports on a mixed-methods study with corporate middle management, high management, and technical staff. Our preliminary findings suggest that financial services practitioners are optimistic about the future of LLMs but distrustful of existing ones. Our paper should be read as a preliminary study rather than a complete research work. Thus our suggestions on improving LLMs for specific domains, such as financial services, are, hopefully, useful to those who share similar interests in improving LLMs for realistic use. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\"#13. Large Scale Financial Time Series Forecasting under Distributional Shift\")]),_vm._v(\" Abstract: Data-driven approaches using deep neural networks have been successful in modeling complex financial time series and generating accurate predictions without requiring extensive domain knowledge. However, most of the existing models that assume independent and identically distributed data may not generalize well to novel situations or distributional shifts inside financial scenarios. To address this challenge, we introduce an invariant learning-based regularizer with relaxed bounds that expands the range of feasible solutions and mitigates overconvergence issues in Invariant Risk Minimization (IRM). The regularizer is incorporated into a Multilayer Perceptron (MLP)-based financial time series forecasting model. Experimental results show that this regularizer enables more robust and adaptable financial forecasting models, enhancing the overall performance and generalizability of data-driven financial forecasting. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\" #14. Integrating Stock Features and Global Information via Large Language Models for Enhanced Stock Return Prediction \")]),_vm._v(\" Abstract: The remarkable achievements and rapid advancements of Large Language Models (LLMs) such as ChatGPT and GPT-4 have showcased their immense potential in quantitative investment. Traders can effectively leverage these LLMs to analyze financial news and predict stock returns accurately. However, integrating LLMs into existing quantitative models presents two primary challenges: the insufficient utilization of semantic information embedded within LLMs and the difficulties in aligning the latent information within LLMs with pre-existing quantitative stock features. We propose a novel framework consisting of two components to surmount these challenges. The first component, the Local Global model, approaches understanding stock features and financial news (through LLMs) as a unified problem. The second component, Self-Correlated Reinforcement Learning, focuses on aligning the embeddings of financial news generated by LLMs with stock features within the same semantic space. By implementing our framework, we have demonstrated superior performance in Rank Information Coefficient and returns, particularly compared to models relying only on stock features in the China A-share market. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\" #15. Unveiling the Potential of Sentiment: Can Large Language Models Predict Chinese Stock Price Movements? \")]),_vm._v(\" Abstract: The rapid advancement of Large Language Models (LLMs) has led to extensive discourse regarding their potential to boost the return of quantitative stock trading strategies. This discourse primarily revolves around harnessing the remarkable comprehension capabilities of LLMs to extract sentiment factors which facilitate informed and high-frequency investment portfolio adjustments. To ensure successful implementations of these LLMs into the analysis of Chinese financial texts and the subsequent trading strategy development within the Chinese stock market, we provide a rigorous and encompassing benchmark as well as a standardized back-testing framework aiming at objectively assessing the efficacy of various types of LLMs in the specialized domain of sentiment factor extraction from Chinese news text data. To illustrate how our benchmark works, we reference three distinctive models: 1) the generative LLM (ChatGPT), 2) the Chinese language-specific pre-trained LLM (Erlangshen-RoBERTa), and 3) the financial domain-specific fine-tuned LLM classifier(Chinese FinBERT). We apply them directly to the task of sentiment factor extraction from large volumes of Chinese news summary texts. We then proceed to building quantitative trading strategies and running back-tests under realistic trading scenarios based on the derived sentiment factors and evaluate their performances with our benchmark. By constructing such a comparative analysis, we invoke the question of what constitutes the most important element for improving a LLM’s performance on extracting sentiment factors. And by ensuring that the LLMs are evaluated on the same benchmark, following the same standardized experimental procedures that are designed with sufficient expertise in quantitative trading, we make the first stride toward answering such a question. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\" #16. Can ChatGPT Overcome Behavioral Biases in the Financial Sector? Classify-and-Rethink: Multi-Step Zero-Shot Reasoning in the Gold Investment \")]),_vm._v(\" Abstract: The framing effect is a phenomenon observed in both humans and large language models where different descriptions of the same objectively identical problem can lead to different decisions. In behavioral finance, this effect must be carefully managed to prevent undesirable outcomes in the investment process. In the application of large language models, prompt engineering is often employed to mitigate the framing effect. While previous research in this domain has focused on sentiment classification or subject recognition, this study employs a large language model to score gold-related news and designs the ”Classify-and-Rethink” prompt strategy from the perspective of behavioral finance. Our experiment demonstrates that the Classify-and-Rethink prompt approach effectively overcomes the framing effect and facilitates excess returns. Compared to alternative prompt designs, the Classify-and-Rethink strategy exhibits higher yields and Sharpe ratios. This framework provides a valuable basis for future research on behavioral finance in large-scale analysis of financial texts and offers investors a reliable means of mitigating behavioral biases. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\"#17. FinVis-GPT: A Multimodal Large Language Model for Financial Chart Analysis\")]),_vm._v(\" Abstract: In this paper, we propose FinVis-GPT, a novel multimodal large language model (LLM) specifically designed for financial chart analysis. By leveraging the power of LLMs and incorporating instruction tuning and multimodal capabilities, FinVis-GPT is capable of interpreting financial charts and providing valuable analysis. To train FinVis-GPT, a financial task oriented dataset was generated for pretraining alignment and instruction tuning, comprising various types of financial charts and their corresponding descriptions. We evaluate the model performance via several case studies due to the time limit, and the promising results demonstrated that FinVis-GPT is superior in various financial chart related tasks, including generating descriptions, answering questions and predicting future market trends, surpassing existing state-of-the-art multimodal LLMs. The proposed FinVis-GPT serves as a pioneering effort in utilizing multimodal LLMs in the finance domain and our generated dataset will be release for public use in the near future to speedup related research. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\" #18. XuanYuan: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters \")]),_vm._v(\" Abstract: In recent years, pre-trained language models have undergone rapid development with the emergence of large-scale models. However, there is a lack of open-sourced chat models specifically designed for the Chinese language, especially in the field of Chinese finance, at the scale of hundreds of billions. To address this gap, we introduce XuanYuan 2.0 (轩辕 2.0), the largest Chinese chat model to date, built upon the BLOOM-176B architecture. To enable the model to generate a larger quantity of correct and financial instruction data, we introduce an innovative framework named SELF-QA, which replaces the traditional practice of human-written instruction seeds with a vast amount of unsupervised knowledge. Additionally, we propose a novel training method called Hybrid-tuning to mitigate catastrophic forgetting. By combining general-domain with domain-specific knowledge and integrating the stages of pre-training and fine-tuning, XuanYuan 2.0 is capable of providing accurate and contextually appropriate responses in the Chinese financial domain. Our model has been made public on Github and Huggingface, and has received widespread attention. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\" #19. CGCE: A Chinese Generative Chat Evaluation Benchmark for General and Financial Domains \")]),_vm._v(\" Abstract: Generative chat models, such as ChatGPT and GPT-4, have revolutionized natural language generation (NLG) by incorporating instructions and human feedback to achieve significant performance improvements. However, the lack of standardized evaluation benchmarks for chat models, particularly for Chinese and domain-specific models, hinders their assessment and progress. To address this gap, we introduce the Chinese Generative Chat Evaluation (CGCE) benchmark, focusing on general and financial domains. The CGCE benchmark encompasses diverse tasks, including 300 questions in the general domain and 150 specific professional questions in the financial domain. Manual scoring evaluates factors such as accuracy, coherence, expression clarity, and completeness. To the best of our knowledge, the financial evaluation section of CGCE is the first evaluation benchmark for largescale chat models in the Chinese financial domain. The CGCE benchmark provides researchers with a standardized framework to assess and compare Chinese generative chat models, fostering advancements in NLG research. \")])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"footer padding\"},[_c('div',[_c('div',{staticClass:\"email-icon\"}),_c('div',{staticClass:\"email-info\"},[_c('p',[_vm._v(\"If you have any question, feel free to contact us:\")]),_c('p',{staticStyle:{\"color\":\"#b3ebff\"}},[_vm._v(\"finllm2023@easychair.org\")])])])])}]\n\nexport { render, staticRenderFns }","<template>\n  <div>\n    <div class=\"header\">\n      <div class=\"w padding flex\">\n        <span>FinLLM 2023</span>\n        <div class=\"right\">\n          <img\n            src=\"@/assets/img/ijcai.png\"\n            alt=\"\"\n          />\n        </div>\n      </div>\n    </div>\n    <div class=\"subtitle padding w\">\n      <div class=\"sub-text\">FinLLM Conference Brochure</div>\n    </div>\n    <div class=\"brochure padding w\">\n      <h1>Schedule</h1>\n      <div class=\"time-list-box\">\n        <div\n          v-for=\"(item, index) in timeList\"\n          :key=\"index\"\n          class=\"time-box\"\n        >\n          <div class=\"time\">\n            <span>{{ item.time }}</span>\n            <div class=\"big-rect\">\n              <div class=\"small-rect\"></div>\n            </div>\n          </div>\n          <div class=\"time-centen\">\n            <p class=\"title\">{{ item.title }}</p>\n            <div class=\"avtar-box\">\n              <!-- <div\n                v-if=\"item.img\"\n                class=\"avtar\"\n              >\n                <img :src=\"item.img\" />\n              </div> -->\n              <div class=\"name\">{{ item.name }}<br />{{ item.text }}</div>\n            </div>\n            <!-- <p class=\"note\">{{ item.note }}</p> -->\n          </div>\n        </div>\n      </div>\n      <h1 class=\"mg-top\">Scope and Objective</h1>\n      <div class=\"cente-text mg-bottom\">\n        Pretrained large language models (LLMs) have demonstrated tremendous potential in various natural language\n        processing tasks, including language generation, machine translation, and question answering. In the financial\n        services industry, pretrained LLMs have the potential to significantly impact tasks such as financial\n        forecasting, risk management, and sentiment analysis. Moreover, LLMs can help automate the process of analyzing\n        financial reports and extracting key insights that can aid businesses in making informed decisions.<br />\n        <br />This symposium provides a platform for researchers, practitioners, and industry experts from around the\n        world to share new ideas, exchange research findings, and discuss the challenges and opportunities in the field\n        of pretrained LLMs for financial services. The symposium will cover two themes: 1) potential applications and\n        best practices of pretrained LLMs for financial services; and 2) challenges that need to be addressed to make\n        them efficient, effective, and trustworthy.<br /><br />\n        The symposium will also feature invited talks by leading researchers and industry experts, as well as panel\n        discussions on the latest trends and challenges in the field. We welcome researchers, practitioners, and\n        industry experts from academia and industry to submit their work and participate in this exciting event.\n      </div>\n      <h1>Papers</h1>\n      <div class=\"cente-text mg-bottom\">\n        <div class=\"cente-title\">#1. NumGPT: Improving Numeracy Ability of Generative Pre-trained Models</div>\n        Abstract: Existing generative pre-trained language models (e.g., GPT) focus on modeling the language structure\n        and semantics of general texts. However, those models do not consider the numerical properties of numbers and\n        cannot perform robustly on numerical reasoning tasks (e.g., math word problems and measurement estimation). In\n        this paper, we propose NumGPT, a generative pre-trained model that explicitly models the numerical properties of\n        numbers in texts. Specifically, it leverages a prototype-based numeral embedding to encode the mantissa of the\n        number and an individual embedding to encode the exponent of the number. A numeralaware loss function is\n        designed to integrate numerals into the pre-training objective of NumGPT. We conduct extensive experiments on\n        four different datasets to evaluate the numeracy ability of NumGPT. The experiment results show that NumGPT\n        outperforms baseline models (e.g., GPT and GPT with DICE) on a range of numerical reasoning tasks such as\n        measurement estimation, number comparison, math word problems, and magnitude classification. Ablation studies\n        are also conducted to evaluate the impact of pre-training and model hyperparameters on the performance.\n\n        <div class=\"cente-title\">#2. Symmetry, Efficient Markets and Monetary Neutrality</div>\n        Abstract: We study the relationship between symmetry, efficient markets and monetary neutrality. We find that\n        information symmetry can lead markets to reach efficient outcomes and will produce the prices which fluctuate\n        randomly. However, information symmetry is almost impossible to achieve without considering the time factor! In\n        addition, efficient markets can lead to monetary neutrality.\n\n        <div class=\"cente-title\">#3. SPAM-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection</div>\n        Abstract: This paper investigates the effectiveness of large language models (LLMs) in email spam detection by\n        comparing prominent models from three distinct families: BERT-like, Sentence Transformers, and Seq2Seq.\n        Additionally, we examine well-established machine learning techniques for spam detection, such as Na¨ıve Bayes\n        and LightGBM, as baseline methods. We assess the performance of these models across four public datasets,\n        utilizing different numbers of training samples (full training set and few-shot settings). Our findings reveal\n        that, in the majority of cases, LLMs surpass the performance of the popular baseline techniques, particularly in\n        few-shot scenarios. This adaptability renders LLMs uniquely suited to spam detection tasks, where labeled\n        samples are limited in number and models require frequent updates. Additionally, we introduce SPAM-T5, a Flan-T5\n        model that has been specifically adapted and fine-tuned for the purpose of detecting email spam. Our results\n        demonstrate that SPAM-T5 surpasses baseline models and other LLMs in the majority of scenarios, particularly\n        when there are a limited number of training samples available. Our code is publicly available at\n        <a href=\"https://github.com/jpmorganchase/llm-email-spam-detection\"\n          >https://github.com/jpmorganchase/llm-email-spam-detection</a\n        >.\n\n        <div class=\"cente-title\">#4. Efficient Fine-tuning on LLaMA-series Models</div>\n        Abstract: This paper presents a comprehensive analysis of the exceptional performance exhibited by LLaMA\n        large-scale language models, along with the introduction of PEFT (Parameter-Efficient Fine-Tuning) methods to\n        enhance model performance. Firstly, the paper showcases the remarkable advancements in LLaMA pre-training\n        models, which are trained on vast text data and display remarkable general capabilities and excellent overall\n        performance in language representation and semantic understanding. However, relying solely on pre-trained LLaMA\n        may lead to limitations when addressing specific text processing tasks. To address these challenges, researchers\n        have proposed PEFT methods, which enable the selection of optimal fine-tuning techniques based on task\n        requirements and resource constraints, thus maximizing the potential of LLaMA. By providing a detailed\n        examination of LLaMA and PEFT methods, this paper aims to offer users a clearer understanding of how to\n        effectively apply large-scale language models.\n\n        <div class=\"cente-title\">#5. Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models</div>\n        Abstract: Entity-level fine-grained sentiment analysis in the financial domain is a crucial subtask of sentiment\n        analysis and currently faces numerous challenges. The primary challenge stems from the lack of high-quality and\n        large-scale annotated corpora specifically designed for financial text sentiment analysis, which in turn limits\n        the availability of data necessary for developing effective text processing techniques. Recent advancements in\n        large language models (LLMs) have yielded remarkable performance in natural language processing tasks, primarily\n        centered around language pattern matching. In this paper, we propose a novel and extensive Chinese fine-grained\n        financial sentiment analysis dataset, FinChina SA, for enterprise early warning. We thoroughly evaluate and\n        experiment with well-known existing open-source LLMs using our dataset. We firmly believe that our dataset will\n        serve as a valuable resource to advance the exploration of real-world financial sentiment analysis tasks, which\n        should be the focus of future research.\n\n        <div class=\"cente-title\">\n          #6. Exploring In-Context Learning for Overnight Stock Price Movement Prediction with Large Language Models\n        </div>\n        Abstract: Stock price movement prediction is an important task in the financial domain since accurate price\n        prediction for the next trading day brings great benefits for making decisions on stock transactions. Except for\n        some common indicators like 20-days moving average, time-series regression models including linear regression,\n        RNN-based models, and Transformer achieve satisfying performance in this task and are widely applied. However,\n        all methods mentioned above only exploit historical data to predict stock price movement in the next trading\n        day. Apart from stock historical data, data in text modality like finance-related news or social platform\n        contents can also reflect the moving trend of stocks. Instead of training a new model to learn the\n        representation of text data, we use large language models (LLMs) to directly predict the price movement of\n        stocks. Considering the possible lack of domain data in training corpora of LLMs, we introduce in-context\n        learning to explore if LLMs can learn necessary knowledge from given prediction examples consisting of both\n        historical data and text data. In this paper, we focus on overnight stock price movement prediction. Our\n        experiment results show that LLMs can boost performance in this task compared to competitive baselines and\n        in-context learning can be beneficial for prediction in some cases. Our further analyses explore the influence\n        of data modalities, sample selection strategies, and the number of examples on models’ performance.\n\n        <div class=\"cente-title\">#7. Financial Large Language Models: A Survey</div>\n        Abstract: In recent years, the emergence of a series of large language models, such as ChatGPT, and their\n        powerful performance have garnered significant attention from both academia and industry. Expanding the\n        application of these models and technologies to specialized fields, specifically the financial industry, has\n        become an intriguing research topic, given its potential for immense value and impact. However, it is apparent\n        that the financial industry presents a unique set of challenges for general large language models, due to its\n        inherently complex and dynamic nature. In this survey, we will review the existing academic work on large\n        language models in the financial domain, and summarize the unique and specific features of the financial field\n        compared to general domains. In addition, we will examine several key peculiarities and discuss relevant\n        research works in the field of large language models. Ultimately, the findings of this paper provide insights\n        into the challenges and opportunities of deploying large language models in the financial field, and contribute\n        to a better understanding of the possibilities for optimizing their performance in complex and specialized\n        domains.\n\n        <div class=\"cente-title\">#8. Could LLM Replace Traditional AI in Loan Business?</div>\n        Abstract: This paper explores several typical applications in the real-world fintech industry and evaluates the\n        possibility of replacing the AI models with the recently booming Large Language Model (LLM). The aim of using\n        LLM is to improve the user experience of our financial services, while also considering the cost and benefits\n        from a business point of view. To accomplish this, we have designed a series of feasible solutions that cover\n        different types of business requirements. Experiments have been carried out in one of our most active scenes.\n        Additionally, we discuss how LLM can be properly used to mitigate risks of misuse and other possible hazardous\n        side-effects to the business\n\n        <div class=\"cente-title\">\n          #9. An Effective Data Creation Pipeline to Generate High-quality Financial Instruction Data for Large Language\n          Model\n        </div>\n        Abstract: At the beginning era of large language model, it is quite critical to generate a highquality financial\n        dataset to fine-tune a large language model for financial related tasks. Thus, this paper presents a carefully\n        designed data creation pipeline for this purpose. Particularly, we initiate a dialogue between an AI investor\n        and financial expert using ChatGPT and incorporate the feedback of human financial experts, leading to the\n        refinement of the dataset. This pipeline yielded a robust instruction tuning dataset comprised of 103k\n        multi-turn chats. Extensive experiments have been conducted on this dataset to evaluate the model’s performance\n        by adopting an external GPT-4 as the judge. The promising experimental results verify that our approach led to\n        significant advancements in generating accurate, relevant, and financial-style responses from AI models, and\n        thus providing a powerful tool for applications within the financial sector.\n\n        <div class=\"cente-title\">\n          #10. Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language\n          Models\n        </div>\n        Abstract: Sentiment analysis is a vital tool for uncovering insights from financial articles, news, and social\n        media, shaping our understanding of market movements. Despite the impressive capabilities of large language\n        models (LLMs) in financial natural language processing (NLP), they still struggle with accurately interpreting\n        numerical values and grasping financial context, limiting their effectiveness in predicting financial sentiment.\n        In this paper, we introduce a simple yet effective instruction tuning approach to address these issues. By\n        transforming a small portion of supervised financial sentiment analysis data into instruction data and\n        finetuning a general-purpose LLM with this method, we achieve remarkable advancements in financial sentiment\n        analysis. In the experiment, our approach outperforms state-of-the-art supervised sentiment analysis models, as\n        well as widely used LLMs like ChatGPT and LLaMAs, particularly in scenarios where numerical understanding and\n        contextual comprehension are vital.\n\n        <div class=\"cente-title\">#11. FinGPT: Open-Source Financial Large Language Model</div>\n        Abstract: Large language models (LLMs) have shown the potential of revolutionizing natural language processing\n        in diverse domains, sparking great interest in finance. However, the finance domain presents unique challenges,\n        including high temporal sensitivity, constant dynamism, and a low signal-to-noise ratio (SNR). While proprietary\n        models like BloombergGPT have taken advantage of their unique data accumulation, such privileged access calls\n        for an open-source alternative to democratize internet-scale financial data. In this paper, we present an\n        opensource large language model, FinGPT, for the finance sector. Unlike proprietary models, FinGPT takes a\n        data-centric approach, providing researchers and practitioners with accessible and transparent resources to\n        customize their financial LLMs (FinLLMs). We highlight the importance of an automatic data curation pipeline and\n        the lightweight low-rank adaptation technique in building FinGPT. Furthermore, we will showcase potential\n        applications as stepping stones for users, such as robo-advising and sentiment analysis. Through collaborative\n        efforts within the opensource AI4Finance community, FinGPT aims to stimulate innovation, democratize FinLLMs,\n        and unlock new opportunities in open finance. Two associated code repos are\n        <a href=\"https://github.com/AI4Finance-Foundation/FinGPT\">https://github.com/AI4Finance-Foundation/FinGPT</a>\n        and <a href=\"https://github.com/AI4FinanceFoundation/FinNLP\">https://github.com/AI4FinanceFoundation/FinNLP</a>\n\n        <div class=\"cente-title\">#12. The Challenges of LLMs in Financial Services</div>\n        Abstract: With the rapid development of Large Language Models (LLMs), the financial services sector has shown\n        outstanding advantages and potential. At the same time, risks and challenges for financial services are brought\n        into traditional businesses, such as data privacy, ethics, misinformation, disinformation, and social\n        injustices. In light of those challenges, this paper reports on a mixed-methods study with corporate middle\n        management, high management, and technical staff. Our preliminary findings suggest that financial services\n        practitioners are optimistic about the future of LLMs but distrustful of existing ones. Our paper should be read\n        as a preliminary study rather than a complete research work. Thus our suggestions on improving LLMs for specific\n        domains, such as financial services, are, hopefully, useful to those who share similar interests in improving\n        LLMs for realistic use.\n\n        <div class=\"cente-title\">#13. Large Scale Financial Time Series Forecasting under Distributional Shift</div>\n        Abstract: Data-driven approaches using deep neural networks have been successful in modeling complex financial\n        time series and generating accurate predictions without requiring extensive domain knowledge. However, most of\n        the existing models that assume independent and identically distributed data may not generalize well to novel\n        situations or distributional shifts inside financial scenarios. To address this challenge, we introduce an\n        invariant learning-based regularizer with relaxed bounds that expands the range of feasible solutions and\n        mitigates overconvergence issues in Invariant Risk Minimization (IRM). The regularizer is incorporated into a\n        Multilayer Perceptron (MLP)-based financial time series forecasting model. Experimental results show that this\n        regularizer enables more robust and adaptable financial forecasting models, enhancing the overall performance\n        and generalizability of data-driven financial forecasting.\n\n        <div class=\"cente-title\">\n          #14. Integrating Stock Features and Global Information via Large Language Models for Enhanced Stock Return\n          Prediction\n        </div>\n        Abstract: The remarkable achievements and rapid advancements of Large Language Models (LLMs) such as ChatGPT and\n        GPT-4 have showcased their immense potential in quantitative investment. Traders can effectively leverage these\n        LLMs to analyze financial news and predict stock returns accurately. However, integrating LLMs into existing\n        quantitative models presents two primary challenges: the insufficient utilization of semantic information\n        embedded within LLMs and the difficulties in aligning the latent information within LLMs with pre-existing\n        quantitative stock features. We propose a novel framework consisting of two components to surmount these\n        challenges. The first component, the Local Global model, approaches understanding stock features and financial\n        news (through LLMs) as a unified problem. The second component, Self-Correlated Reinforcement Learning, focuses\n        on aligning the embeddings of financial news generated by LLMs with stock features within the same semantic\n        space. By implementing our framework, we have demonstrated superior performance in Rank Information Coefficient\n        and returns, particularly compared to models relying only on stock features in the China A-share market.\n\n        <div class=\"cente-title\">\n          #15. Unveiling the Potential of Sentiment: Can Large Language Models Predict Chinese Stock Price Movements?\n        </div>\n        Abstract: The rapid advancement of Large Language Models (LLMs) has led to extensive discourse regarding their\n        potential to boost the return of quantitative stock trading strategies. This discourse primarily revolves around\n        harnessing the remarkable comprehension capabilities of LLMs to extract sentiment factors which facilitate\n        informed and high-frequency investment portfolio adjustments. To ensure successful implementations of these LLMs\n        into the analysis of Chinese financial texts and the subsequent trading strategy development within the Chinese\n        stock market, we provide a rigorous and encompassing benchmark as well as a standardized back-testing framework\n        aiming at objectively assessing the efficacy of various types of LLMs in the specialized domain of sentiment\n        factor extraction from Chinese news text data. To illustrate how our benchmark works, we reference three\n        distinctive models: 1) the generative LLM (ChatGPT), 2) the Chinese language-specific pre-trained LLM\n        (Erlangshen-RoBERTa), and 3) the financial domain-specific fine-tuned LLM classifier(Chinese FinBERT). We apply\n        them directly to the task of sentiment factor extraction from large volumes of Chinese news summary texts. We\n        then proceed to building quantitative trading strategies and running back-tests under realistic trading\n        scenarios based on the derived sentiment factors and evaluate their performances with our benchmark. By\n        constructing such a comparative analysis, we invoke the question of what constitutes the most important element\n        for improving a LLM’s performance on extracting sentiment factors. And by ensuring that the LLMs are evaluated\n        on the same benchmark, following the same standardized experimental procedures that are designed with sufficient\n        expertise in quantitative trading, we make the first stride toward answering such a question.\n\n        <div class=\"cente-title\">\n          #16. Can ChatGPT Overcome Behavioral Biases in the Financial Sector? Classify-and-Rethink: Multi-Step\n          Zero-Shot Reasoning in the Gold Investment\n        </div>\n        Abstract: The framing effect is a phenomenon observed in both humans and large language models where different\n        descriptions of the same objectively identical problem can lead to different decisions. In behavioral finance,\n        this effect must be carefully managed to prevent undesirable outcomes in the investment process. In the\n        application of large language models, prompt engineering is often employed to mitigate the framing effect. While\n        previous research in this domain has focused on sentiment classification or subject recognition, this study\n        employs a large language model to score gold-related news and designs the ”Classify-and-Rethink” prompt strategy\n        from the perspective of behavioral finance. Our experiment demonstrates that the Classify-and-Rethink prompt\n        approach effectively overcomes the framing effect and facilitates excess returns. Compared to alternative prompt\n        designs, the Classify-and-Rethink strategy exhibits higher yields and Sharpe ratios. This framework provides a\n        valuable basis for future research on behavioral finance in large-scale analysis of financial texts and offers\n        investors a reliable means of mitigating behavioral biases.\n\n        <div class=\"cente-title\">#17. FinVis-GPT: A Multimodal Large Language Model for Financial Chart Analysis</div>\n        Abstract: In this paper, we propose FinVis-GPT, a novel multimodal large language model (LLM) specifically\n        designed for financial chart analysis. By leveraging the power of LLMs and incorporating instruction tuning and\n        multimodal capabilities, FinVis-GPT is capable of interpreting financial charts and providing valuable analysis.\n        To train FinVis-GPT, a financial task oriented dataset was generated for pretraining alignment and instruction\n        tuning, comprising various types of financial charts and their corresponding descriptions. We evaluate the model\n        performance via several case studies due to the time limit, and the promising results demonstrated that\n        FinVis-GPT is superior in various financial chart related tasks, including generating descriptions, answering\n        questions and predicting future market trends, surpassing existing state-of-the-art multimodal LLMs. The\n        proposed FinVis-GPT serves as a pioneering effort in utilizing multimodal LLMs in the finance domain and our\n        generated dataset will be release for public use in the near future to speedup related research.\n\n        <div class=\"cente-title\">\n          #18. XuanYuan: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters\n        </div>\n        Abstract: In recent years, pre-trained language models have undergone rapid development with the emergence of\n        large-scale models. However, there is a lack of open-sourced chat models specifically designed for the Chinese\n        language, especially in the field of Chinese finance, at the scale of hundreds of billions. To address this gap,\n        we introduce XuanYuan 2.0 (轩辕 2.0), the largest Chinese chat model to date, built upon the BLOOM-176B\n        architecture. To enable the model to generate a larger quantity of correct and financial instruction data, we\n        introduce an innovative framework named SELF-QA, which replaces the traditional practice of human-written\n        instruction seeds with a vast amount of unsupervised knowledge. Additionally, we propose a novel training method\n        called Hybrid-tuning to mitigate catastrophic forgetting. By combining general-domain with domain-specific\n        knowledge and integrating the stages of pre-training and fine-tuning, XuanYuan 2.0 is capable of providing\n        accurate and contextually appropriate responses in the Chinese financial domain. Our model has been made public\n        on Github and Huggingface, and has received widespread attention.\n\n        <div class=\"cente-title\">\n          #19. CGCE: A Chinese Generative Chat Evaluation Benchmark for General and Financial Domains\n        </div>\n        Abstract: Generative chat models, such as ChatGPT and GPT-4, have revolutionized natural language generation\n        (NLG) by incorporating instructions and human feedback to achieve significant performance improvements. However,\n        the lack of standardized evaluation benchmarks for chat models, particularly for Chinese and domain-specific\n        models, hinders their assessment and progress. To address this gap, we introduce the Chinese Generative Chat\n        Evaluation (CGCE) benchmark, focusing on general and financial domains. The CGCE benchmark encompasses diverse\n        tasks, including 300 questions in the general domain and 150 specific professional questions in the financial\n        domain. Manual scoring evaluates factors such as accuracy, coherence, expression clarity, and completeness. To\n        the best of our knowledge, the financial evaluation section of CGCE is the first evaluation benchmark for\n        largescale chat models in the Chinese financial domain. The CGCE benchmark provides researchers with a\n        standardized framework to assess and compare Chinese generative chat models, fostering advancements in NLG\n        research.\n      </div>\n    </div>\n    <div class=\"footer padding\">\n      <div>\n        <div class=\"email-icon\"></div>\n        <div class=\"email-info\">\n          <p>If you have any question, feel free to contact us:</p>\n          <p style=\"color: #b3ebff\">finllm2023@easychair.org</p>\n        </div>\n      </div>\n    </div>\n  </div>\n</template>\n\n<script>\nexport default {\n  data() {\n    return {\n      timeList: [\n        // {\n        //   time: '08:30-09:00',\n        //   title: 'Registeration',\n        //   img: require('../../assets/img/6.png'),\n        //   name: 'Shuoling Liu',\n        //   note: 'Online 20min talk + 5 min QA',\n        // },\n        {\n          time: '09:00-09:05',\n          title: 'Opening Speech',\n          img: require('../../assets/img/6.png'),\n          name: 'Shuoling Liu',\n          note: 'Online 20min talk + 5 min QA',\n        },\n        {\n          time: '09:05-09:30',\n          title: 'Invited talk 1',\n          img: require('../../assets/img/6.png'),\n          name: 'Qiang Yang',\n          text: 'Federated Large Language Models',\n          note: 'Online 20min talk + 5 min QA',\n        },\n        {\n          time: '09:30-09:55',\n          title: 'Invited talk 2',\n          img: require('../../assets/img/7.png'),\n          name: 'Li Deng',\n          text: 'GenAI in Quant Finance',\n          note: 'Online 20min talk + 5 min QA',\n        },\n        {\n          time: '09:55-10:20',\n          title: 'Invited talk 3',\n          img: require('../../assets/img/6.png'),\n          name: 'Jie Tang',\n          text: 'TBA',\n          note: 'Online 20min talk + 5 min QA',\n        },\n        {\n          time: '10:20-10:35',\n          name: 'Break',\n        },\n        {\n          time: '10:35-11:05',\n          title: 'Invited talk 4',\n          img: require('../../assets/img/Mark Dredze.png'),\n          name: 'Mark Dredze(VCR)',\n          text: 'BloombergGPT: A Large Language Model for Finance',\n          note: 'Online 20min talk + 5 min QA',\n        },\n        {\n          time: '11:05-11:20',\n          title: 'Oral presentation 1',\n          name: 'XuanYuan',\n          text: ' A Large Chinese Financial Chat Model with Hundreds of Billions Parameters,Xuanyuu Zhang(VCR)',\n          note: 'Online 10 min pre+5min QA',\n        },\n        {\n          time: '11:20-11:35',\n          title: 'Oral presentation 2',\n          name: 'Hongyang Yang',\n          text: 'FinGPT: Open-Source Financial Large Language Models',\n          note: 'Online 10 min pre+5min QA',\n        },\n        {\n          time: '11:35-11:50',\n          title: 'Oral presentation 3',\n          name: 'Hongyang Yang',\n          text: 'Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models',\n          note: 'Online 10 min pre+5min QA',\n        },\n        {\n          time: '13:00-13:25',\n          title: 'Invited talk 5',\n          img: require('../../assets/img/白硕.jpg'),\n          name: 'Shuo Bai',\n          text: 'LLMs：Trends and Paths to Finance Landing',\n          note: '25min talk + 5 min QA',\n        },\n        {\n          time: '13:25-13:50',\n          title: 'Invited talk 6',\n          img: require('../../assets/img/王巍巍.jpg'),\n          name: 'Weiwei Wang',\n          text: 'The challenges and paths to applying large-scale models',\n          note: '25min talk + 5 min QA',\n        },\n        {\n          time: '13:50-14:15',\n          title: 'Invited talk 7',\n          img: require('../../assets/img/6.png'),\n          name: 'Ding Chen',\n          text: 'Fusional or Authentic? Integrating Large Language Model and “Small” Quantitative Model for Fixed-Income Investment, a Practitioners Perspective',\n          note: '25min talk + 5 min QA',\n        },\n        {\n          time: '14:15-14:40',\n          title: 'Invited talk 8',\n          img: require('../../assets/img/6.png'),\n          name: 'Donglin Mao',\n          text: 'Practice of Improving Digital Investment Research Efficiency with Large Language Models',\n          note: '25min talk + 5 min QA',\n        },\n        {\n          time: '14:40-15:05',\n          title: 'Invited talk 9',\n          img: require('../../assets/img/6.png'),\n          name: 'Yunan Zhou',\n          text: 'Introduction to Automated Machine Learning in Financial Quantitative Analysis',\n          note: '25min talk + 5 min QA',\n        },\n        {\n          time: '15:05-15:20',\n          name: 'Break',\n        },\n        {\n          time: '15:20-15:45',\n          title: 'Invited talk 10',\n          name: 'Junjie Lai',\n          text: 'Efficient deployment of large language models',\n          note: 'Online 10 min pre+5min QA ',\n        },\n        {\n          time: '15:45-16:10',\n          title: 'Invited talk 11',\n          name: 'Changle Lin',\n          text: 'Using Data Asset Graph for Data Equity Sharing and Model Accuracy Improvement for Financial LLM',\n          note: 'Online 10 min pre+5min QA ',\n        },\n        {\n          time: '16:10-16:25',\n          title: 'Oral presentations 4',\n          name: 'Xiaofeng Zhang',\n          text: 'An Effective Data Creation Pipeline for Generating Highquality Financial Instruction Data',\n          note: 'Online 10 min pre+5min QA',\n        },\n        {\n          time: '16:25-16:40',\n          title: 'Oral presentations 5',\n          name: 'Yanru Wu(VCR)',\n          text: 'Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models',\n          note: 'Online 15 min pre',\n        },\n        {\n          time: '16:40-16:55',\n          title: 'Oral presentations 6',\n          name: 'Zhihua Jin(VCR)',\n          text: 'NumGPT: Improving Numeracy Ability of Generative Pretrained Models',\n          note: 'Online 15 min pre',\n        },\n        {\n          time: '16:55-17:10',\n          title: 'Oral presentations 7',\n          name: 'Junyang Li',\n          text: 'Financial Large Language Models: A Survey',\n          note: 'Online 15 min pre',\n        },\n        {\n          time: '17:10-17:25',\n          title: 'Oral presentations 8',\n          name: 'Gaoguo Jia/Yuhang Jiang',\n          text: 'Can ChatGPT Overcome Behavioral Biases in the Financial Sector? Classify-and-Rethink: Multi-Step Zero-Shot Reasoning in the Gold Investment',\n          note: 'Online 15 min pre',\n        },\n        {\n          time: '17:25-17:40',\n          title: 'Oral presentations 9',\n          name: 'Tianyi Ma',\n          text: 'Integrating Stock Features and Global Information via Large Language Models for Enhanced Stock Return Prediction',\n          note: 'Online 15 min pre',\n        },\n        {\n          time: '17:40-18:00',\n          name: 'Award Ceremony',\n        },\n      ],\n    }\n  },\n}\n</script>\n\n<style lang=\"less\" scoped>\n@import '../../style/public.less';\n.flex {\n  display: flex;\n  align-items: center;\n  justify-content: space-between;\n}\n.header,\n.footer {\n  min-width: 1715px;\n  height: 103px;\n  line-height: 103px;\n  font-size: 28px;\n  color: #ffffff;\n  letter-spacing: 0;\n  font-weight: 700;\n  background: url('@/assets/img/CTA.png') no-repeat;\n  background-size: cover;\n  .right {\n    width: 172px;\n    height: 64px;\n    @media (max-width: 750px) {\n      width: 300px;\n      height: 120px;\n    }\n    img {\n      width: 100%;\n      height: 100%;\n      vertical-align: top;\n    }\n  }\n}\n.footer {\n  height: 151px;\n  margin-top: 100px;\n  font-size: 20px;\n  color: #ffffff;\n  font-weight: 500;\n  display: flex;\n  align-items: center;\n  & > div {\n    display: flex;\n    .email-icon {\n      width: 30px;\n      height: 30px;\n      margin-right: 9px;\n      background: url('@/assets/img/email.png') no-repeat;\n      background-size: cover;\n    }\n    p {\n      margin: 0;\n      line-height: 1.5;\n    }\n  }\n}\n.padding {\n  padding: 0 130px;\n}\n\n.w {\n  min-width: 1715px;\n  max-width: 1920px;\n  margin: 0 auto;\n}\n\n.brochure {\n  margin-top: 80px;\n}\n\nh1 {\n  font-size: 48px;\n  color: #0d152e;\n  line-height: 60px;\n  font-weight: 700;\n  display: flex;\n  align-items: center;\n\n  @media (max-width: 750px) {\n    font-size: 58px;\n  }\n\n  &::before {\n    content: '';\n    display: inline-block;\n    width: 10px;\n    height: 50px;\n    background-color: #4774f4;\n    margin-right: 10px;\n    vertical-align: baseline;\n  }\n}\n\n.subtitle {\n  background: url('@/assets/img/header-bg.jpg') no-repeat;\n  height: 240px;\n  box-sizing: border-box;\n  background-size: 100% 100%;\n  display: flex;\n  align-items: center;\n\n  @media (max-width: 750px) {\n    height: 300px;\n  }\n\n  .sub-text {\n    font-size: 43px;\n    color: #ffffff;\n    font-weight: 700;\n\n    @media (max-width: 750px) {\n      font-size: 80px;\n    }\n  }\n}\n\n.mg-top {\n  margin-top: 50px;\n}\n\n.time-list-box {\n  position: relative;\n\n  .time-box {\n    display: flex;\n    align-items: start;\n    margin-bottom: 24px;\n\n    &:not(:last-child) {\n      .time {\n        .big-rect {\n          &::after {\n            content: '';\n            position: absolute;\n            border-left: 1px dashed #4774f4;\n            height: 280px;\n            top: 5px;\n          }\n        }\n      }\n    }\n\n    .time {\n      display: flex;\n      align-items: center;\n\n      @media (max-width: 750px) {\n        font-size: 24px;\n      }\n\n      .big-rect {\n        position: relative;\n        width: 17px;\n        height: 17px;\n        display: flex;\n        justify-content: center;\n        align-items: center;\n        background-color: #edf1fe;\n        border-radius: 50%;\n        margin-left: 27.5px;\n\n        @media (max-width: 750px) {\n          width: 34px;\n          height: 34px;\n        }\n\n        .small-rect {\n          width: 10px;\n          height: 10px;\n          background-color: #4774f4;\n          border-radius: 50%;\n\n          @media (max-width: 750px) {\n            width: 20px;\n            height: 20px;\n          }\n        }\n      }\n    }\n\n    .time-centen {\n      flex: 1;\n      background-color: #f7f9fb;\n      border-radius: 8px;\n      margin-left: 16px;\n      display: flex;\n      flex-direction: column;\n      justify-content: center;\n      box-sizing: border-box;\n      padding: 24px 40px;\n      height: 260px;\n\n      .title {\n        font-size: 26px;\n        color: #0d152e;\n        margin-bottom: 25px;\n\n        @media (max-width: 750px) {\n          font-size: 46px;\n        }\n      }\n\n      .avtar-box {\n        font-size: 18px;\n        color: #292f36;\n        display: flex;\n        align-items: center;\n\n        .avtar {\n          width: 40px;\n          height: 40px;\n          margin-right: 12px;\n          border-radius: 50%;\n          overflow: hidden;\n\n          @media (max-width: 750px) {\n            width: 80px;\n            height: 80px;\n          }\n\n          img {\n            width: 100%;\n            height: 100%;\n          }\n        }\n\n        .name {\n          font-size: 18px;\n          color: #292f36;\n\n          @media (max-width: 750px) {\n            font-size: 28px;\n          }\n        }\n      }\n\n      .note {\n        margin-top: 25px;\n        font-size: 16px;\n        color: #81838c;\n        margin-bottom: 0;\n\n        @media (max-width: 750px) {\n          font-size: 28px;\n        }\n      }\n    }\n  }\n\n  .line {\n    position: absolute;\n    top: 15px;\n    bottom: 190px;\n    left: 176px;\n    border-left: 1px dashed #4774f4;\n    box-sizing: border-box;\n  }\n}\n\n.cente-text {\n  font-size: 20px;\n  color: #81838c;\n  text-align: justify;\n  line-height: 1.5;\n\n  @media (max-width: 750px) {\n    font-size: 40px;\n  }\n\n  .cente-title {\n    font-size: 26px;\n    color: #81838c;\n    margin-top: 68px;\n    font-weight: 500;\n    text-align: left;\n\n    @media (max-width: 750px) {\n      font-size: 52px;\n    }\n  }\n}\n\n.mg-bottom {\n  margin-bottom: 50px;\n}\n\n@media (max-width: 750px) {\n  .header,\n  .footer,\n  .middle {\n    min-width: 375px;\n  }\n  .header {\n    height: 200px;\n    line-height: 200px;\n    font-size: 80px;\n  }\n  .w {\n    min-width: 100vw;\n    max-width: 100vw;\n  }\n  .center-box {\n    .cente-text {\n      line-height: 50px;\n    }\n    .item-box {\n      line-height: 60px;\n      display: block;\n      .item-box-left,\n      .item-box-right {\n        width: 100%;\n        text-align: left;\n      }\n      .item-box-left {\n        margin-bottom: 30px;\n        background-position-x: 700px;\n      }\n      .media-bottom {\n        margin-bottom: 0;\n      }\n    }\n    .card-box {\n      flex-wrap: wrap;\n      .card {\n        width: 49%;\n        margin-bottom: 30px;\n        .title {\n          margin-bottom: 20px;\n        }\n      }\n    }\n  }\n  .middle {\n    height: auto;\n    padding: 72px 0;\n    & > div {\n      .middle-mg {\n        margin: 0;\n      }\n      & > div {\n        display: unset;\n        & > div {\n          line-height: 60px;\n        }\n      }\n    }\n  }\n}\n\n/* 针对 Firefox 浏览器 */\n@supports (-moz-appearance: none) {\n  @media (max-width: 750px) {\n    .public-mixin();\n  }\n}\n\n/* 针对 iOS 设备，Retina 屏幕 */\n@media screen and (-webkit-min-device-pixel-ratio: 2) and (max-width: 750px) {\n  .public-mixin();\n}\n\n/* 针对 iOS 设备，非 Retina 屏幕 */\n@media screen and (-webkit-max-device-pixel-ratio: 1) and (max-width: 750px) {\n  .public-mixin();\n}\n\n/* 针对特定的 iOS 设备（例如：iPhone 6/7/8 Plus） */\n@media screen and (device-aspect-ratio: 16/9) and (-webkit-min-device-pixel-ratio: 3) and (max-width: 750px) {\n  .public-mixin();\n}\n</style>\n","import mod from \"-!../../../node_modules/.pnpm/registry.npmmirror.com+thread-loader@3.0.4_webpack@5.88.2/node_modules/thread-loader/dist/cjs.js!../../../node_modules/.pnpm/registry.npmmirror.com+babel-loader@8.3.0_@babel+core@7.12.16_webpack@5.88.2/node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../../node_modules/.pnpm/registry.npmmirror.com+vue-loader@15.10.1_css-loader@6.8.1_vue-template-compiler@2.6.14_webpack@5.88.2/node_modules/vue-loader/lib/index.js??vue-loader-options!./index.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../../node_modules/.pnpm/registry.npmmirror.com+thread-loader@3.0.4_webpack@5.88.2/node_modules/thread-loader/dist/cjs.js!../../../node_modules/.pnpm/registry.npmmirror.com+babel-loader@8.3.0_@babel+core@7.12.16_webpack@5.88.2/node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../../node_modules/.pnpm/registry.npmmirror.com+vue-loader@15.10.1_css-loader@6.8.1_vue-template-compiler@2.6.14_webpack@5.88.2/node_modules/vue-loader/lib/index.js??vue-loader-options!./index.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./index.vue?vue&type=template&id=30d900d8&scoped=true&\"\nimport script from \"./index.vue?vue&type=script&lang=js&\"\nexport * from \"./index.vue?vue&type=script&lang=js&\"\nimport style0 from \"./index.vue?vue&type=style&index=0&id=30d900d8&prod&lang=less&scoped=true&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/.pnpm/registry.npmmirror.com+vue-loader@15.10.1_css-loader@6.8.1_vue-template-compiler@2.6.14_webpack@5.88.2/node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"30d900d8\",\n  null\n  \n)\n\nexport default component.exports","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"conference-brochure\"},[_vm._m(0),_c('h1',{staticClass:\"title w padding\"},[_vm._v(\"FinLLM Conference Brochure\")]),_vm._m(1),_c('div',{staticClass:\"jz-box w padding\"},_vm._l((6),function(item){return _c('img',{key:item,attrs:{\"src\":require((\"../../assets/img/jiangzhuang (\" + item + \").jpg\")),\"alt\":\"\"}})}),0),_vm._m(2)])}\nvar staticRenderFns = [function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"header\"},[_c('div',{staticClass:\"w padding flex\"},[_c('span',[_vm._v(\"FinLLM 2023\")]),_c('div',{staticClass:\"right\"},[_c('img',{attrs:{\"src\":require(\"@/assets/img/ijcai.png\"),\"alt\":\"\"}})])])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"centen-box w padding\"},[_c('h1',{staticClass:\"big-title\"},[_vm._v(\"Scope and Objective\")]),_c('div',{staticClass:\"text\"},[_vm._v(\" Pretrained Large Language Models (LLMs) have demonstrated tremendous potential in various natural language processing tasks, including language generation, machine translation, and question answering. In the financial services industry, pretrained LLMs have the potential to significantly impact tasks such as financial forecasting, risk management, and sentiment analysis. Moreover, LLMs can help automate the process of analyzing financial reports and extracting key insights that can aid businesses in making informed decisions. This symposium provides a platform for researchers, practitioners, and industry experts from around the world to share new ideas, exchange research findings, and discuss the challenges and opportunities in the field of pretrained LLMs for financial services. The symposium will cover two themes: 1) potential applications and best practices of pretrained LLMs for financial services; and 2) challenges that need to be addressed to make them efficient, effective, and trustworthy. The symposium will also feature invited talks by leading researchers and industry experts, as well as panel discussions on the latest trends and challenges in the field. We welcome researchers, practitioners, and industry experts from academia and industry to submit their work and participate in this exciting event. \")]),_c('h1',{staticClass:\"big-title\"},[_vm._v(\"Accepted Papers\")]),_c('div',{staticClass:\"cente-text mg-bottom\"},[_c('div',{staticClass:\"cente-title mg\"},[_vm._v(\"Oral Presentation 1: Financial Large Language Models: A Survey\")]),_vm._v(\" Abstract: In recent years, the emergence of a series of large language models, such as ChatGPT, and their powerful performance have garnered significant attention from both academia and industry. Expanding the application of these models and technologies to specialized fields, specifically the financial industry, has become an intriguing research topic, given its potential for immense value and impact. However, it is apparent that the financial industry presents a unique set of challenges for general large language models, due to its inherently complex and dynamic nature. In this survey, we will review the existing academic work on large language models in the financial domain, and summarize the unique and specific features of the financial field compared to general domains. In addition, we will examine several key peculiarities and discuss relevant research works in the field of large language models. Ultimately, the findings of this paper provide insights into the challenges and opportunities of deploying large language models in the financial field, and contribute to a better understanding of the possibilities for optimizing their performance in complex and specialized domains. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\"Oral Presentation 2: FinGPT: Open-Source Financial Large Language Models\")]),_vm._v(\" Abstract: Large language models (LLMs) have shown the potential of revolutionizing natural language processing in diverse domains, sparking great interest in finance. However, the finance domain presents unique challenges, including high temporal sensitivity, constant dynamism, and a low signal-to-noise ratio (SNR). While proprietary models like BloombergGPT have taken advantage of their unique data accumulation, such privileged access calls for an open-source alternative to democratize internet-scale financial data. In this paper, we present an open-source large language model, FinGPT, for the finance sector. Unlike proprietary models, FinGPT takes a data-centric approach, providing researchers and practitioners with accessible and transparent resources to customize their financial LLMs (FinLLMs). We highlight the importance of an automatic data curation pipeline and the lightweight low-rank adaptation technique in building FinGPT. Furthermore, we will showcase potential applications as stepping stones for users, such as robo-advising and sentiment analysis. Through collaborative efforts within the opensource AI4Finance community, FinGPT aims to stimulate innovation, democratize FinLLMs, and unlock new opportunities in open finance. Two associated code repos are https://github.com/AI4Finance-Foundation/FinGPT and https://github.com/AI4Finance-Foundation/FinNLP \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\" Oral Presentation 3: Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models \")]),_vm._v(\" Abstract: Sentiment analysis is a vital tool for uncovering insights from financial articles, news, and social media, shaping our understanding of market movements. Despite the impressive capabilities of large language models (LLMs) in financial natural language processing (NLP), they still struggle with accurately interpreting numerical values and grasping financial context, limiting their effectiveness in predicting financial sentiment. In this paper, we introduce a simple yet effective instruction tuning approach to address these issues. By transforming a small portion of supervised financial sentiment analysis data into instruction data and finetuning a general-purpose LLM with this method, we achieve remarkable advancements in financial sentiment analysis. In the experiment, our approach outperforms state-of-the-art supervised sentiment analysis models, as well as widely used LLMs like ChatGPT and LLaMAs, particularly in scenarios where numerical understanding and contextual comprehension are vital. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\" Oral Presentation 4: An Effective Data Creation Pipeline to Generate High-Quality Financial Instruction Data for Large Language Model \")]),_vm._v(\" Abstract: At the beginning era of large language model, it is quite critical to generate a high-quality financial dataset to fine-tune a large language model for financial related tasks. Thus, this paper presents a carefully designed data creation pipeline for this purpose. Particularly, we initiate a dialogue between an AI investor and financial expert using ChatGPT and incorporate the feedback of human financial experts, leading to the refinement of the dataset. This pipeline yielded a robust instruction tuning dataset comprised of 103k multi-turn chats. Extensive experiments have been conducted on this dataset to evaluate the model’s performance by adopting an external GPT-4 as the judge. The promising experimental results verify that our approach led to significant advancements in generating accurate, relevant, and financial-style responses from AI models, and thus providing a powerful tool for applications within the financial sector. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\" Oral Presentation 5: Integrating Stock Features and Global Information via Large Language Models for Enhanced Stock Return Prediction \")]),_vm._v(\" Abstract: The remarkable achievements and rapid advancements of Large Language Models (LLMs) such as ChatGPT and GPT-4 have showcased their immense potential in quantitative investment. Traders can effectively leverage these LLMs to analyze financial news and predict stock returns accurately. However, integrating LLMs into existing quantitative models presents two primary challenges: the insufficient utilization of semantic information embedded within LLMs and the difficulties in aligning the latent information within LLMs with pre-existing quantitative stock features. We propose a novel framework consisting of two components to surmount these challenges. The first component, the Local Global model, approaches understanding stock features and financial news (through LLMs) as a unified problem. The second component, Self-Correlated Reinforcement Learning, focuses on aligning the embeddings of financial news generated by LLMs with stock features within the same semantic space. By implementing our framework, we have demonstrated superior performance in Rank Information Coefficient and returns, particularly compared to models relying only on stock features in the China A-share market. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\" Oral Presentation 6: Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models \")]),_vm._v(\" Abstract: Entity-level fine-grained sentiment analysis in the financial domain is a crucial subtask of sentiment analysis and currently faces numerous challenges. The primary challenge stems from the lack of high-quality and large-scale annotated corpora specifically designed for financial text sentiment analysis, which in turn limits the availability of data necessary for developing effective text processing techniques. Recent advancements in large language models (LLMs) have yielded remarkable performance in natural language processing tasks, primarily centered around language pattern matching. In this paper, we propose a novel and extensive Chinese fine-grained financial sentiment analysis dataset, FinChina SA, for enterprise early warning. We thoroughly evaluate and experiment with well-known existing open-source LLMs using our dataset. We firmly believe that our dataset will serve as a valuable resource to advance the exploration of real-world financial sentiment analysis tasks, which should be the focus of future research. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\" Oral Presentation 7: NumGPT: Improving Numeracy Ability of Generative Pre-trained Models \")]),_vm._v(\" Abstract: Existing generative pre-trained language models (e.g., GPT) focus on modeling the language structure and semantics of general texts. However, those models do not consider the numerical properties of numbers and cannot perform robustly on numerical reasoning tasks (e.g., math word problems and measurement estimation). In this paper, we propose NumGPT, a generative pre-trained model that explicitly models the numerical properties of numbers in texts. Specifically, it leverages a prototype-based numeral embedding to encode the mantissa of the number and an individual embedding to encode the exponent of the number. A numeral-aware loss function is designed to integrate numerals into the pre-training objective of NumGPT. We conduct extensive experiments on four different datasets to evaluate the numeracy ability of NumGPT. The experiment results show that NumGPT outperforms baseline models (e.g., GPT and GPT with DICE) on a range of numerical reasoning tasks such as measurement estimation, number comparison, math word problems, and magnitude classification. Ablation studies are also conducted to evaluate the impact of pre-training and model hyperparameters on the performance. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\" Can ChatGPT Overcome Behavioral Biases in the Financial Sector? Classify-and-Rethink: Multi-Step Zero-Shot Reasoning in the Gold Investment \")]),_vm._v(\" Abstract: The framing effect is a phenomenon observed in both humans and large language models where different descriptions of the same objectively identical problem can lead to different decisions. In behavioral finance, this effect must be carefully managed to prevent undesirable outcomes in the investment process. In the application of large language models, prompt engineering is often employed to mitigate the framing effect. While previous research in this domain has focused on sentiment classification or subject recognition, this study employs a large language model to score gold-related news and designs the ”Classify-and-Rethink” prompt strategy from the perspective of behavioral finance. Our experiment demonstrates that the Classify-and-Rethink prompt approach effectively overcomes the framing effect and facilitates excess returns. Compared to alternative prompt designs, the Classify-and-Rethink strategy exhibits higher yields and Sharpe ratios. This framework provides a valuable basis for future research on behavioral finance in large-scale analysis of financial texts and offers investors a reliable means of mitigating behavioral biases. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\" Oral Presentation 9: XuanYuan: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters \")]),_vm._v(\" Abstract: In recent years, pre-trained language models have undergone rapid development with the emergence of large-scale models. However, there is a lack of open-sourced chat models specifically designed for the Chinese language, especially in the field of Chinese finance, at the scale of hundreds of billions. To address this gap, we introduce XuanYuan 2.0 (轩辕 2.0), the largest Chinese chat model to date, built upon the BLOOM-176B architecture. To enable the model to generate a larger quantity of correct and financial instruction data, we introduce an innovative framework named SELF-QA, which replaces the traditional practice of human-written instruction seeds with a vast amount of unsupervised knowledge. Additionally, we propose a novel training method called Hybrid-tuning to mitigate catastrophic forgetting. By combining general-domain with domain-specific knowledge and integrating the stages of pre-training and fine-tuning, XuanYuan 2.0 is capable of providing accurate and contextually appropriate responses in the Chinese financial domain. Our model has been made public on Github and Huggingface, and has received widespread attention. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\"SPAM-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection\")]),_vm._v(\" Abstract: This paper investigates the effectiveness of large language models (LLMs) in email spam detection by comparing prominent models from three distinct families: BERT-like, Sentence Transformers, and Seq2Seq. Additionally, we examine well-established machine learning techniques for spam detection, such as Naïve Bayes and LightGBM, as baseline methods. We assess the performance of these models across four public datasets, utilizing different numbers of training samples (full training set and few-shot settings). Our findings reveal that, in the majority of cases, LLMs surpass the performance of the popular baseline techniques, particularly in few-shot scenarios. This adaptability renders LLMs uniquely suited to spam detection tasks, where labeled samples are limited in number and models require frequent updates. Additionally, we introduce SPAM-T5, a Flan-T5 model that has been specifically adapted and fine-tuned for the purpose of detecting email spam. Our results demonstrate that SPAM-T5 surpasses baseline models and other LLMs in the majority of scenarios, particularly when there are a limited number of training samples available. Our code is publicly available at https://github.com/jpmorganchase/llm-email-spam-detection. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\" Exploring In-Context Learning for Overnight Stock Price Movement Prediction with Large Language Models \")]),_vm._v(\" Abstract: Stock price movement prediction is an important task in the financial domain since accurate price prediction for the next trading day brings great benefits for making decisions on stock transactions. Except for some common indicators like 20-days moving average, time-series regression models including linear regression, RNN-based models, and Transformer achieve satisfying performance in this task and are widely applied. However, all methods mentioned above only exploit historical data to predict stock price movement in the next trading day. Apart from stock historical data, data in text modality like finance-related news or social platform contents can also reflect the moving trend of stocks. Instead of training a new model to learn the representation of text data, we use large language models (LLMs) to directly predict the price movement of stocks. Considering the possible lack of domain data in training corpora of LLMs, we introduce in-context learning to explore if LLMs can learn necessary knowledge from given prediction examples consisting of both historical data and text data. In this paper, we focus on overnight stock price movement prediction. Our experiment results show that LLMs can boost performance in this task compared to competitive baselines and in-context learning can be beneficial for prediction in some cases. Our further analyses explore the influence of data modalities, sample selection strategies, and the number of examples on models’ performance. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\"The Challenges of LLMs in Financial Services\")]),_vm._v(\" Abstract: With the rapid development of Large Language Models (LLMs), the financial services sector has shown outstanding advantages and potential. At the same time, risks and challenges for financial services are brought into traditional businesses, such as data privacy, ethics, misinformation, disinformation, and social injustices. In light of those challenges, this paper reports on a mixed-methods study with corporate middle management, high management, and technical staff. Our preliminary findings suggest that financial services practitioners are optimistic about the future of LLMs but distrustful of existing ones. Our paper should be read as a preliminary study rather than a complete research work. Thus our suggestions on improving LLMs for specific domains, such as financial services, are, hopefully, useful to those who share similar interests in improving LLMs for realistic use. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\" Unveiling the Potential of Sentiment: Can Large Language Models Predict Chinese Stock Price Movements? \")]),_vm._v(\" Abstract: The rapid advancement of Large Language Models (LLMs) has led to extensive discourse regarding their potential to boost the return of quantitative stock trading strategies. This discourse primarily revolves around harnessing the remarkable comprehension capabilities of LLMs to extract sentiment factors which facilitate informed and high-frequency investment portfolio adjustments. To ensure successful implementations of these LLMs into the analysis of Chinese financial texts and the subsequent trading strategy development within the Chinese stock market, we provide a rigorous and encompassing benchmark as well as a standardized back-testing framework aiming at objectively assessing the efficacy of various types of LLMs in the specialized domain of sentiment factor extraction from Chinese news text data. To illustrate how our benchmark works, we reference three distinctive models: 1) the generative LLM (ChatGPT), 2) the Chinese language-specific pre-trained LLM (Erlangshen-RoBERTa), and 3) the financial domain-specific fine-tuned LLM classifier(Chinese FinBERT). We apply them directly to the task of sentiment factor extraction from large volumes of Chinese news summary texts. We then proceed to building quantitative trading strategies and running back-tests under realistic trading scenarios based on the derived sentiment factors and evaluate their performances with our benchmark. By constructing such a comparative analysis, we invoke the question of what constitutes the most important element for improving a LLM’s performance on extracting sentiment factors. And by ensuring that the LLMs are evaluated on the same benchmark, following the same standardized experimental procedures that are designed with sufficient expertise in quantitative trading, we make the first stride toward answering such a question. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\"FinVis-GPT: A Multimodal Large Language Model for Financial Chart Analysis\")]),_vm._v(\" Abstract: In this paper, we propose FinVis-GPT, a novel multimodal large language model (LLM) specifically designed for financial chart analysis. By leveraging the power of LLMs and incorporating instruction tuning and multimodal capabilities, FinVis-GPT is capable of interpreting financial charts and providing valuable analysis. To train FinVis-GPT, a financial task oriented dataset was generated for pretraining alignment and instruction tuning, comprising various types of financial charts and their corresponding descriptions. We evaluate the model performance via several case studies due to the time limit, and the promising results demonstrated that FinVis-GPT is superior in various financial chart related tasks, including generating descriptions, answering questions and predicting future market trends, surpassing existing state-of-the-art multimodal LLMs. The proposed FinVis-GPT serves as a pioneering effort in utilizing multimodal LLMs in the finance domain and our generated dataset will be release for public use in the near future to speedup related research. \"),_c('div',{staticClass:\"cente-title\"},[_vm._v(\" CGCE: A Chinese Generative Chat Evaluation Benchmark for General and Financial Domains \")]),_vm._v(\" Abstract: Generative chat models, such as ChatGPT and GPT-4, have revolutionized natural language generation (NLG) by incorporating instructions and human feedback to achieve significant performance improvements. However, the lack of standardized evaluation benchmarks for chat models, particularly for Chinese and domain-specific models, hinders their assessment and progress. To address this gap, we introduce the Chinese Generative Chat Evaluation (CGCE) benchmark, focusing on general and financial domains. The CGCE benchmark encompasses diverse tasks, including 300 questions in the general domain and 150 specific professional questions in the financial domain. Manual scoring evaluates factors such as accuracy, coherence, expression clarity, and completeness. To the best of our knowledge, the financial evaluation section of CGCE is the first evaluation benchmark for large-scale chat models in the Chinese financial domain. The CGCE benchmark provides researchers with a standardized framework to assess and compare Chinese generative chat models, fostering advancements in NLG research. \")]),_c('h1',{staticClass:\"big-title\"},[_vm._v(\"Award\")])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"footer padding\"},[_c('div',[_c('div',{staticClass:\"email-icon\"}),_c('div',{staticClass:\"email-info\"},[_c('p',[_vm._v(\"If you have any question, feel free to contact us:\")]),_c('p',{staticStyle:{\"color\":\"#b3ebff\"}},[_vm._v(\"finllm2023@easychair.org\")])])])])}]\n\nexport { render, staticRenderFns }","<template>\r\n  <div class=\"conference-brochure\">\r\n    <div class=\"header\">\r\n      <div class=\"w padding flex\">\r\n        <span>FinLLM 2023</span>\r\n        <div class=\"right\">\r\n          <img\r\n            src=\"@/assets/img/ijcai.png\"\r\n            alt=\"\"\r\n          />\r\n        </div>\r\n      </div>\r\n    </div>\r\n    <h1 class=\"title w padding\">FinLLM Conference Brochure</h1>\r\n    <div class=\"centen-box w padding\">\r\n      <h1 class=\"big-title\">Scope and Objective</h1>\r\n      <div class=\"text\">\r\n        Pretrained Large Language Models (LLMs) have demonstrated tremendous potential in various natural language\r\n        processing tasks, including language generation, machine translation, and question answering. In the financial\r\n        services industry, pretrained LLMs have the potential to significantly impact tasks such as financial\r\n        forecasting, risk management, and sentiment analysis. Moreover, LLMs can help automate the process of analyzing\r\n        financial reports and extracting key insights that can aid businesses in making informed decisions. This\r\n        symposium provides a platform for researchers, practitioners, and industry experts from around the world to\r\n        share new ideas, exchange research findings, and discuss the challenges and opportunities in the field of\r\n        pretrained LLMs for financial services. The symposium will cover two themes: 1) potential applications and best\r\n        practices of pretrained LLMs for financial services; and 2) challenges that need to be addressed to make them\r\n        efficient, effective, and trustworthy. The symposium will also feature invited talks by leading researchers and\r\n        industry experts, as well as panel discussions on the latest trends and challenges in the field. We welcome\r\n        researchers, practitioners, and industry experts from academia and industry to submit their work and participate\r\n        in this exciting event.\r\n      </div>\r\n      <h1 class=\"big-title\">Accepted Papers</h1>\r\n      <div class=\"cente-text mg-bottom\">\r\n        <div class=\"cente-title mg\">Oral Presentation 1: Financial Large Language Models: A Survey</div>\r\n        Abstract: In recent years, the emergence of a series of large language models, such as ChatGPT, and their\r\n        powerful performance have garnered significant attention from both academia and industry. Expanding the\r\n        application of these models and technologies to specialized fields, specifically the financial industry, has\r\n        become an intriguing research topic, given its potential for immense value and impact. However, it is apparent\r\n        that the financial industry presents a unique set of challenges for general large language models, due to its\r\n        inherently complex and dynamic nature. In this survey, we will review the existing academic work on large\r\n        language models in the financial domain, and summarize the unique and specific features of the financial field\r\n        compared to general domains. In addition, we will examine several key peculiarities and discuss relevant\r\n        research works in the field of large language models. Ultimately, the findings of this paper provide insights\r\n        into the challenges and opportunities of deploying large language models in the financial field, and contribute\r\n        to a better understanding of the possibilities for optimizing their performance in complex and specialized\r\n        domains.\r\n\r\n        <div class=\"cente-title\">Oral Presentation 2: FinGPT: Open-Source Financial Large Language Models</div>\r\n        Abstract: Large language models (LLMs) have shown the potential of revolutionizing natural language processing\r\n        in diverse domains, sparking great interest in finance. However, the finance domain presents unique challenges,\r\n        including high temporal sensitivity, constant dynamism, and a low signal-to-noise ratio (SNR). While proprietary\r\n        models like BloombergGPT have taken advantage of their unique data accumulation, such privileged access calls\r\n        for an open-source alternative to democratize internet-scale financial data. In this paper, we present an\r\n        open-source large language model, FinGPT, for the finance sector. Unlike proprietary models, FinGPT takes a\r\n        data-centric approach, providing researchers and practitioners with accessible and transparent resources to\r\n        customize their financial LLMs (FinLLMs). We highlight the importance of an automatic data curation pipeline and\r\n        the lightweight low-rank adaptation technique in building FinGPT. Furthermore, we will showcase potential\r\n        applications as stepping stones for users, such as robo-advising and sentiment analysis. Through collaborative\r\n        efforts within the opensource AI4Finance community, FinGPT aims to stimulate innovation, democratize FinLLMs,\r\n        and unlock new opportunities in open finance. Two associated code repos are\r\n        https://github.com/AI4Finance-Foundation/FinGPT and https://github.com/AI4Finance-Foundation/FinNLP\r\n\r\n        <div class=\"cente-title\">\r\n          Oral Presentation 3: Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose\r\n          Large Language Models\r\n        </div>\r\n        Abstract: Sentiment analysis is a vital tool for uncovering insights from financial articles, news, and social\r\n        media, shaping our understanding of market movements. Despite the impressive capabilities of large language\r\n        models (LLMs) in financial natural language processing (NLP), they still struggle with accurately interpreting\r\n        numerical values and grasping financial context, limiting their effectiveness in predicting financial sentiment.\r\n        In this paper, we introduce a simple yet effective instruction tuning approach to address these issues. By\r\n        transforming a small portion of supervised financial sentiment analysis data into instruction data and\r\n        finetuning a general-purpose LLM with this method, we achieve remarkable advancements in financial sentiment\r\n        analysis. In the experiment, our approach outperforms state-of-the-art supervised sentiment analysis models, as\r\n        well as widely used LLMs like ChatGPT and LLaMAs, particularly in scenarios where numerical understanding and\r\n        contextual comprehension are vital.\r\n\r\n        <div class=\"cente-title\">\r\n          Oral Presentation 4: An Effective Data Creation Pipeline to Generate High-Quality Financial Instruction Data\r\n          for Large Language Model\r\n        </div>\r\n        Abstract: At the beginning era of large language model, it is quite critical to generate a high-quality\r\n        financial dataset to fine-tune a large language model for financial related tasks. Thus, this paper presents a\r\n        carefully designed data creation pipeline for this purpose. Particularly, we initiate a dialogue between an AI\r\n        investor and financial expert using ChatGPT and incorporate the feedback of human financial experts, leading to\r\n        the refinement of the dataset. This pipeline yielded a robust instruction tuning dataset comprised of 103k\r\n        multi-turn chats. Extensive experiments have been conducted on this dataset to evaluate the model’s performance\r\n        by adopting an external GPT-4 as the judge. The promising experimental results verify that our approach led to\r\n        significant advancements in generating accurate, relevant, and financial-style responses from AI models, and\r\n        thus providing a powerful tool for applications within the financial sector.\r\n\r\n        <div class=\"cente-title\">\r\n          Oral Presentation 5: Integrating Stock Features and Global Information via Large Language Models for Enhanced\r\n          Stock Return Prediction\r\n        </div>\r\n        Abstract: The remarkable achievements and rapid advancements of Large Language Models (LLMs) such as ChatGPT and\r\n        GPT-4 have showcased their immense potential in quantitative investment. Traders can effectively leverage these\r\n        LLMs to analyze financial news and predict stock returns accurately. However, integrating LLMs into existing\r\n        quantitative models presents two primary challenges: the insufficient utilization of semantic information\r\n        embedded within LLMs and the difficulties in aligning the latent information within LLMs with pre-existing\r\n        quantitative stock features. We propose a novel framework consisting of two components to surmount these\r\n        challenges. The first component, the Local Global model, approaches understanding stock features and financial\r\n        news (through LLMs) as a unified problem. The second component, Self-Correlated Reinforcement Learning, focuses\r\n        on aligning the embeddings of financial news generated by LLMs with stock features within the same semantic\r\n        space. By implementing our framework, we have demonstrated superior performance in Rank Information Coefficient\r\n        and returns, particularly compared to models relying only on stock features in the China A-share market.\r\n\r\n        <div class=\"cente-title\">\r\n          Oral Presentation 6: Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models\r\n        </div>\r\n        Abstract: Entity-level fine-grained sentiment analysis in the financial domain is a crucial subtask of sentiment\r\n        analysis and currently faces numerous challenges. The primary challenge stems from the lack of high-quality and\r\n        large-scale annotated corpora specifically designed for financial text sentiment analysis, which in turn limits\r\n        the availability of data necessary for developing effective text processing techniques. Recent advancements in\r\n        large language models (LLMs) have yielded remarkable performance in natural language processing tasks, primarily\r\n        centered around language pattern matching. In this paper, we propose a novel and extensive Chinese fine-grained\r\n        financial sentiment analysis dataset, FinChina SA, for enterprise early warning. We thoroughly evaluate and\r\n        experiment with well-known existing open-source LLMs using our dataset. We firmly believe that our dataset will\r\n        serve as a valuable resource to advance the exploration of real-world financial sentiment analysis tasks, which\r\n        should be the focus of future research.\r\n\r\n        <div class=\"cente-title\">\r\n          Oral Presentation 7: NumGPT: Improving Numeracy Ability of Generative Pre-trained Models\r\n        </div>\r\n        Abstract: Existing generative pre-trained language models (e.g., GPT) focus on modeling the language structure\r\n        and semantics of general texts. However, those models do not consider the numerical properties of numbers and\r\n        cannot perform robustly on numerical reasoning tasks (e.g., math word problems and measurement estimation). In\r\n        this paper, we propose NumGPT, a generative pre-trained model that explicitly models the numerical properties of\r\n        numbers in texts. Specifically, it leverages a prototype-based numeral embedding to encode the mantissa of the\r\n        number and an individual embedding to encode the exponent of the number. A numeral-aware loss function is\r\n        designed to integrate numerals into the pre-training objective of NumGPT. We conduct extensive experiments on\r\n        four different datasets to evaluate the numeracy ability of NumGPT. The experiment results show that NumGPT\r\n        outperforms baseline models (e.g., GPT and GPT with DICE) on a range of numerical reasoning tasks such as\r\n        measurement estimation, number comparison, math word problems, and magnitude classification. Ablation studies\r\n        are also conducted to evaluate the impact of pre-training and model hyperparameters on the performance.\r\n\r\n        <div class=\"cente-title\">\r\n          Can ChatGPT Overcome Behavioral Biases in the Financial Sector? Classify-and-Rethink: Multi-Step Zero-Shot\r\n          Reasoning in the Gold Investment\r\n        </div>\r\n        Abstract: The framing effect is a phenomenon observed in both humans and large language models where different\r\n        descriptions of the same objectively identical problem can lead to different decisions. In behavioral finance,\r\n        this effect must be carefully managed to prevent undesirable outcomes in the investment process. In the\r\n        application of large language models, prompt engineering is often employed to mitigate the framing effect. While\r\n        previous research in this domain has focused on sentiment classification or subject recognition, this study\r\n        employs a large language model to score gold-related news and designs the ”Classify-and-Rethink” prompt strategy\r\n        from the perspective of behavioral finance. Our experiment demonstrates that the Classify-and-Rethink prompt\r\n        approach effectively overcomes the framing effect and facilitates excess returns. Compared to alternative prompt\r\n        designs, the Classify-and-Rethink strategy exhibits higher yields and Sharpe ratios. This framework provides a\r\n        valuable basis for future research on behavioral finance in large-scale analysis of financial texts and offers\r\n        investors a reliable means of mitigating behavioral biases.\r\n\r\n        <div class=\"cente-title\">\r\n          Oral Presentation 9: XuanYuan: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters\r\n        </div>\r\n        Abstract: In recent years, pre-trained language models have undergone rapid development with the emergence of\r\n        large-scale models. However, there is a lack of open-sourced chat models specifically designed for the Chinese\r\n        language, especially in the field of Chinese finance, at the scale of hundreds of billions. To address this gap,\r\n        we introduce XuanYuan 2.0 (轩辕 2.0), the largest Chinese chat model to date, built upon the BLOOM-176B\r\n        architecture. To enable the model to generate a larger quantity of correct and financial instruction data, we\r\n        introduce an innovative framework named SELF-QA, which replaces the traditional practice of human-written\r\n        instruction seeds with a vast amount of unsupervised knowledge. Additionally, we propose a novel training method\r\n        called Hybrid-tuning to mitigate catastrophic forgetting. By combining general-domain with domain-specific\r\n        knowledge and integrating the stages of pre-training and fine-tuning, XuanYuan 2.0 is capable of providing\r\n        accurate and contextually appropriate responses in the Chinese financial domain. Our model has been made public\r\n        on Github and Huggingface, and has received widespread attention.\r\n\r\n        <div class=\"cente-title\">SPAM-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection</div>\r\n        Abstract: This paper investigates the effectiveness of large language models (LLMs) in email spam detection by\r\n        comparing prominent models from three distinct families: BERT-like, Sentence Transformers, and Seq2Seq.\r\n        Additionally, we examine well-established machine learning techniques for spam detection, such as Naïve Bayes\r\n        and LightGBM, as baseline methods. We assess the performance of these models across four public datasets,\r\n        utilizing different numbers of training samples (full training set and few-shot settings). Our findings reveal\r\n        that, in the majority of cases, LLMs surpass the performance of the popular baseline techniques, particularly in\r\n        few-shot scenarios. This adaptability renders LLMs uniquely suited to spam detection tasks, where labeled\r\n        samples are limited in number and models require frequent updates. Additionally, we introduce SPAM-T5, a Flan-T5\r\n        model that has been specifically adapted and fine-tuned for the purpose of detecting email spam. Our results\r\n        demonstrate that SPAM-T5 surpasses baseline models and other LLMs in the majority of scenarios, particularly\r\n        when there are a limited number of training samples available. Our code is publicly available at\r\n        https://github.com/jpmorganchase/llm-email-spam-detection.\r\n\r\n        <div class=\"cente-title\">\r\n          Exploring In-Context Learning for Overnight Stock Price Movement Prediction with Large Language Models\r\n        </div>\r\n        Abstract: Stock price movement prediction is an important task in the financial domain since accurate price\r\n        prediction for the next trading day brings great benefits for making decisions on stock transactions. Except for\r\n        some common indicators like 20-days moving average, time-series regression models including linear regression,\r\n        RNN-based models, and Transformer achieve satisfying performance in this task and are widely applied. However,\r\n        all methods mentioned above only exploit historical data to predict stock price movement in the next trading\r\n        day. Apart from stock historical data, data in text modality like finance-related news or social platform\r\n        contents can also reflect the moving trend of stocks. Instead of training a new model to learn the\r\n        representation of text data, we use large language models (LLMs) to directly predict the price movement of\r\n        stocks. Considering the possible lack of domain data in training corpora of LLMs, we introduce in-context\r\n        learning to explore if LLMs can learn necessary knowledge from given prediction examples consisting of both\r\n        historical data and text data. In this paper, we focus on overnight stock price movement prediction. Our\r\n        experiment results show that LLMs can boost performance in this task compared to competitive baselines and\r\n        in-context learning can be beneficial for prediction in some cases. Our further analyses explore the influence\r\n        of data modalities, sample selection strategies, and the number of examples on models’ performance.\r\n\r\n        <div class=\"cente-title\">The Challenges of LLMs in Financial Services</div>\r\n        Abstract: With the rapid development of Large Language Models (LLMs), the financial services sector has shown\r\n        outstanding advantages and potential. At the same time, risks and challenges for financial services are brought\r\n        into traditional businesses, such as data privacy, ethics, misinformation, disinformation, and social\r\n        injustices. In light of those challenges, this paper reports on a mixed-methods study with corporate middle\r\n        management, high management, and technical staff. Our preliminary findings suggest that financial services\r\n        practitioners are optimistic about the future of LLMs but distrustful of existing ones. Our paper should be read\r\n        as a preliminary study rather than a complete research work. Thus our suggestions on improving LLMs for specific\r\n        domains, such as financial services, are, hopefully, useful to those who share similar interests in improving\r\n        LLMs for realistic use.\r\n\r\n        <div class=\"cente-title\">\r\n          Unveiling the Potential of Sentiment: Can Large Language Models Predict Chinese Stock Price Movements?\r\n        </div>\r\n        Abstract: The rapid advancement of Large Language Models (LLMs) has led to extensive discourse regarding their\r\n        potential to boost the return of quantitative stock trading strategies. This discourse primarily revolves around\r\n        harnessing the remarkable comprehension capabilities of LLMs to extract sentiment factors which facilitate\r\n        informed and high-frequency investment portfolio adjustments. To ensure successful implementations of these LLMs\r\n        into the analysis of Chinese financial texts and the subsequent trading strategy development within the Chinese\r\n        stock market, we provide a rigorous and encompassing benchmark as well as a standardized back-testing framework\r\n        aiming at objectively assessing the efficacy of various types of LLMs in the specialized domain of sentiment\r\n        factor extraction from Chinese news text data. To illustrate how our benchmark works, we reference three\r\n        distinctive models: 1) the generative LLM (ChatGPT), 2) the Chinese language-specific pre-trained LLM\r\n        (Erlangshen-RoBERTa), and 3) the financial domain-specific fine-tuned LLM classifier(Chinese FinBERT). We apply\r\n        them directly to the task of sentiment factor extraction from large volumes of Chinese news summary texts. We\r\n        then proceed to building quantitative trading strategies and running back-tests under realistic trading\r\n        scenarios based on the derived sentiment factors and evaluate their performances with our benchmark. By\r\n        constructing such a comparative analysis, we invoke the question of what constitutes the most important element\r\n        for improving a LLM’s performance on extracting sentiment factors. And by ensuring that the LLMs are evaluated\r\n        on the same benchmark, following the same standardized experimental procedures that are designed with sufficient\r\n        expertise in quantitative trading, we make the first stride toward answering such a question.\r\n\r\n        <div class=\"cente-title\">FinVis-GPT: A Multimodal Large Language Model for Financial Chart Analysis</div>\r\n        Abstract: In this paper, we propose FinVis-GPT, a novel multimodal large language model (LLM) specifically\r\n        designed for financial chart analysis. By leveraging the power of LLMs and incorporating instruction tuning and\r\n        multimodal capabilities, FinVis-GPT is capable of interpreting financial charts and providing valuable analysis.\r\n        To train FinVis-GPT, a financial task oriented dataset was generated for pretraining alignment and instruction\r\n        tuning, comprising various types of financial charts and their corresponding descriptions. We evaluate the model\r\n        performance via several case studies due to the time limit, and the promising results demonstrated that\r\n        FinVis-GPT is superior in various financial chart related tasks, including generating descriptions, answering\r\n        questions and predicting future market trends, surpassing existing state-of-the-art multimodal LLMs. The\r\n        proposed FinVis-GPT serves as a pioneering effort in utilizing multimodal LLMs in the finance domain and our\r\n        generated dataset will be release for public use in the near future to speedup related research.\r\n        <div class=\"cente-title\">\r\n          CGCE: A Chinese Generative Chat Evaluation Benchmark for General and Financial Domains\r\n        </div>\r\n        Abstract: Generative chat models, such as ChatGPT and GPT-4, have revolutionized natural language generation\r\n        (NLG) by incorporating instructions and human feedback to achieve significant performance improvements. However,\r\n        the lack of standardized evaluation benchmarks for chat models, particularly for Chinese and domain-specific\r\n        models, hinders their assessment and progress. To address this gap, we introduce the Chinese Generative Chat\r\n        Evaluation (CGCE) benchmark, focusing on general and financial domains. The CGCE benchmark encompasses diverse\r\n        tasks, including 300 questions in the general domain and 150 specific professional questions in the financial\r\n        domain. Manual scoring evaluates factors such as accuracy, coherence, expression clarity, and completeness. To\r\n        the best of our knowledge, the financial evaluation section of CGCE is the first evaluation benchmark for\r\n        large-scale chat models in the Chinese financial domain. The CGCE benchmark provides researchers with a\r\n        standardized framework to assess and compare Chinese generative chat models, fostering advancements in NLG\r\n        research.\r\n      </div>\r\n      <h1 class=\"big-title\">Award</h1>\r\n    </div>\r\n    <div class=\"jz-box w padding\">\r\n      <img\r\n        v-for=\"item in 6\"\r\n        :key=\"item\"\r\n        :src=\"require(`../../assets/img/jiangzhuang (${item}).jpg`)\"\r\n        alt=\"\"\r\n      />\r\n    </div>\r\n    <div class=\"footer padding\">\r\n      <div>\r\n        <div class=\"email-icon\"></div>\r\n        <div class=\"email-info\">\r\n          <p>If you have any question, feel free to contact us:</p>\r\n          <p style=\"color: #b3ebff\">finllm2023@easychair.org</p>\r\n        </div>\r\n      </div>\r\n    </div>\r\n  </div>\r\n</template>\r\n\r\n<script>\r\nexport default {}\r\n</script>\r\n\r\n<style lang=\"less\" scoped>\r\n@import '../../style/public.less';\r\n.flex {\r\n  display: flex;\r\n  align-items: center;\r\n  justify-content: space-between;\r\n}\r\n.header,\r\n.footer {\r\n  min-width: 1715px;\r\n  height: 103px;\r\n  line-height: 103px;\r\n  font-size: 28px;\r\n  color: #ffffff;\r\n  letter-spacing: 0;\r\n  font-weight: 700;\r\n  background: url('@/assets/img/CTA.png') no-repeat;\r\n  background-size: cover;\r\n  .right {\r\n    width: 172px;\r\n    height: 64px;\r\n    @media (max-width: 750px) {\r\n      width: 300px;\r\n      height: 120px;\r\n    }\r\n    img {\r\n      width: 100%;\r\n      height: 100%;\r\n      vertical-align: top;\r\n    }\r\n  }\r\n}\r\n.footer {\r\n  height: 151px;\r\n  margin-top: 100px;\r\n  font-size: 20px;\r\n  color: #ffffff;\r\n  font-weight: 500;\r\n  display: flex;\r\n  align-items: center;\r\n  & > div {\r\n    display: flex;\r\n    .email-icon {\r\n      width: 30px;\r\n      height: 30px;\r\n      margin-right: 9px;\r\n      background: url('@/assets/img/email.png') no-repeat;\r\n      background-size: cover;\r\n    }\r\n    p {\r\n      margin: 0;\r\n      line-height: 1.5;\r\n    }\r\n  }\r\n}\r\n.padding {\r\n  padding: 0 130px;\r\n}\r\n.title {\r\n  font-size: 40px;\r\n  color: #0d152e;\r\n  font-weight: 600;\r\n  margin-top: 59px;\r\n  @media (max-width: 750px) {\r\n    font-size: 60px;\r\n  }\r\n}\r\n.big-title {\r\n  @media (max-width: 750px) {\r\n    font-size: 60px;\r\n  }\r\n}\r\n.cente-text {\r\n  font-size: 20px;\r\n  color: #81838c;\r\n  text-align: justify;\r\n  line-height: 1.5;\r\n\r\n  @media (max-width: 750px) {\r\n    font-size: 40px;\r\n  }\r\n\r\n  .cente-title {\r\n    font-size: 26px;\r\n    color: #81838c;\r\n    margin-top: 68px;\r\n    font-weight: 500;\r\n    text-align: left;\r\n\r\n    @media (max-width: 750px) {\r\n      font-size: 52px;\r\n    }\r\n  }\r\n  .mg {\r\n    margin-top: 25px !important;\r\n  }\r\n}\r\n.centen-box {\r\n  margin-top: 50px;\r\n  h1 {\r\n    font-size: 32px;\r\n    color: #0d152e;\r\n    line-height: 2;\r\n    font-weight: 600;\r\n    display: flex;\r\n    align-items: center;\r\n    @media (max-width: 750px) {\r\n      font-size: 50px;\r\n    }\r\n    &::before {\r\n      content: '';\r\n      display: inline-block;\r\n      width: 10px;\r\n      height: 35px;\r\n      background-color: #4774f4;\r\n      margin-right: 10px;\r\n      vertical-align: baseline;\r\n      @media (max-width: 750px) {\r\n        height: 50px;\r\n      }\r\n    }\r\n  }\r\n  .text {\r\n    font-size: 20px;\r\n    color: #81838c;\r\n    text-align: justify;\r\n    line-height: 1.5;\r\n    font-weight: 400;\r\n    margin-bottom: 30px;\r\n    @media (max-width: 750px) {\r\n      font-size: 40px;\r\n    }\r\n    .email {\r\n      color: #0088ff;\r\n    }\r\n  }\r\n  .link {\r\n    display: block;\r\n    font-size: 20px;\r\n    color: #0088ff;\r\n    line-height: 2;\r\n    font-weight: 400;\r\n    margin-bottom: 20px;\r\n    @media (max-width: 750px) {\r\n      font-size: 40px;\r\n    }\r\n  }\r\n  .process {\r\n    font-size: 20px;\r\n    color: #81838c;\r\n    line-height: 2;\r\n    font-weight: 400;\r\n    @media (max-width: 750px) {\r\n      font-size: 40px;\r\n    }\r\n    span {\r\n      color: #1eb9e1;\r\n    }\r\n    .process-img {\r\n      display: flex;\r\n      margin-bottom: 28px;\r\n      @media (max-width: 750px) {\r\n        justify-content: space-between;\r\n      }\r\n      & > div {\r\n        width: 200px;\r\n        margin-right: 30px;\r\n        @media (max-width: 750px) {\r\n          width: 48%;\r\n          margin: 0;\r\n        }\r\n        img {\r\n          width: 100%;\r\n          height: 100%;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n.cente-text {\r\n  font-size: 20px;\r\n  color: #81838c;\r\n  text-align: justify;\r\n  line-height: 1.5;\r\n\r\n  @media (max-width: 750px) {\r\n    font-size: 40px;\r\n  }\r\n\r\n  .cente-title {\r\n    font-size: 26px;\r\n    color: #81838c;\r\n    margin-top: 68px;\r\n    font-weight: 500;\r\n    text-align: left;\r\n\r\n    @media (max-width: 750px) {\r\n      font-size: 52px;\r\n    }\r\n  }\r\n}\r\n\r\n.mg-bottom {\r\n  margin-bottom: 50px;\r\n}\r\n@media (max-width: 750px) {\r\n  .header,\r\n  .footer {\r\n    min-width: 375px;\r\n  }\r\n  .header {\r\n    height: 200px;\r\n    line-height: 200px;\r\n    font-size: 80px;\r\n  }\r\n  .w {\r\n    min-width: 100vw;\r\n    max-width: 100vw;\r\n  }\r\n}\r\n.jz-box {\r\n  display: flex;\r\n  justify-content: space-between;\r\n  flex-wrap: wrap;\r\n  img {\r\n    width: 48%;\r\n    margin-bottom: 50px;\r\n  }\r\n}\r\n</style>\r\n","import mod from \"-!../../../node_modules/.pnpm/registry.npmmirror.com+thread-loader@3.0.4_webpack@5.88.2/node_modules/thread-loader/dist/cjs.js!../../../node_modules/.pnpm/registry.npmmirror.com+babel-loader@8.3.0_@babel+core@7.12.16_webpack@5.88.2/node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../../node_modules/.pnpm/registry.npmmirror.com+vue-loader@15.10.1_css-loader@6.8.1_vue-template-compiler@2.6.14_webpack@5.88.2/node_modules/vue-loader/lib/index.js??vue-loader-options!./index.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../../node_modules/.pnpm/registry.npmmirror.com+thread-loader@3.0.4_webpack@5.88.2/node_modules/thread-loader/dist/cjs.js!../../../node_modules/.pnpm/registry.npmmirror.com+babel-loader@8.3.0_@babel+core@7.12.16_webpack@5.88.2/node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../../node_modules/.pnpm/registry.npmmirror.com+vue-loader@15.10.1_css-loader@6.8.1_vue-template-compiler@2.6.14_webpack@5.88.2/node_modules/vue-loader/lib/index.js??vue-loader-options!./index.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./index.vue?vue&type=template&id=5ead337c&scoped=true&\"\nimport script from \"./index.vue?vue&type=script&lang=js&\"\nexport * from \"./index.vue?vue&type=script&lang=js&\"\nimport style0 from \"./index.vue?vue&type=style&index=0&id=5ead337c&prod&lang=less&scoped=true&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/.pnpm/registry.npmmirror.com+vue-loader@15.10.1_css-loader@6.8.1_vue-template-compiler@2.6.14_webpack@5.88.2/node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"5ead337c\",\n  null\n  \n)\n\nexport default component.exports","import Vue from 'vue'\r\nimport VueRouter from 'vue-router'\r\nVue.use(VueRouter)\r\nimport Home from '@/views/Home/index.vue'\r\nimport Brochure from '@/views/Brochure/index.vue'\r\nimport ConferenceBrochure from '@/views/ConferenceBrochure/index.vue'\r\nconst routes = [\r\n  {\r\n    path: '/',\r\n    name: 'Home',\r\n    component: Home,\r\n  },\r\n  {\r\n    path: '/brochure',\r\n    name: 'Brochure',\r\n    component: Brochure,\r\n  },\r\n  {\r\n    path: '/fcb',\r\n    name: 'ConferenceBrochure',\r\n    component: ConferenceBrochure,\r\n  },\r\n]\r\n\r\nconst router = new VueRouter({ mode: 'hash', routes })\r\n\r\nexport default router\r\n","import Vue from 'vue'\r\nimport { Icon, Button, notification } from 'ant-design-vue'\r\nconst components = [Icon, Button]\r\n\r\nconst getComponent = () => {\r\n  Vue.prototype.$notification = notification\r\n  components.forEach(component => {\r\n    Vue.component(component.name, component)\r\n  })\r\n}\r\n\r\nexport default getComponent\r\n","const setRem = () => {\r\n  const screenWidth = window.innerWidth || document.documentElement.clientWidth\r\n  let scale = 1\r\n  const maxWidth = 750\r\n\r\n  if (screenWidth >= maxWidth) {\r\n    scale = screenWidth / 1920\r\n  } else if (screenWidth >= 375) {\r\n    scale = screenWidth / 1600\r\n  } else {\r\n    scale = screenWidth / 1280\r\n  }\r\n\r\n  const rem = 16 * scale\r\n  document.documentElement.style.fontSize = `${rem}px`\r\n}\r\n\r\nexport { setRem }\r\n","import Vue from 'vue'\r\nimport App from './App.vue'\r\nimport store from './store'\r\nimport router from './routes'\r\nimport getComponent from '@/plugins/ant'\r\nimport { setRem } from '@/utils/rem.js'\r\ngetComponent()\r\nVue.config.productionTip = false\r\nsetRem()\r\nwindow.onresize = () => {\r\n  setRem()\r\n}\r\nnew Vue({\r\n  store,\r\n  router,\r\n  render: h => h(App),\r\n}).$mount('#app')\r\n","var map = {\n\t\"./jiangzhuang (1).jpg\": 5123,\n\t\"./jiangzhuang (2).jpg\": 4126,\n\t\"./jiangzhuang (3).jpg\": 4651,\n\t\"./jiangzhuang (4).jpg\": 1564,\n\t\"./jiangzhuang (5).jpg\": 8977,\n\t\"./jiangzhuang (6).jpg\": 4842\n};\n\n\nfunction webpackContext(req) {\n\tvar id = webpackContextResolve(req);\n\treturn __webpack_require__(id);\n}\nfunction webpackContextResolve(req) {\n\tif(!__webpack_require__.o(map, req)) {\n\t\tvar e = new Error(\"Cannot find module '\" + req + \"'\");\n\t\te.code = 'MODULE_NOT_FOUND';\n\t\tthrow e;\n\t}\n\treturn map[req];\n}\nwebpackContext.keys = function webpackContextKeys() {\n\treturn Object.keys(map);\n};\nwebpackContext.resolve = webpackContextResolve;\nmodule.exports = webpackContext;\nwebpackContext.id = 2986;","// The module cache\nvar __webpack_module_cache__ = {};\n\n// The require function\nfunction __webpack_require__(moduleId) {\n\t// Check if module is in cache\n\tvar cachedModule = __webpack_module_cache__[moduleId];\n\tif (cachedModule !== undefined) {\n\t\treturn cachedModule.exports;\n\t}\n\t// Create a new module (and put it into the cache)\n\tvar module = __webpack_module_cache__[moduleId] = {\n\t\t// no module.id needed\n\t\t// no module.loaded needed\n\t\texports: {}\n\t};\n\n\t// Execute the module function\n\t__webpack_modules__[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n\n\t// Return the exports of the module\n\treturn module.exports;\n}\n\n// expose the modules object (__webpack_modules__)\n__webpack_require__.m = __webpack_modules__;\n\n","var deferred = [];\n__webpack_require__.O = function(result, chunkIds, fn, priority) {\n\tif(chunkIds) {\n\t\tpriority = priority || 0;\n\t\tfor(var i = deferred.length; i > 0 && deferred[i - 1][2] > priority; i--) deferred[i] = deferred[i - 1];\n\t\tdeferred[i] = [chunkIds, fn, priority];\n\t\treturn;\n\t}\n\tvar notFulfilled = Infinity;\n\tfor (var i = 0; i < deferred.length; i++) {\n\t\tvar chunkIds = deferred[i][0];\n\t\tvar fn = deferred[i][1];\n\t\tvar priority = deferred[i][2];\n\t\tvar fulfilled = true;\n\t\tfor (var j = 0; j < chunkIds.length; j++) {\n\t\t\tif ((priority & 1 === 0 || notFulfilled >= priority) && Object.keys(__webpack_require__.O).every(function(key) { return __webpack_require__.O[key](chunkIds[j]); })) {\n\t\t\t\tchunkIds.splice(j--, 1);\n\t\t\t} else {\n\t\t\t\tfulfilled = false;\n\t\t\t\tif(priority < notFulfilled) notFulfilled = priority;\n\t\t\t}\n\t\t}\n\t\tif(fulfilled) {\n\t\t\tdeferred.splice(i--, 1)\n\t\t\tvar r = fn();\n\t\t\tif (r !== undefined) result = r;\n\t\t}\n\t}\n\treturn result;\n};","// getDefaultExport function for compatibility with non-harmony modules\n__webpack_require__.n = function(module) {\n\tvar getter = module && module.__esModule ?\n\t\tfunction() { return module['default']; } :\n\t\tfunction() { return module; };\n\t__webpack_require__.d(getter, { a: getter });\n\treturn getter;\n};","// define getter functions for harmony exports\n__webpack_require__.d = function(exports, definition) {\n\tfor(var key in definition) {\n\t\tif(__webpack_require__.o(definition, key) && !__webpack_require__.o(exports, key)) {\n\t\t\tObject.defineProperty(exports, key, { enumerable: true, get: definition[key] });\n\t\t}\n\t}\n};","__webpack_require__.g = (function() {\n\tif (typeof globalThis === 'object') return globalThis;\n\ttry {\n\t\treturn this || new Function('return this')();\n\t} catch (e) {\n\t\tif (typeof window === 'object') return window;\n\t}\n})();","__webpack_require__.o = function(obj, prop) { return Object.prototype.hasOwnProperty.call(obj, prop); }","__webpack_require__.p = \"/workshop/\";","// no baseURI\n\n// object to store loaded and loading chunks\n// undefined = chunk not loaded, null = chunk preloaded/prefetched\n// [resolve, reject, Promise] = chunk loading, 0 = chunk loaded\nvar installedChunks = {\n\t143: 0\n};\n\n// no chunk on demand loading\n\n// no prefetching\n\n// no preloaded\n\n// no HMR\n\n// no HMR manifest\n\n__webpack_require__.O.j = function(chunkId) { return installedChunks[chunkId] === 0; };\n\n// install a JSONP callback for chunk loading\nvar webpackJsonpCallback = function(parentChunkLoadingFunction, data) {\n\tvar chunkIds = data[0];\n\tvar moreModules = data[1];\n\tvar runtime = data[2];\n\t// add \"moreModules\" to the modules object,\n\t// then flag all \"chunkIds\" as loaded and fire callback\n\tvar moduleId, chunkId, i = 0;\n\tif(chunkIds.some(function(id) { return installedChunks[id] !== 0; })) {\n\t\tfor(moduleId in moreModules) {\n\t\t\tif(__webpack_require__.o(moreModules, moduleId)) {\n\t\t\t\t__webpack_require__.m[moduleId] = moreModules[moduleId];\n\t\t\t}\n\t\t}\n\t\tif(runtime) var result = runtime(__webpack_require__);\n\t}\n\tif(parentChunkLoadingFunction) parentChunkLoadingFunction(data);\n\tfor(;i < chunkIds.length; i++) {\n\t\tchunkId = chunkIds[i];\n\t\tif(__webpack_require__.o(installedChunks, chunkId) && installedChunks[chunkId]) {\n\t\t\tinstalledChunks[chunkId][0]();\n\t\t}\n\t\tinstalledChunks[chunkId] = 0;\n\t}\n\treturn __webpack_require__.O(result);\n}\n\nvar chunkLoadingGlobal = self[\"webpackChunkworkshop\"] = self[\"webpackChunkworkshop\"] || [];\nchunkLoadingGlobal.forEach(webpackJsonpCallback.bind(null, 0));\nchunkLoadingGlobal.push = webpackJsonpCallback.bind(null, chunkLoadingGlobal.push.bind(chunkLoadingGlobal));","// startup\n// Load entry module and return exports\n// This entry module depends on other loaded chunks and execution need to be delayed\nvar __webpack_exports__ = __webpack_require__.O(undefined, [998], function() { return __webpack_require__(6314); })\n__webpack_exports__ = __webpack_require__.O(__webpack_exports__);\n"],"names":["render","_vm","this","_h","$createElement","_c","_self","attrs","staticRenderFns","component","Vue","use","Vuex","store","state","mutations","actions","getters","modules","staticClass","_m","on","$event","$router","push","_v","staticStyle","_l","item","index","key","_s","time","title","name","text","VueRouter","routes","path","Home","Brochure","ConferenceBrochure","router","mode","components","_Icon","_Button","getComponent","prototype","$notification","_notification","forEach","setRem","screenWidth","window","innerWidth","document","documentElement","clientWidth","scale","maxWidth","rem","style","fontSize","config","productionTip","onresize","h","App","$mount","map","webpackContext","req","id","webpackContextResolve","__webpack_require__","o","e","Error","code","keys","Object","resolve","module","exports","__webpack_module_cache__","moduleId","cachedModule","undefined","__webpack_modules__","call","m","deferred","O","result","chunkIds","fn","priority","notFulfilled","Infinity","i","length","fulfilled","j","every","splice","r","n","getter","__esModule","d","a","definition","defineProperty","enumerable","get","g","globalThis","Function","obj","prop","hasOwnProperty","p","installedChunks","chunkId","webpackJsonpCallback","parentChunkLoadingFunction","data","moreModules","runtime","some","chunkLoadingGlobal","self","bind","__webpack_exports__"],"sourceRoot":""}